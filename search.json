[
  {
    "objectID": "team/wjakethompson/index.html",
    "href": "team/wjakethompson/index.html",
    "title": "W. Jake Thompson",
    "section": "",
    "text": "Jake Thompson is the assistant director of psychometrics at Accessible Teaching, Learning, and Assessment Systems (ATLAS), a strategic center within the Achievement and Assessment Institute at the University of Kansas. He received a PhD in educational psychology and research from the University of Kansas’ School of Education and Human Sciences in 2018.\nJake has been the Principal Investigator for two grants funded by the Institute of Education Sciences to develop the r-dcm suite of packages for the estimation and evaluation of diagnostic classification models:\n\nImproving Software and Methods for Estimating Diagnostic Classification Models and Evaluating Model Fit (R305D210045)\nExpanding the Functionality and Accessibility of Software for Diagnostic Measurement (R305D240032)"
  },
  {
    "objectID": "team/auburnjimenez34/index.html",
    "href": "team/auburnjimenez34/index.html",
    "title": "Auburn Jimenez",
    "section": "",
    "text": "Auburn Jimenez is a psychometrician at Accessible Teaching, Learning, and Assessment Systems (ATLAS), a strategic center within the Achievement and Assessment Institute at the University of Kansas. He received a PhD in quantitative psychology from the University of Illinois Urbana-Champaign in 2023. His professional interests include the development and application of diagnostic classification and item response theory models in educational assessment."
  },
  {
    "objectID": "start/specify/index.html",
    "href": "start/specify/index.html",
    "title": "Specify a diagnostic model",
    "section": "",
    "text": "The first article will focus on how to specify a DCM with measr including:\n\nChoosing a measurement model\nChoosing a structural model\nChoosing prior distributions\nCombining these choices into a DCM specification using dcm_specify()\n\nThe article will then describe how to estimate the model with MCMC or an optimizer using either {rstan} or {cmdstanr} with dcm_estimate().",
    "crumbs": [
      "Get Started",
      "Specify a diagnostic model"
    ]
  },
  {
    "objectID": "start/specify/index.html#introduction",
    "href": "start/specify/index.html#introduction",
    "title": "Specify a diagnostic model",
    "section": "",
    "text": "The first article will focus on how to specify a DCM with measr including:\n\nChoosing a measurement model\nChoosing a structural model\nChoosing prior distributions\nCombining these choices into a DCM specification using dcm_specify()\n\nThe article will then describe how to estimate the model with MCMC or an optimizer using either {rstan} or {cmdstanr} with dcm_estimate().",
    "crumbs": [
      "Get Started",
      "Specify a diagnostic model"
    ]
  },
  {
    "objectID": "start/hierarchies/index.html",
    "href": "start/hierarchies/index.html",
    "title": "Define attribute relationships",
    "section": "",
    "text": "The third article will focus on identifying and estimating attribute relationships. We will discuss how to identify when a hierarchy might be present in your data and then how to model that hierarchy using different structural model specifications (e.g., hdcm(), bayes_net()).\nWe’ll then explore how to compare models with a specified hierarchy to a saturated model to evaluate the impact of the hierarchy on model fit.",
    "crumbs": [
      "Get Started",
      "Define attribute relationships"
    ]
  },
  {
    "objectID": "start/hierarchies/index.html#introduction",
    "href": "start/hierarchies/index.html#introduction",
    "title": "Define attribute relationships",
    "section": "",
    "text": "The third article will focus on identifying and estimating attribute relationships. We will discuss how to identify when a hierarchy might be present in your data and then how to model that hierarchy using different structural model specifications (e.g., hdcm(), bayes_net()).\nWe’ll then explore how to compare models with a specified hierarchy to a saturated model to evaluate the impact of the hierarchy on model fit.",
    "crumbs": [
      "Get Started",
      "Define attribute relationships"
    ]
  },
  {
    "objectID": "start/case-study/index.html",
    "href": "start/case-study/index.html",
    "title": "A diagnostic assessment case study",
    "section": "",
    "text": "The Examination for the Certificate of Proficiency in English (ECPE) is an assessment that measures advanced English skills for individuals for whom English is not the primary language. In this case study, we’ll use data from the grammar section of the ECPE, which uses 28 items to measure 3 skills: morphosyntactic rules, cohesive rules, and lexical rules. This data set has previously been used by Templin & Hoffman (2013) and X. Liu & Johnson (2019) to demonstrate how to estimate diagnostic classification models (DCMs) with Mplus and Markov chain Monte Carlo (MCMC), respectively. Additionally, Templin & Bradshaw (2014) used this ECPE data as a motivating example for developing a hierarchical DCM, and Chen et al. (2018) used the ECPE data to evaluate the effectiveness of the M2 statistic for assessing model fit in the presence of attribute hierarchies.",
    "crumbs": [
      "Get Started",
      "A diagnostic assessment case study"
    ]
  },
  {
    "objectID": "start/case-study/index.html#introduction",
    "href": "start/case-study/index.html#introduction",
    "title": "A diagnostic assessment case study",
    "section": "",
    "text": "The Examination for the Certificate of Proficiency in English (ECPE) is an assessment that measures advanced English skills for individuals for whom English is not the primary language. In this case study, we’ll use data from the grammar section of the ECPE, which uses 28 items to measure 3 skills: morphosyntactic rules, cohesive rules, and lexical rules. This data set has previously been used by Templin & Hoffman (2013) and X. Liu & Johnson (2019) to demonstrate how to estimate diagnostic classification models (DCMs) with Mplus and Markov chain Monte Carlo (MCMC), respectively. Additionally, Templin & Bradshaw (2014) used this ECPE data as a motivating example for developing a hierarchical DCM, and Chen et al. (2018) used the ECPE data to evaluate the effectiveness of the M2 statistic for assessing model fit in the presence of attribute hierarchies.",
    "crumbs": [
      "Get Started",
      "A diagnostic assessment case study"
    ]
  },
  {
    "objectID": "start/case-study/index.html#explore-the-data",
    "href": "start/case-study/index.html#explore-the-data",
    "title": "A diagnostic assessment case study",
    "section": "Explore the Data",
    "text": "Explore the Data\nThe ECPE data is built into measr and can be accessed by loading the package. A complete description of the data can be viewed using ?ecpe_data.\n\nlibrary(dcmdata)\n\necpe_data\n#&gt; # A tibble: 2,922 × 29\n#&gt;    resp_id    E1    E2    E3    E4    E5    E6    E7    E8    E9   E10   E11\n#&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt;  1       1     1     1     1     0     1     1     1     1     1     1     1\n#&gt;  2       2     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  3       3     1     1     1     1     1     1     0     1     1     1     1\n#&gt;  4       4     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  5       5     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  6       6     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  7       7     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  8       8     0     1     1     1     1     1     0     1     1     1     0\n#&gt;  9       9     1     1     1     1     1     1     1     1     1     1     1\n#&gt; 10      10     1     1     1     1     0     0     1     1     1     1     1\n#&gt; # ℹ 2,912 more rows\n#&gt; # ℹ 17 more variables: E12 &lt;int&gt;, E13 &lt;int&gt;, E14 &lt;int&gt;, E15 &lt;int&gt;, E16 &lt;int&gt;,\n#&gt; #   E17 &lt;int&gt;, E18 &lt;int&gt;, E19 &lt;int&gt;, E20 &lt;int&gt;, E21 &lt;int&gt;, E22 &lt;int&gt;,\n#&gt; #   E23 &lt;int&gt;, E24 &lt;int&gt;, E25 &lt;int&gt;, E26 &lt;int&gt;, E27 &lt;int&gt;, E28 &lt;int&gt;\n\nWe can see that the data set has one row for each respondent, and that 2,922 respondents completed this section of the ECPE. We also see that the data has 29 columns. The first column contains respondent identifiers, and the remaining 28 columns contain dichotomous item responses for each items. The item responses are coded as 0 for an incorrect response and 1 for a correct response.\nIn addition to the data, we also have a Q-matrix that define which attributes are measured by each item. The Q-matrix has 28 rows, which corresponds to the total number of items. The first column of the Q-matrix contains item identifiers, which are the same as the column names in ecpe_data that contain item responses. The remaining columns define the attributes measured by the ECPE. A value of 0 indicates that the item does not measure that attribute, whereas a value of 1 indicates that the attribute is measured by that item. For example, item E1 measures both morphosyntactic rules and cohesive rules, and item E4 measures only lexical rules.\n\necpe_qmatrix\n#&gt; # A tibble: 28 × 4\n#&gt;    item_id morphosyntactic cohesive lexical\n#&gt;    &lt;chr&gt;             &lt;int&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt;  1 E1                    1        1       0\n#&gt;  2 E2                    0        1       0\n#&gt;  3 E3                    1        0       1\n#&gt;  4 E4                    0        0       1\n#&gt;  5 E5                    0        0       1\n#&gt;  6 E6                    0        0       1\n#&gt;  7 E7                    1        0       1\n#&gt;  8 E8                    0        1       0\n#&gt;  9 E9                    0        0       1\n#&gt; 10 E10                   1        0       0\n#&gt; # ℹ 18 more rows\n\nFor a quick summary of the data, we can calculate the proportion of respondents that answered each question correctly (i.e., the item p-values).\n\nlibrary(tidyverse)\n\necpe_data |&gt;\n  summarize(across(-resp_id, mean)) |&gt;\n  pivot_longer(everything(), names_to = \"item_id\", values_to = \"pvalue\")\n#&gt; # A tibble: 28 × 2\n#&gt;    item_id pvalue\n#&gt;    &lt;chr&gt;    &lt;dbl&gt;\n#&gt;  1 E1       0.803\n#&gt;  2 E2       0.830\n#&gt;  3 E3       0.579\n#&gt;  4 E4       0.706\n#&gt;  5 E5       0.887\n#&gt;  6 E6       0.854\n#&gt;  7 E7       0.721\n#&gt;  8 E8       0.898\n#&gt;  9 E9       0.702\n#&gt; 10 E10      0.658\n#&gt; # ℹ 18 more rows\n\nWe can then join the item p-values with the Q-matrix to get a sense of which attributes are the most difficult. Overall, most of the items have relatively high p-values, with most items having a p-value between .6 and .9. Note that in general, items measuring morphosyntactic rules tend to be the most difficult (i.e., lower p-values), followed by items measuring cohesive rules, and finally items measuring lexical rules.\n\n\nPlot code\necpe_data |&gt;\n  summarize(across(-resp_id, mean)) |&gt;\n  pivot_longer(everything(), names_to = \"item_id\", values_to = \"pvalue\") |&gt;\n  left_join(ecpe_qmatrix, join_by(item_id)) |&gt;\n  pivot_longer(\n    c(morphosyntactic, cohesive, lexical),\n    names_to = \"attribute\",\n    values_to = \"measured\"\n  ) |&gt;\n  filter(measured == 1) |&gt;\n  summarize(\n    measures = paste(str_to_title(attribute), collapse = \"/\\n\"),\n    .by = c(item_id, pvalue)\n  ) |&gt;\n  mutate(measures = fct_reorder(measures, pvalue, mean)) |&gt;\n  ggplot(aes(x = pvalue, y = measures)) +\n  geom_point(\n    aes(color = measures),\n    position = position_jitter(height = 0.2, width = 0, seed = 1213),\n    size = 4,\n    show.legend = FALSE\n  ) +\n  scale_color_manual(\n    values = c(\"#023047\", \"#D7263D\", \"#8ECAE6\", \"#219EBC\", \"#F3D3BD\", \"#000000\")\n  ) +\n  expand_limits(x = c(0, 1)) +\n  scale_x_continuous(breaks = seq(0, 1, 0.2)) +\n  labs(x = \"Item *p*-value\", y = \"Measured attributes\")",
    "crumbs": [
      "Get Started",
      "A diagnostic assessment case study"
    ]
  },
  {
    "objectID": "start/case-study/index.html#dcm-estimation",
    "href": "start/case-study/index.html#dcm-estimation",
    "title": "A diagnostic assessment case study",
    "section": "DCM Estimation",
    "text": "DCM Estimation\nNow that we have a feel for our data, we will estimate a DCM. Following the original analysis of the ECPE data by Templin & Hoffman (2013), we’ll estimate a loglinear cognitive diagnostic model (LCDM). The LCDM is a general diagnostic model that allows for different attribute relationships on items (e.g., compensatory, non-compensatory) and subsumes many other types of DCMs (Henson et al., 2009; Henson & Templin, 2019).\nThe following code will estimate an LCDM. In the first two lines, we specify our data, Q-matrix, and the respondent and item identifiers. We then specify the type of DCM we want to estimate and define how the model should be estimated. In this case, we want to estimate the model using MCMC with the {rstan} package as the estimation engine. Finally, we can customize how the MCMC process is executed. For this example, we specified 4 chains, each with 1,000 warmup iterations and 500 retained iterations for 1,500 iterations total. This results in a total posterior distribution of 2,000 samples for each parameter (i.e., 500 iterations from each of the 4 chains). We also specified a file so that the estimated model will be saved once it is estimated.\n\nlibrary(measr)\n#&gt; \n#&gt; Attaching package: 'measr'\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     optim\n\necpe_spec &lt;- dcm_specify(\n  qmatrix = ecpe_qmatrix,\n  identifier = \"item_id\",\n  measurement_model = lcdm(),\n  structural_model = unconstrained()\n)\n\necpe_lcdm &lt;- dcm_estimate(\n  ecpe_spec,\n  data = ecpe_data,\n  identifier = \"resp_id\",\n  method = \"mcmc\",\n  backend = \"rstan\",\n  chains = 4,\n  iter = 1500,\n  warmup = 1000,\n  file = \"fits/ecpe-lcdm\"\n)\n\nNow that we’ve estimated a model, let’s examine the output. There are three types of information we’ll examine: structural parameters, item parameters, and respondent proficiency.\n\nStructural Parameters\nThe structural parameters define the base rate of membership in each of attribute profiles. Because the ECPE data consists of 3 dichotomous attributes, there are a total of 23 = 8 possible profiles, or classes. We can view the possible profiles using measr_extract(). This function extracts different aspects of a model estimated with measr. The order of the attributes in the profiles corresponds to the order the attributes were listed in the Q-matrix used to estimate the model. This means that attributes 1, 2, and 3 correspond to morphosyntactic, cohesive, and lexical rules, respectively.\n\necpe_classes &lt;- measr_extract(ecpe_lcdm, \"classes\")\necpe_classes\n#&gt; # A tibble: 8 × 4\n#&gt;   class   morphosyntactic cohesive lexical\n#&gt;   &lt;chr&gt;             &lt;int&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt; 1 [0,0,0]               0        0       0\n#&gt; 2 [1,0,0]               1        0       0\n#&gt; 3 [0,1,0]               0        1       0\n#&gt; 4 [0,0,1]               0        0       1\n#&gt; 5 [1,1,0]               1        1       0\n#&gt; 6 [1,0,1]               1        0       1\n#&gt; 7 [0,1,1]               0        1       1\n#&gt; 8 [1,1,1]               1        1       1\n\nWe can extract the structural parameters also using measr_extract(). For structural parameters, we see the class, or the attribute profile, and the estimated proportion of respondents in that class with a measure of error (the standard deviation of the posterior). For example, nearly 30% of respondents are estimated to not be proficient on any of the attributes (class 1), and 17% are estimated to proficient on just attributes 2 and 3 (class 7).\n\nstructural_parameters &lt;- measr_extract(ecpe_lcdm, \"strc_param\")\nstructural_parameters\n#&gt; # A tibble: 8 × 2\n#&gt;   class           estimate\n#&gt;   &lt;chr&gt;         &lt;rvar[1d]&gt;\n#&gt; 1 [0,0,0]  0.2987 ± 0.0164\n#&gt; 2 [1,0,0]  0.0117 ± 0.0066\n#&gt; 3 [0,1,0]  0.0157 ± 0.0106\n#&gt; 4 [0,0,1]  0.1287 ± 0.0198\n#&gt; 5 [1,1,0]  0.0093 ± 0.0057\n#&gt; 6 [1,0,1]  0.0183 ± 0.0102\n#&gt; 7 [0,1,1]  0.1727 ± 0.0199\n#&gt; 8 [1,1,1]  0.3448 ± 0.0177\n\nWhen looking at the structural parameters, we can see that respondents typically fall into only 4 of the 8 possible profiles. Specifically, respondents are typically proficient on no attributes, only attribute 3 (lexical rules), only attributes 2 and 3 (cohesive and lexical rules), or all attributes. This may indicate the presence of an attribute hierarchy, as suggested by Templin & Bradshaw (2014), where respondents must gain proficiency of lexical rules before they can gain proficiency of cohesive rules, and then finally morphosyntactic rules.\n\n\nPlot code\nstructural_parameters |&gt;\n  mutate(class = fct_inorder(class), prob = map_dbl(estimate, mean)) |&gt;\n  ggplot(aes(x = class, y = prob)) +\n  geom_col(fill = msr_colors[2]) +\n  labs(x = \"Class\", y = \"Base rate\")\n\n\n\n\n\n\n\n\n\nWe can also collapse across classes to calculate the base rate of proficiency for each individual attribute. Overall, the model estimates that 38% of respondents are proficient on morphosyntactic rules, 54% of respondents are proficient on cohesive rules, and 66% of respondents are proficient on lexical rules.\n\necpe_classes |&gt;\n  left_join(structural_parameters, join_by(class)) |&gt;\n  summarize(\n    morphosyntactic = rvar_sum(estimate[which(morphosyntactic == 1)]),\n    cohesive = rvar_sum(estimate[which(cohesive == 1)]),\n    lexical = rvar_sum(estimate[which(lexical == 1)])\n  )\n#&gt; # A tibble: 1 × 3\n#&gt;   morphosyntactic      cohesive       lexical\n#&gt;        &lt;rvar[1d]&gt;    &lt;rvar[1d]&gt;    &lt;rvar[1d]&gt;\n#&gt; 1    0.38 ± 0.019  0.54 ± 0.029  0.66 ± 0.014\n\nIn summary, both the profile- and attribute-level base rates tell a similar story. Respondents are most likely to be proficient on lexical rules and least likely to be proficient on morphosyntactic rules. This also mirrors our analysis of item p-values when we were exploring the data, which showed that items measuring morphosyntactic rules were more difficult than items measuring lexical or cohesive rules.\n\n\nItem Parameters\nThe item parameters define the log-odds of a respondent in each class providing a correct response. We can again extract our estimated item parameters using measr_extract(). Here, the estimate column reports estimated value for each parameter and a measure of the associated error (i.e., the standard deviation of the posterior distribution). For example, item E1 has four parameters, as it measures two attributes:\n\nAn intercept, which represents the log-odds of providing a correct response for a respondent who is proficient in neither of the attributes this item measures (i.e., morphosyntactic rules and cohesive rules).\nA main effect for morphosyntactic rules, which represents the increase in the log-odds of providing a correct response for a respondent who is proficient in that attribute.\nA main effect for cohesive rules, which represents the increase in the log-odds of providing a correct response for a respondent who is proficient in that attribute.\nAn interaction between morphosyntactic and cohesive rules, which is the change in the log-odds for a respondent who is proficient in both attributes.\n\n\nitem_parameters &lt;- measr_extract(ecpe_lcdm, what = \"item_param\")\nitem_parameters\n#&gt; # A tibble: 74 × 5\n#&gt;    item_id type        attributes                coefficient       estimate\n#&gt;    &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;                     &lt;chr&gt;           &lt;rvar[1d]&gt;\n#&gt;  1 E1      intercept   &lt;NA&gt;                      l1_0          0.82 ± 0.074\n#&gt;  2 E1      maineffect  morphosyntactic           l1_11         0.63 ± 0.395\n#&gt;  3 E1      maineffect  cohesive                  l1_12         0.64 ± 0.216\n#&gt;  4 E1      interaction morphosyntactic__cohesive l1_212        0.51 ± 0.507\n#&gt;  5 E2      intercept   &lt;NA&gt;                      l2_0          1.04 ± 0.077\n#&gt;  6 E2      maineffect  cohesive                  l2_12         1.23 ± 0.150\n#&gt;  7 E3      intercept   &lt;NA&gt;                      l3_0         -0.35 ± 0.073\n#&gt;  8 E3      maineffect  morphosyntactic           l3_11         0.76 ± 0.382\n#&gt;  9 E3      maineffect  lexical                   l3_13         0.36 ± 0.118\n#&gt; 10 E3      interaction morphosyntactic__lexical  l3_213        0.52 ± 0.397\n#&gt; # ℹ 64 more rows\n\nWe can compare these estimates to those that Templin & Hoffman (2013) reported when using different software to estimate the same model. In the following figure, most parameters fall on or very close to the dashed line, which represents perfect agreement.\n\n\nPlot code\nlibrary(glue)\n\necpe_templin &lt;- read_csv(\n  \"data/mplus-estimates-templin.csv\",\n  col_types = cols(.default = col_double())\n)\n\nparam_compare &lt;- item_parameters |&gt;\n  full_join(\n    ecpe_templin |&gt;\n      pivot_longer(-item) |&gt;\n      mutate(param = glue(\"l{item}_{name}\")) |&gt;\n      select(param, mplus_est = value) |&gt;\n      drop_na(everything()),\n    by = c(\"coefficient\" = \"param\")\n  ) |&gt;\n  mutate(\n    measr_est = map_dbl(estimate, mean),\n    type = case_when(\n      str_detect(coefficient, \"_0\") ~ \"Intercept\",\n      str_detect(coefficient, \"_1\") ~ \"Main Effect\",\n      str_detect(coefficient, \"_2\") ~ \"Interaction\"\n    ),\n    type = factor(type, levels = c(\"Intercept\", \"Main Effect\", \"Interaction\"))\n  )\n\nparam_compare |&gt;\n  ggplot(aes(x = measr_est, y = mplus_est)) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\n  geom_point(aes(color = type, shape = type), size = 3) +\n  scale_color_manual(values = msr_colors) +\n  expand_limits(x = c(-2, 3), y = c(-2, 3)) +\n  coord_fixed() +\n  labs(\n    x = \"measr\",\n    y = \"Templin & Hoffman (2013)\",\n    color = \"Parameter Type\",\n    shape = \"Parameter Type\"\n  )\n\n\n\n\n\n\n\n\n\nThere are some parameters that deviate from the line of perfect agreement, but these are expected. For example, take item E7, which measures morphosyntactic and lexical rules. Both measr and Templin & Hoffman (2013) report values of approximately -0.09 for the intercept and 0.94 for the main effect of lexical rules. For the main effect of morphosyntactic rules, measr estimated a value of 1.59, compared to a value of 2.86 reported by Templin & Hoffman (2013), a difference of -1.26. Similarly, the interaction term estimated by measr is 0.35, compared to a value of -0.95 reported by Templin & Hoffman (2013), a difference of 1.3. This indicates that the log-odds of providing a correct response for an individual who has mastered both attributes is approximately the same, regardless of software. That is, for measr, we get a log-odds of -0.07 + 1.59 + 0.92 + 0.35 = 2.79, and from Templin & Hoffman (2013), we get a log-odds of -0.11 + 2.86 + 0.95 + -0.95 = 2.75. This is true for all of the differences in the figure. There is a change to the main effect for morphosyntactic rules and corresponding change to the interaction term that “cancels out” the difference.\nWhy is this happening? Let’s revisit the proportion of respondents in each class. There are very few respondents who are proficient in morphosyntactic rules without also being proficient in both of the other attributes (classes 2, 5, and 6; less than 4% of all respondents). Therefore, there is less information for estimating the morphosyntactic main effects, which for items that measure multiple attributes, represent the increase in log-odds for proficiency in morphosyntactic rules conditional on not being proficient on the other attribute.\n\nstructural_parameters\n#&gt; # A tibble: 8 × 2\n#&gt;   class           estimate\n#&gt;   &lt;chr&gt;         &lt;rvar[1d]&gt;\n#&gt; 1 [0,0,0]  0.2987 ± 0.0164\n#&gt; 2 [1,0,0]  0.0117 ± 0.0066\n#&gt; 3 [0,1,0]  0.0157 ± 0.0106\n#&gt; 4 [0,0,1]  0.1287 ± 0.0198\n#&gt; 5 [1,1,0]  0.0093 ± 0.0057\n#&gt; 6 [1,0,1]  0.0183 ± 0.0102\n#&gt; 7 [0,1,1]  0.1727 ± 0.0199\n#&gt; 8 [1,1,1]  0.3448 ± 0.0177\n\nBecause there is less information available for the morphosyntactic main effects, the prior will have more influence on these parameters. Note in the above figure that the main effect estimates that are off the diagonal are less extreme when using measr. For example, the triangle at the top right is a main effect that was estimated to be nearly 3 by Templin & Hoffman (2013), but is just over 1.5 when the model is estimated with measr. Thus, there is a regularizing effect, where the prior is pulling in extreme values, which is an intended outcome. We did not discuss priors when estimating our model and instead used the default priors provided by measr. For more information on prior distributions, including information on how to specify your own prior distributions for the model parameters, see ?prior and vignette(\"model-estimation\", package = \"measr\").\n\n\nRespondent Proficiency\nThe final piece of output from our model we will examine is the respondent probabilities. There are two types of probabilities that we can calculate, both of which are returned by the score() function.\n\nresp_probs &lt;- score(ecpe_lcdm)\nresp_probs\n#&gt; $class_probabilities\n#&gt; # A tibble: 23,376 × 5\n#&gt;    resp_id class   probability       `2.5%`    `97.5%`\n#&gt;    &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1 1       [0,0,0] 0.00000773  0.00000385   0.0000137 \n#&gt;  2 1       [1,0,0] 0.0000981   0.00000762   0.000314  \n#&gt;  3 1       [0,1,0] 0.000000496 0.0000000249 0.00000148\n#&gt;  4 1       [0,0,1] 0.00133     0.000728     0.00211   \n#&gt;  5 1       [1,1,0] 0.0000876   0.00000653   0.000259  \n#&gt;  6 1       [1,0,1] 0.0435      0.00367      0.101     \n#&gt;  7 1       [0,1,1] 0.00208     0.00128      0.00311   \n#&gt;  8 1       [1,1,1] 0.953       0.895        0.992     \n#&gt;  9 2       [0,0,0] 0.00000588  0.00000286   0.0000108 \n#&gt; 10 2       [1,0,0] 0.0000746   0.00000549   0.000223  \n#&gt; # ℹ 23,366 more rows\n#&gt; \n#&gt; $attribute_probabilities\n#&gt; # A tibble: 8,766 × 5\n#&gt;    resp_id attribute       probability `2.5%` `97.5%`\n#&gt;    &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 1       morphosyntactic       0.997  0.995   0.998\n#&gt;  2 1       cohesive              0.955  0.897   0.995\n#&gt;  3 1       lexical               1.000  1.000   1.000\n#&gt;  4 2       morphosyntactic       0.995  0.992   0.997\n#&gt;  5 2       cohesive              0.898  0.765   0.987\n#&gt;  6 2       lexical               1.000  1.000   1.000\n#&gt;  7 3       morphosyntactic       0.983  0.970   0.991\n#&gt;  8 3       cohesive              0.988  0.975   0.997\n#&gt;  9 3       lexical               1.000  1.000   1.000\n#&gt; 10 4       morphosyntactic       0.998  0.996   0.998\n#&gt; # ℹ 8,756 more rows\n\nThe class_probabilites are the probabilities that the respondent belongs to each of the 8 possible classes (i.e., profiles of proficiency). The attribute_probabilites are the probabilities that the respondent is proficient on each of the individual attributes. To better describe the difference between these two probabilities, let’s look at the results for respondent 73. When looking at the class probabilities, the most likely profile is [1,1,1], meaning that the respondent is proficient on all attributes. However, there is only a 39% chance that the respondent belongs to that class. They also have a greater than 10% chance of belonging to the [0,0,1] and [0,1,1] classes.\n\nresp_probs$class_probabilities |&gt;\n  filter(resp_id == 73)\n#&gt; # A tibble: 8 × 5\n#&gt;   resp_id class   probability   `2.5%` `97.5%`\n#&gt;   &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 73      [0,0,0]     0.0495  0.0243    0.0878\n#&gt; 2 73      [1,0,0]     0.00581 0.000360  0.0217\n#&gt; 3 73      [0,1,0]     0.00422 0.000207  0.0122\n#&gt; 4 73      [0,0,1]     0.172   0.100     0.269 \n#&gt; 5 73      [1,1,0]     0.00763 0.000319  0.0282\n#&gt; 6 73      [1,0,1]     0.0134  0.00107   0.0337\n#&gt; 7 73      [0,1,1]     0.360   0.242     0.484 \n#&gt; 8 73      [1,1,1]     0.387   0.248     0.537\n\nThe attribute probabilities for respondent 73 show a slightly different story. These probabilities indicate that there is a 41% chance the respondent is proficient on morphosyntactic rules, a 76% chance the respondent is proficient on cohesive rules, and a 93% chance the respondent is proficient on lexical rules. That is, we’re fairly confident respondent 73 is proficient on lexical rules, somewhat confident they are proficient on cohesive rules, and not confident about whether or not the student is proficient on morphosyntactic rules (i.e., the proficiency probability could reasonably be as low as 27% or as high as 57%). These probabilities can be turned into classifications by setting proficiency thresholds. For example, we might decide that probabilities greater than .5 (i.e., more likely than not) indicate proficiency (e.g., Bradshaw & Levy, 2019). On the other hand, we might want to be more confident that a respondent is proficient before reporting as such, and therefore might set a higher threshold (e.g., .8; Thompson et al., 2019). For respondent 73, thresholds of .5 and .8 would result in proficiency profiles of [0,1,1] and [0,0,1], respectively. Either way, both profiles differ from the overall most likely profile indicated by the class probabilities. Thus, it is important to give careful consideration to how results are determined before they are reported.\n\nresp_probs$attribute_probabilities |&gt;\n  filter(resp_id == 73)\n#&gt; # A tibble: 3 × 5\n#&gt;   resp_id attribute       probability `2.5%` `97.5%`\n#&gt;   &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 73      morphosyntactic       0.413  0.268   0.573\n#&gt; 2 73      cohesive              0.759  0.644   0.848\n#&gt; 3 73      lexical               0.933  0.882   0.966\n\nBy default, score() only returns a summary of the posterior distribution for each probability (i.e., the mean and 95% credible interval). There are many class and attribute probabilities, and therefore the object containing the full posterior distributions would be quite large. You can change the percentiles that are returned in the posterior summary by setting the probs argument to quantiles other than the default of probs = c(0.025, 0.975). Alternatively, if you do want the full posterior distribution for each probability, you can set summary = FALSE. This will return a posterior::rvar() object (the same as the structural and item parameter summaries) that contains all of the posterior draws for each probability, and is displayed as the mean of the posterior ±1 standard deviation. For more information on rvar objects, see the accompanying vignette (vignette(\"rvar\", package = \"posterior\")).\n\nscore(ecpe_lcdm, summary = FALSE)\n#&gt; $class_probabilities\n#&gt; # A tibble: 2,922 × 9\n#&gt;    resp_id          `[0,0,0]`              `[1,0,0]`          `[0,1,0]`\n#&gt;    &lt;chr&gt;           &lt;rvar[1d]&gt;             &lt;rvar[1d]&gt;         &lt;rvar[1d]&gt;\n#&gt;  1 1        7.7e-06 ± 2.5e-06  0.0000981 ± 0.0000824  5.0e-07 ± 4.0e-07\n#&gt;  2 2        5.9e-06 ± 2.0e-06  0.0000746 ± 0.0000644  1.9e-07 ± 1.7e-07\n#&gt;  3 3        5.6e-06 ± 2.1e-06  0.0000170 ± 0.0000207  1.7e-06 ± 1.4e-06\n#&gt;  4 4        3.2e-07 ± 1.1e-07  0.0000041 ± 0.0000035  9.5e-08 ± 7.6e-08\n#&gt;  5 5        1.2e-03 ± 3.7e-04  0.0083757 ± 0.0064401  3.4e-04 ± 2.6e-04\n#&gt;  6 6        3.0e-06 ± 1.0e-06  0.0000155 ± 0.0000149  8.8e-07 ± 7.2e-07\n#&gt;  7 7        3.0e-06 ± 1.0e-06  0.0000155 ± 0.0000149  8.8e-07 ± 7.2e-07\n#&gt;  8 8        3.7e-02 ± 1.2e-02  0.0000850 ± 0.0001022  1.3e-03 ± 1.1e-03\n#&gt;  9 9        6.6e-05 ± 2.0e-05  0.0002019 ± 0.0001488  1.9e-05 ± 1.5e-05\n#&gt; 10 10       4.1e-01 ± 1.4e-01  0.4056911 ± 0.1803258  3.5e-03 ± 2.9e-03\n#&gt; # ℹ 2,912 more rows\n#&gt; # ℹ 5 more variables: `[0,0,1]` &lt;rvar[1d]&gt;, `[1,1,0]` &lt;rvar[1d]&gt;,\n#&gt; #   `[1,0,1]` &lt;rvar[1d]&gt;, `[0,1,1]` &lt;rvar[1d]&gt;, `[1,1,1]` &lt;rvar[1d]&gt;\n#&gt; \n#&gt; $attribute_probabilities\n#&gt; # A tibble: 2,922 × 4\n#&gt;    resp_id   morphosyntactic       cohesive          lexical\n#&gt;    &lt;chr&gt;          &lt;rvar[1d]&gt;     &lt;rvar[1d]&gt;       &lt;rvar[1d]&gt;\n#&gt;  1 1        0.9966 ± 0.00070  0.96 ± 0.0251  1.00 ± 0.000115\n#&gt;  2 2        0.9949 ± 0.00137  0.90 ± 0.0586  1.00 ± 0.000075\n#&gt;  3 3        0.9827 ± 0.00541  0.99 ± 0.0058  1.00 ± 0.000079\n#&gt;  4 4        0.9975 ± 0.00052  0.99 ± 0.0057  1.00 ± 0.000014\n#&gt;  5 5        0.9880 ± 0.00244  0.98 ± 0.0086  0.95 ± 0.026761\n#&gt;  6 6        0.9924 ± 0.00214  0.99 ± 0.0057  1.00 ± 0.000063\n#&gt;  7 7        0.9924 ± 0.00214  0.99 ± 0.0057  1.00 ± 0.000063\n#&gt;  8 8        0.0044 ± 0.00201  0.44 ± 0.0802  0.96 ± 0.012098\n#&gt;  9 9        0.9453 ± 0.01288  0.98 ± 0.0061  1.00 ± 0.000780\n#&gt; 10 10       0.5356 ± 0.15668  0.12 ± 0.0611  0.12 ± 0.047149\n#&gt; # ℹ 2,912 more rows",
    "crumbs": [
      "Get Started",
      "A diagnostic assessment case study"
    ]
  },
  {
    "objectID": "start/case-study/index.html#dcm-evaluation",
    "href": "start/case-study/index.html#dcm-evaluation",
    "title": "A diagnostic assessment case study",
    "section": "DCM Evaluation",
    "text": "DCM Evaluation\nThere are several ways we might evaluate an estimate model. In this case study, we’ll focus on two: absolute model fit and classification reliability.\n\nAbsolute Model Fit\nOne of the most common measures of model fit for DCMs is the M2 statistic. This index is a limited information goodness-of-fit measure originally described by Maydeu-Olivares & Joe (2005, 2006) and adapted for DCMs by Y. Liu et al. (2016). We can calculate the M2 for a model estimated with measr with fit_m2(). In addition to the calculated M2 statistic, fit_m2() also returns the root mean square error of approximation (RMSEA) with an associated confidence interval and the standardized root mean square residual (SRMSR).\n\nfit_m2(ecpe_lcdm)\n#&gt; # A tibble: 1 × 8\n#&gt;      m2    df     pval  rmsea ci_lower ci_upper `90% CI`          srmsr\n#&gt;   &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n#&gt; 1  513.   325 1.35e-10 0.0141   0.0117   0.0163 [0.0117, 0.0163] 0.0319\n\nFor our estimated LCDM, we see an M2 value of 512.8, which has a corresponding p-value of &lt;.01. When interpreting the M2 and its p-value, the null hypothesis is that the model fits. Thus, the p-value represents the probability of observing an M2 value this large if the model fits. For our estimated LCDM, the p-value is extremely small, indicating that our model has poor fit.\nA fully Bayesian estimation allows us to evaluate model fit using posterior predictive model checks (PPMCs). Specifically, measr supports a PPMC of the overall raw score distribution as described by Park et al. (2015) and Thompson (2019). For each of the replicated data sets, we calculate the number of students with each raw score (i.e., the number of correct responses). This can be done using fit_ppmc(). Note that we can also calculate item-level PPMCs. However, because in this case study we are only interested in overall model fit, we’ll set item_fit = NULL to save some computation time.\n\nrawscore_ppmc &lt;- fit_ppmc(\n  ecpe_lcdm,\n  model_fit = \"raw_score\",\n  return_draws = 500\n)\nrawscore_ppmc\n#&gt; $ppmc_raw_score\n#&gt; # A tibble: 1 × 7\n#&gt;   obs_chisq ppmc_mean `2.5%` `97.5%` rawscore_samples   chisq_samples    ppp\n#&gt;       &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;             &lt;list&gt;         &lt;dbl&gt;\n#&gt; 1      931.      28.8   12.4    60.1 &lt;tibble [500 × 1]&gt; &lt;dbl [500]&gt;   0.0005\n\nIn the output, the posterior predictive p-value (ppp) is very small, indicating poor fit. To unpack what this really means, let’s visualize the PPMC. In the following figure, the blue bars show the credible intervals for the number of respondents we would expect to see at each raw score point, given our estimated model parameters. The red dots and line indicate the number of respondents that were observed at each raw score point in our observed data (ecpe_data). For example, the model expects there to be between about 110 and 170 respondents with a total score of 14. In the observed data, there were 92 respondents with a total score of 14. In general, the model tends to overestimate the number of respondents with a raw score between 14–16 and 23–25. On the other hand, the model underestimates the number of respondents with a raw score between 6–10 and 27–28.\n\n\nPlot code\nlibrary(ggdist)\n\nobs_scores &lt;- ecpe_data |&gt;\n  pivot_longer(cols = -\"resp_id\") |&gt;\n  summarize(raw_score = sum(value), .by = resp_id) |&gt;\n  count(raw_score) |&gt;\n  complete(raw_score = 0:28, fill = list(n = 0L))\n\nrawscore_ppmc$ppmc_raw_score |&gt;\n  dplyr::select(rawscore_samples) |&gt;\n  unnest(rawscore_samples) |&gt;\n  unnest(raw_scores) |&gt;\n  ggplot() +\n  stat_interval(\n    aes(x = raw_score, y = n, color_ramp = after_stat(level)),\n    point_interval = \"mean_qi\",\n    color = msr_colors[2],\n    linewidth = 5,\n    show.legend = c(color = FALSE)\n  ) +\n  geom_line(\n    data = obs_scores,\n    aes(x = raw_score, y = n),\n    color = msr_colors[3]\n  ) +\n  geom_point(\n    data = obs_scores,\n    aes(x = raw_score, y = n, fill = \"Observed Data\"),\n    shape = 21,\n    color = msr_colors[3],\n    size = 2\n  ) +\n  scale_color_ramp_discrete(\n    from = \"white\",\n    range = c(0.2, 1),\n    breaks = c(0.5, 0.8, 0.95),\n    labels = ~ sprintf(\"%0.2f\", as.numeric(.x))\n  ) +\n  scale_fill_manual(values = c(msr_colors[3])) +\n  scale_x_continuous(breaks = seq(0, 28, 2), expand = c(0, 0)) +\n  scale_y_comma() +\n  labs(\n    x = \"Raw score\",\n    y = \"Respondents\",\n    color_ramp = \"Credible Interval\",\n    fill = NULL\n  ) +\n  guides(fill = guide_legend(override.aes = list(size = 3)))\n\n\n\n\n\n\n\n\n\nWe can quantify how different the observed raw score distribution is from the replicated data sets by calculating a χ2-like statistic. To do this, we first calculate the expected number of students at each raw score by taking the mean of the posterior distribution for each score point. Then, for each replicated data set, we calculate χ2-like statistic as\n\\[\n\\chi^2_{rep} = \\sum_{s=0}^S \\frac{[n_s - E(n_s)]^2}{E(n_s)},\n\\]\nwhere s represents the raw score, ns is the number of respondents at score point s, and E(ns) is the expected number of respondents at score point s (i.e., the mean of the posterior distribution). This calculation is completed on each of the replicated data sets, creating a posterior distribution of χ2rep that represents the plausible values for the χ2-like statistic if our model is correct. This distribution is summarized in the fit_ppmc() output. Specifically, we expect the χ2-like statistic for our observed data to be between 12 and 60, as shown in the following figure. However, when we calculate the statistic on our observed data, we get a value of 931, way beyond our expected range. This is represented by the ppp value, which is the proportion of χ2rep values that are larger than our observed value. In this case, no values of χ2rep were larger than our observed value, leading to a ppp of 0.\n\n\nPlot code\nrawscore_ppmc$ppmc_raw_score |&gt;\n  dplyr::select(chisq_samples) |&gt;\n  unnest(chisq_samples) |&gt;\n  ggplot(aes(x = chisq_samples)) +\n  stat_dots(\n    quantiles = 500,\n    layout = \"hex\",\n    stackratio = 0.9,\n    color = msr_colors[2],\n    fill = msr_colors[2],\n    na.rm = TRUE\n  ) +\n  scale_x_continuous(limits = c(0, 100)) +\n  labs(y = NULL, x = \"&chi;^2^&lt;sub&gt;rep&lt;/sub&gt;\") +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\nIn summary, both the M2 and raw score PPMC indicate poor fit of our estimated LCDM to the observed data. This is not unexpected, given that some classes are very small. Recall from our discussion of the estimated structural parameters that there are three classes that combine to include less than 4% of all respondents. When classes are this small, parameter estimates can be unstable, leading to poor model fit (e.g., Hu & Templin, 2020; Ma et al., 2023; Martinez & Templin, 2023; Templin & Bradshaw, 2014; Wang & Lu, 2021).\n\n\nClassification Reliability\nDepending on the intended uses of our assessment, we may be less concerned with overall model fit and more concerned with the consistency and accuracy of classifications. In other words, we may be focused on the reliability of the classifications produced by our model. There are several ways to evaluate the reliability evidence for DCMs. For a comprehensive summary of methods, see Sinharay & Johnson (2019). Using measr, we can easily calculate a wide variety of reliability metrics for our estimated LCDM using reliability().\n\necpe_reliability &lt;- reliability(ecpe_lcdm)\necpe_reliability\n#&gt; $pattern_reliability\n#&gt;       p_a       p_c \n#&gt; 0.7393205 0.6637904 \n#&gt; \n#&gt; $map_reliability\n#&gt; $map_reliability$accuracy\n#&gt; # A tibble: 3 × 8\n#&gt;   attribute         acc lambda_a kappa_a youden_a tetra_a  tp_a  tn_a\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 morphosyntactic 0.896    0.729   0.787    0.775   0.942 0.852 0.924\n#&gt; 2 cohesive        0.852    0.676   0.704    0.699   0.893 0.876 0.823\n#&gt; 3 lexical         0.916    0.750   0.611    0.802   0.959 0.947 0.855\n#&gt; \n#&gt; $map_reliability$consistency\n#&gt; # A tibble: 3 × 10\n#&gt;   attribute       consist lambda_c kappa_c youden_c tetra_c  tp_c  tn_c gammak\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 morphosyntactic   0.834    0.557   0.685    0.646   0.853 0.779 0.868  0.852\n#&gt; 2 cohesive          0.807    0.565   0.682    0.609   0.818 0.827 0.783  0.789\n#&gt; 3 lexical           0.857    0.554   0.625    0.671   0.876 0.894 0.777  0.880\n#&gt; # ℹ 1 more variable: pc_prime &lt;dbl&gt;\n#&gt; \n#&gt; \n#&gt; $eap_reliability\n#&gt; # A tibble: 3 × 5\n#&gt;   attribute       rho_pf rho_bs rho_i rho_tb\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 morphosyntactic  0.735  0.687 0.573  0.884\n#&gt; 2 cohesive         0.730  0.575 0.506  0.786\n#&gt; 3 lexical          0.760  0.730 0.587  0.915\n\nBy default, reliability() returns several different types of reliability evidence. For all types of evidence, the indices range from 0–1, with values close to 1 indicating high accuracy or consistency. Which information is most relevant will depend on how scores are determined and reported. For example, we could determine each respondent’s scores by choosing the overall profile that is most consistent with their observed responses (i.e., the class probabilities returned by score()). For this type of classification we would want to look at pattern reliability, as we are classifying responding into an overall pattern of proficiency on the attributes.\n\necpe_reliability$pattern_reliability\n#&gt;       p_a       p_c \n#&gt; 0.7393205 0.6637904\n\nThe values p_a and p_c are described by Cui et al. (2012). Pa is the probability classifying a random respondent into the correct class, and Pc is the probability of consistently classifying a random respondent into the same class across two test administrations.\nOn the other hand, rather than basing results on the overall most likely profile, we could score each attribute individually (i.e., the attribute probabilities returned by score()). This is accomplished by calculating the probability of proficiency on each attribute and creating classifications based on a given threshold (usually .5). This result is known as the maximum a posteriori (MAP) because it represents the most likely latent state for each respondent on each attribute. As with the pattern-level classifications, attribute level classifications can be evaluated through accuracy and consistency. Johnson & Sinharay (2018) developed accuracy (acc) and consistency (consist) metrics for attribute level classifications, and also examined other agreement measures based on contingency tables such as Goodman & Kruskal’s λ, Cohen’s κ, Youden’s J, the true positive rate, and the true negative rate.\n\necpe_reliability$map_reliability\n#&gt; $accuracy\n#&gt; # A tibble: 3 × 8\n#&gt;   attribute         acc lambda_a kappa_a youden_a tetra_a  tp_a  tn_a\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 morphosyntactic 0.896    0.729   0.787    0.775   0.942 0.852 0.924\n#&gt; 2 cohesive        0.852    0.676   0.704    0.699   0.893 0.876 0.823\n#&gt; 3 lexical         0.916    0.750   0.611    0.802   0.959 0.947 0.855\n#&gt; \n#&gt; $consistency\n#&gt; # A tibble: 3 × 10\n#&gt;   attribute       consist lambda_c kappa_c youden_c tetra_c  tp_c  tn_c gammak\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 morphosyntactic   0.834    0.557   0.685    0.646   0.853 0.779 0.868  0.852\n#&gt; 2 cohesive          0.807    0.565   0.682    0.609   0.818 0.827 0.783  0.789\n#&gt; 3 lexical           0.857    0.554   0.625    0.671   0.876 0.894 0.777  0.880\n#&gt; # ℹ 1 more variable: pc_prime &lt;dbl&gt;\n\nUsing the cutoffs recommended by Johnson & Sinharay (2018), the cohesive rules attribute has fair accuracy while the morphosyntactic and lexical rules attributes have good accuracy. All attributes have fair classification consistency.\nFinally, results could be reported as probabilities of proficiency on each attribute, rather than a categorical classification. In this instance, because the probabilities themselves are reported, we would want to report the reliability or precision of that probability estimate. This type of result is known as the expected a posteriori (EAP) estimate because it is the expected value of the classification. Johnson & Sinharay (2020) described four metrics for evaluating the reliability of the EAP estimates: (1) biserial, (2) informational, (3) parallel forms, and (4) a constrained parallel forms originally proposed by Templin & Bradshaw (2013). In their paper, Johnson & Sinharay (2020) note that both types of parallel form reliability tend to over estimate the reliability, and therefore recommend using the biserial or informational reliability metrics. These metrics are available in the reliability output as rho_bs and rho_i.\n\necpe_reliability$eap_reliability\n#&gt; # A tibble: 3 × 5\n#&gt;   attribute       rho_pf rho_bs rho_i rho_tb\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 morphosyntactic  0.735  0.687 0.573  0.884\n#&gt; 2 cohesive         0.730  0.575 0.506  0.786\n#&gt; 3 lexical          0.760  0.730 0.587  0.915\n\nUsing the cutoffs suggested by Johnson & Sinharay (2020), all three attributes have poor EAP reliability. It’s not surprising that the EAP reliability is lower than MAP reliability, as it is harder to place respondents at a specific point on a scale (i.e., the probability scale) than it is to place respondents into a category. For example, let’s return to our example of respondent 73.\n\nresp_probs$attribute_probabilities |&gt;\n  filter(resp_id == 73)\n#&gt; # A tibble: 3 × 5\n#&gt;   resp_id attribute       probability `2.5%` `97.5%`\n#&gt;   &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 73      morphosyntactic       0.413  0.268   0.573\n#&gt; 2 73      cohesive              0.759  0.644   0.848\n#&gt; 3 73      lexical               0.933  0.882   0.966\n\nWe estimated that there is a 93% chance that this respondent was proficient on lexical rules. However, the credible interval tells us this probability could be anywhere from 88% to 97%. There’s a nine percentage point range that is plausible. We don’t have a great deal of certainty for the specific probability that the respondent is proficient in lexical rules; however, the entire plausible range is high, so we would make a classification of “proficient” regardless of where in the range the true probability of proficiency is. That is, we would consistently make the same classification decision, regardless of the uncertainty in the probability itself.",
    "crumbs": [
      "Get Started",
      "A diagnostic assessment case study"
    ]
  },
  {
    "objectID": "start/case-study/index.html#summary",
    "href": "start/case-study/index.html#summary",
    "title": "A diagnostic assessment case study",
    "section": "Summary",
    "text": "Summary\nIn this case study, we estimated an LCDM to analyze the ECPE data. From the model estimation, we saw that the estimates provided by measr were highly consistent with previously reported parameters estimates for the ECPE. However, model fit indices indicated that the LCDM does not do a great job of represented the observed data. This is likely due to dependencies among the attributes. To further analyze this data, we might consider a model with a different attribute structure, such as the hierarchical diagnostic classification model (Templin & Bradshaw, 2014). Despite the poor model fit, reliability indices showed that classification consistency and accuracy were generally in the fair to good range, and therefore, depending on our intended uses, this model may be sufficient for reporting respondent proficiency of the three attributes.",
    "crumbs": [
      "Get Started",
      "A diagnostic assessment case study"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "r-dcm",
    "section": "",
    "text": "The r-dcm suite is a collection of packages for estimating and evaluating diagnostic classification models in R using tidyverse principles.\nWhat do you need to know to start using the r-dcm packages?  Get started by learning what you need in four articles, starting with how to specify a model and ending with a beginning-to-end case study.\n\n\n\n\n\n\n\nSome intro text… \n\n\n\n\n\n\n\n\n\n\n\nEstimate and evaluate DCMs with R and Stan.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample datasets for testing and teaching.\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate Stan code for customizing your model.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#diagnostic-psychometrics-with-r",
    "href": "index.html#diagnostic-psychometrics-with-r",
    "title": "r-dcm",
    "section": "",
    "text": "The r-dcm suite is a collection of packages for estimating and evaluating diagnostic classification models in R using tidyverse principles.\nWhat do you need to know to start using the r-dcm packages?  Get started by learning what you need in four articles, starting with how to specify a model and ending with a beginning-to-end case study."
  },
  {
    "objectID": "index.html#packages",
    "href": "index.html#packages",
    "title": "r-dcm",
    "section": "",
    "text": "Some intro text… \n\n\n\n\n\n\n\n\n\n\n\nEstimate and evaluate DCMs with R and Stan.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample datasets for testing and teaching.\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate Stan code for customizing your model.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Posts",
    "section": "",
    "text": "dcmstan 0.1.0\n\n\n\n\n\n\npackage\n\ndcmstan\n\n\n\nInitial release dcmstan for specifying diagnostic models and generating Stan code.\n\n\n\n\n\nNovember 26, 2025\n\n\nW. Jake Thompson\n\n\n\n\n\n\n\n\n\n\n\n\ndcmdata 0.1.0\n\n\n\n\n\n\npackage\n\ndcmdata\n\n\n\nInitial release dcmdata for accessing data from diagnostic assessments.\n\n\n\n\n\nAugust 21, 2025\n\n\nW. Jake Thompson\n\n\n\n\n\n\n\n\n\n\n\n\nmeasr 1.0.0\n\n\n\n\n\n\npackage\n\nmeasr\n\n\n\nA major release to reflect development milestones.\n\n\n\n\n\nFebruary 5, 2024\n\n\nW. Jake Thompson\n\n\n\n\n\n\n\n\n\n\n\n\nmeasr 0.3.1\n\n\n\n\n\n\npackage\n\nmeasr\n\n\n\nA few updates for specifying models and priors.\n\n\n\n\n\nJune 6, 2023\n\n\nW. Jake Thompson\n\n\n\n\n\n\n\n\n\n\n\n\nmeasr 0.2.1\n\n\n\n\n\n\npackage\n\nmeasr\n\n\n\nInitial release of measr package for estimating and evaluating diagnostic classification models.\n\n\n\n\n\nApril 10, 2023\n\n\nW. Jake Thompson\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2025-08-dcmdata-0.1.0/index.html",
    "href": "blog/2025-08-dcmdata-0.1.0/index.html",
    "title": "dcmdata 0.1.0",
    "section": "",
    "text": "We are so happy to announce the release of a new package, dcmdata. The goal of dcmdata is to provide easy access to data sets that can be used for demonstrating and testing diagnostic classification models (DCM; also called cognitive diagnostic models [CDMs]).\nYou can install dcmdata from CRAN with:\nThis blog post will highlight the major features and plans for future development.\nlibrary(dcmdata)"
  },
  {
    "objectID": "blog/2025-08-dcmdata-0.1.0/index.html#data-sets",
    "href": "blog/2025-08-dcmdata-0.1.0/index.html#data-sets",
    "title": "dcmdata 0.1.0",
    "section": "Data sets",
    "text": "Data sets\ndcmdata contains both real and simulated data sets. All data sets include both response data and a Q-matrix. The real data sets include the MacReady and Dayton (1977) multiplication data (MDM) and the Examination for the certificate of proficiency in English (ECPE), as described by Templin & Hoffman (2013).\nThe MDM data are a small data set of four items that measure a single attribute, multiplication. As such, this data is useful for use cases where you are interested in a fairly short estimation time. For example, this data could be used to quickly interate while testing model code, or in training workshops where time is limited.\n\nmdm_data\n#&gt; # A tibble: 142 × 5\n#&gt;    respondent  mdm1  mdm2  mdm3  mdm4\n#&gt;    &lt;fct&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt;  1 m8qre          1     1     1     1\n#&gt;  2 8wMPc          1     1     1     1\n#&gt;  3 xdbT8          1     1     1     1\n#&gt;  4 Ee9ob          1     1     1     1\n#&gt;  5 0tyTA          1     1     1     1\n#&gt;  6 L4bzq          1     1     1     1\n#&gt;  7 QTW1v          1     1     1     1\n#&gt;  8 w4NOH          1     1     1     1\n#&gt;  9 t9sIe          1     1     1     1\n#&gt; 10 FDa7I          1     1     1     1\n#&gt; # ℹ 132 more rows\n\nmdm_qmatrix\n#&gt; # A tibble: 4 × 2\n#&gt;   item  multiplication\n#&gt;   &lt;chr&gt;          &lt;int&gt;\n#&gt; 1 mdm1               1\n#&gt; 2 mdm2               1\n#&gt; 3 mdm3               1\n#&gt; 4 mdm4               1\n\nIn contrast, the ECPE data are perhaps a more representative of data you might gather in practice. These data consist of 28 items measuring 3 attributes and taken by 2,922 respondents. Because multiple attributes are measured, this data can be used to demonstrate how different attributes interact on a single item when using different compensatory and noncompensatory DCMs. Additionally, Templin & Bradshaw (2014) demonstrated that the three attributes follow a linear hierarchy, such that respondents are typically demonstrate proficiency on lexical, cohesive, and morphosyntactic rules, in that order. That is, the earlier skills represent precursor knowledge necessary for proficiency on the later skills. The hierarchy makes the ECPE data excellent for demonstrating the effect of different structural model specifications in a DCM.\n\necpe_data\n#&gt; # A tibble: 2,922 × 29\n#&gt;    resp_id    E1    E2    E3    E4    E5    E6    E7    E8    E9   E10   E11\n#&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt;  1       1     1     1     1     0     1     1     1     1     1     1     1\n#&gt;  2       2     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  3       3     1     1     1     1     1     1     0     1     1     1     1\n#&gt;  4       4     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  5       5     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  6       6     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  7       7     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  8       8     0     1     1     1     1     1     0     1     1     1     0\n#&gt;  9       9     1     1     1     1     1     1     1     1     1     1     1\n#&gt; 10      10     1     1     1     1     0     0     1     1     1     1     1\n#&gt; # ℹ 2,912 more rows\n#&gt; # ℹ 17 more variables: E12 &lt;int&gt;, E13 &lt;int&gt;, E14 &lt;int&gt;, E15 &lt;int&gt;, E16 &lt;int&gt;,\n#&gt; #   E17 &lt;int&gt;, E18 &lt;int&gt;, E19 &lt;int&gt;, E20 &lt;int&gt;, E21 &lt;int&gt;, E22 &lt;int&gt;,\n#&gt; #   E23 &lt;int&gt;, E24 &lt;int&gt;, E25 &lt;int&gt;, E26 &lt;int&gt;, E27 &lt;int&gt;, E28 &lt;int&gt;\n\necpe_qmatrix\n#&gt; # A tibble: 28 × 4\n#&gt;    item_id morphosyntactic cohesive lexical\n#&gt;    &lt;chr&gt;             &lt;int&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt;  1 E1                    1        1       0\n#&gt;  2 E2                    0        1       0\n#&gt;  3 E3                    1        0       1\n#&gt;  4 E4                    0        0       1\n#&gt;  5 E5                    0        0       1\n#&gt;  6 E6                    0        0       1\n#&gt;  7 E7                    1        0       1\n#&gt;  8 E8                    0        1       0\n#&gt;  9 E9                    0        0       1\n#&gt; 10 E10                   1        0       0\n#&gt; # ℹ 18 more rows\n\nFinally, one simulated data set is included. This data set is based on the diagnosing teachers’ multiplicative reasoning (DTMR) data presented by Bradshaw et al. (2014), and consists of 990 responses to 27 items that collectively measure 4 attributes. Consistent with the results presented by Bradshaw et al. (2014), the data was generated using the loglinear cognitive diagnostic model (LCDM; Henson et al., 2009; Henson & Templin, 2019), and item and attribute names are included as reported in their Table 1.\n\ndtmr_data\n#&gt; # A tibble: 990 × 28\n#&gt;    id      `1`   `2`   `3`   `4`   `5`   `6`   `7`  `8a`  `8b`  `8c`  `8d`   `9`\n#&gt;    &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt;  1 0008…     1     1     0     1     0     0     1     1     0     1     1     0\n#&gt;  2 0009…     0     1     0     0     0     0     0     1     1     1     0     1\n#&gt;  3 0024…     0     1     0     0     0     0     1     1     1     1     0     0\n#&gt;  4 0031…     0     1     0     0     1     0     1     1     1     0     0     0\n#&gt;  5 0061…     0     1     1     0     0     0     0     0     0     1     0     0\n#&gt;  6 0087…     0     1     1     1     0     0     0     1     1     1     1     0\n#&gt;  7 0092…     0     1     1     1     1     0     0     1     1     1     0     0\n#&gt;  8 0097…     0     0     0     1     0     0     0     1     0     1     0     0\n#&gt;  9 0111…     0     1     1     0     0     0     0     1     0     1     1     0\n#&gt; 10 0121…     0     1     0     0     0     0     0     1     1     1     1     0\n#&gt; # ℹ 980 more rows\n#&gt; # ℹ 15 more variables: `10a` &lt;int&gt;, `10b` &lt;int&gt;, `10c` &lt;int&gt;, `11` &lt;int&gt;,\n#&gt; #   `12` &lt;int&gt;, `13` &lt;int&gt;, `14` &lt;int&gt;, `15a` &lt;int&gt;, `15b` &lt;int&gt;, `15c` &lt;int&gt;,\n#&gt; #   `16` &lt;int&gt;, `17` &lt;int&gt;, `18` &lt;int&gt;, `21` &lt;int&gt;, `22` &lt;int&gt;\n\ndtmr_qmatrix\n#&gt; # A tibble: 27 × 5\n#&gt;    item  referent_units partitioning_iterating appropriateness\n#&gt;    &lt;chr&gt;          &lt;dbl&gt;                  &lt;dbl&gt;           &lt;dbl&gt;\n#&gt;  1 1                  1                      0               0\n#&gt;  2 2                  0                      0               1\n#&gt;  3 3                  0                      1               0\n#&gt;  4 4                  1                      0               0\n#&gt;  5 5                  1                      0               0\n#&gt;  6 6                  0                      1               0\n#&gt;  7 7                  1                      0               0\n#&gt;  8 8a                 0                      0               1\n#&gt;  9 8b                 0                      0               1\n#&gt; 10 8c                 0                      0               1\n#&gt; # ℹ 17 more rows\n#&gt; # ℹ 1 more variable: multiplicative_comparison &lt;dbl&gt;\n\nBecause the data is simulated we have access to the “true” values for respondents and items. The class probabilities of respondents belonging to a given proficiency pattern are reported in Izsák et al. (2019). Using these probabilities, we generated a class for each of the 990 respondents, which is available in dtmr_true_profiles.\n\ndtmr_true_structural\n#&gt; # A tibble: 16 × 5\n#&gt;    referent_units partitioning_iterating appropriateness multiplicative_compar…¹\n#&gt;             &lt;dbl&gt;                  &lt;dbl&gt;           &lt;dbl&gt;                   &lt;dbl&gt;\n#&gt;  1              0                      0               0                       0\n#&gt;  2              1                      0               0                       0\n#&gt;  3              0                      1               0                       0\n#&gt;  4              0                      0               1                       0\n#&gt;  5              0                      0               0                       1\n#&gt;  6              1                      1               0                       0\n#&gt;  7              1                      0               1                       0\n#&gt;  8              1                      0               0                       1\n#&gt;  9              0                      1               1                       0\n#&gt; 10              0                      1               0                       1\n#&gt; 11              0                      0               1                       1\n#&gt; 12              1                      1               1                       0\n#&gt; 13              1                      1               0                       1\n#&gt; 14              1                      0               1                       1\n#&gt; 15              0                      1               1                       1\n#&gt; 16              1                      1               1                       1\n#&gt; # ℹ abbreviated name: ¹​multiplicative_comparison\n#&gt; # ℹ 1 more variable: class_probability &lt;dbl&gt;\n\ndtmr_true_profiles\n#&gt; # A tibble: 990 × 5\n#&gt;    id     referent_units partitioning_iterating appropriateness\n#&gt;    &lt;fct&gt;           &lt;dbl&gt;                  &lt;dbl&gt;           &lt;dbl&gt;\n#&gt;  1 039517              0                      0               0\n#&gt;  2 500170              0                      0               0\n#&gt;  3 104795              1                      1               1\n#&gt;  4 113558              1                      1               1\n#&gt;  5 564266              0                      1               1\n#&gt;  6 039726              1                      1               1\n#&gt;  7 075968              0                      1               1\n#&gt;  8 375846              0                      0               0\n#&gt;  9 032129              0                      0               0\n#&gt; 10 138501              0                      1               0\n#&gt; # ℹ 980 more rows\n#&gt; # ℹ 1 more variable: multiplicative_comparison &lt;dbl&gt;\n\nThe item parameters for the LCDM are reported in Table 1 of Bradshaw et al. (2014). Using the item parameters and the true profiles, we can calculate the probability that each simulated respondent provides a correct response to each item. These probabilities are then used to generate the simulated item responses in dtmr_data.\n\ndtmr_true_items\n#&gt; # A tibble: 27 × 7\n#&gt;    item  intercept referent_units partitioning_iterating appropriateness\n#&gt;    &lt;chr&gt;     &lt;dbl&gt;          &lt;dbl&gt;                  &lt;dbl&gt;           &lt;dbl&gt;\n#&gt;  1 1         -1.12           2.24                  NA              NA   \n#&gt;  2 2          0.59          NA                     NA               1.27\n#&gt;  3 3         -2.07          NA                      1.7            NA   \n#&gt;  4 4         -1.19           0.65                  NA              NA   \n#&gt;  5 5         -1.67           1.52                  NA              NA   \n#&gt;  6 6         -3.81          NA                      2.08           NA   \n#&gt;  7 7         -0.73           1.2                   NA              NA   \n#&gt;  8 8a        -0.62          NA                     NA               4.25\n#&gt;  9 8b        -0.09          NA                     NA               2.16\n#&gt; 10 8c         0.28          NA                     NA               0.87\n#&gt; # ℹ 17 more rows\n#&gt; # ℹ 2 more variables: multiplicative_comparison &lt;dbl&gt;,\n#&gt; #   referent_units__partitioning_iterating &lt;dbl&gt;\n\nFor a complete description of how the data was simulated, see ?dtmr."
  },
  {
    "objectID": "blog/2025-08-dcmdata-0.1.0/index.html#future-work",
    "href": "blog/2025-08-dcmdata-0.1.0/index.html#future-work",
    "title": "dcmdata 0.1.0",
    "section": "Future work",
    "text": "Future work\nFuture work will focus on providing tools for simulating data from a variety of DCMs. The goal is to provide a tool that will make it easier to quickly simulate a large number a data sets, as is often required for simulation studies.\nWe also plan to continue adding more real data sets (e.g., from the Item Response Warehouse). If you know of any data sets that would be a good fit, or have a data set you’d like to contribute yourself, please open an issue!"
  },
  {
    "objectID": "blog/2025-08-dcmdata-0.1.0/index.html#acknowledgments",
    "href": "blog/2025-08-dcmdata-0.1.0/index.html#acknowledgments",
    "title": "dcmdata 0.1.0",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThe research reported here was supported by the Institute of Education Sciences, U.S. Department of Education, through Grants R305D210045 and R305D240032 to the University of Kansas Center for Research, Inc., ATLAS. The opinions expressed are those of the authors and do not represent the views of the the Institute or the U.S. Department of Education.\nFeatured photo by Scott Graham on Unsplash."
  },
  {
    "objectID": "blog/2023-06-measr-0.3.1/index.html",
    "href": "blog/2023-06-measr-0.3.1/index.html",
    "title": "measr 0.3.1",
    "section": "",
    "text": "We’re stoked to announce the release of measr 0.3.1. The goal of measr is to provide applied researchers and psychometricians with a user friendly interface for estimating and evaluating diagnostic classification models (DCMs). This update is a minor release to introduce a couple of enhancements to model and prior specifications.\nYou can install the update version of measr from CRAN with:\ninstall.packages(\"measr\")\nThis blog post will highlight the two enhancements included in the update. For a complete list of changes, check out the changelog.\nlibrary(measr)"
  },
  {
    "objectID": "blog/2023-06-measr-0.3.1/index.html#model-specifications",
    "href": "blog/2023-06-measr-0.3.1/index.html#model-specifications",
    "title": "measr 0.3.1",
    "section": "Model Specifications",
    "text": "Model Specifications\nIn this version, support for a new DCM subtype was added. The compensatory reparameterized unified model (C-RUM) model can now be specified in measr_dcm() with type = \"crum\". The C-RUM is similar to the log-linear cognitive diagnostic model (LCDM), except the C-RUM estimates only item-level intercepts and main effects (i.e., no interactions when multiple attributes are measured by an item). Because of this, along with the addition of the C-RUM, the LCDM now has additional flexibility through the new max_interaction argument. When using type = \"lcdm\", max_interaction specifies the highest level interaction to be estimated. For example, setting max_interaction = 2 would estimate the LCDM with only intercepts, main effects, and two-way interactions. If an item measures 3 or more attributes, the 3-way interactions and higher will be excluded. Specifying type = \"lcdm\" with max_interaction = 1 is equivalent to the C-RUM, as 1 indicates that main effects are the highest-level interaction to be estimated.\nThis version also introduces new options for the structural component of the DCMs. Currently two attributes structures are possible, and are defined through the attribute_structure argument. The first is attribute_structure = \"unconstrained\". This is the default, which makes no assumptions about the relationships between attributes. Specifying and unconstrained structural model is equivalent to the saturated structural model described by Hu & Templin (2020) and in Chapter 8 of Rupp et al. (2010). The other option currently supported is attribute_structure = \"independent\". When an independent attribute structure is specified, the proficiency or the presence of one attribute is independent of other attributes. Future development will include additional attribute structure specifications such as a reduced loglinear model (e.g., Thompson, 2018) or a Bayesian Network (e.g., Hu & Templin, 2020; Martinez & Templin, 2023)."
  },
  {
    "objectID": "blog/2023-06-measr-0.3.1/index.html#prior-specifications",
    "href": "blog/2023-06-measr-0.3.1/index.html#prior-specifications",
    "title": "measr 0.3.1",
    "section": "Prior Specifications",
    "text": "Prior Specifications\nThere are two main improvements to prior specifications included in this release. First, custom prior distributions can be specified for the structural model parameters. We can view the default parameters for each attribute structure specification with:\n\ndefault_dcm_priors(type = \"lcdm\", attribute_structure = \"unconstrained\")\n#&gt; # A tibble: 4 × 3\n#&gt;   class       coef  prior_def                  \n#&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;                      \n#&gt; 1 intercept   &lt;NA&gt;  normal(0, 2)               \n#&gt; 2 maineffect  &lt;NA&gt;  lognormal(0, 1)            \n#&gt; 3 interaction &lt;NA&gt;  normal(0, 2)               \n#&gt; 4 structural  Vc    dirichlet(rep_vector(1, C))\n\ndefault_dcm_priors(type = \"lcdm\", attribute_structure = \"independent\")\n#&gt; # A tibble: 4 × 3\n#&gt;   class       coef  prior_def      \n#&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;          \n#&gt; 1 intercept   &lt;NA&gt;  normal(0, 2)   \n#&gt; 2 maineffect  &lt;NA&gt;  lognormal(0, 1)\n#&gt; 3 interaction &lt;NA&gt;  normal(0, 2)   \n#&gt; 4 structural  &lt;NA&gt;  beta(1, 1)\n\nWe can also view the specific parameters available for a specific model using the get_parameters() function. For example, using the ECPE Q-matrix, we can see the available parameters for each type of model.\n\nlibrary(tidyverse)\n\nget_parameters(ecpe_qmatrix, attribute_structure = \"unconstrained\") |&gt; \n  filter(class == \"structural\")\n#&gt; # A tibble: 1 × 4\n#&gt;   item_id class      attributes coef  \n#&gt;     &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;glue&gt;\n#&gt; 1      NA structural &lt;NA&gt;       Vc\n\nget_parameters(ecpe_qmatrix, attribute_structure = \"independent\") |&gt; \n  filter(class == \"structural\")\n#&gt; # A tibble: 3 × 4\n#&gt;   item_id class      attributes coef  \n#&gt;     &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;glue&gt;\n#&gt; 1      NA structural &lt;NA&gt;       eta[1]\n#&gt; 2      NA structural &lt;NA&gt;       eta[2]\n#&gt; 3      NA structural &lt;NA&gt;       eta[3]\n\nThe second improvement is additional checking of user-specified priors. Specifically, measr_dcm() will now throw and error if we try to specify a prior for a class or coefficient that is inconsistent with our chosen DCM. For example, in the previous example of structural model priors that there are different parameters depending on whether an unconstrained or independent structure is specified. If we try to define a prior for eta[1], which is only relevant for an independent structure, but an unconstrained structure is specified, we get an error.\n\nmeasr_dcm(data = ecpe_data, qmatrix = ecpe_qmatrix,\n          resp_id = \"resp_id\", item_id = \"item_id\",\n          attribute_structure = \"unconstrained\",\n          prior = prior(beta(5,17), class = \"structural\", coef = \"eta[1]\"))\n#&gt; Error in `abort_bad_argument()`:\n#&gt; ! Prior for parameter `eta[1]` with class `structural` is not relevant for the chosen model or specified Q-matrix. See `?get_parameters()` for a list of relevant parameters.\n\nAs always, please open an issue with any bugs or feature requests for future development!"
  },
  {
    "objectID": "blog/2023-04-measr-0.2.1/index.html",
    "href": "blog/2023-04-measr-0.2.1/index.html",
    "title": "measr 0.2.1",
    "section": "",
    "text": "We’re excited to announce the release of measr 0.2.1. The goal of measr is to provide applied researchers and psychometricians with a user friendly interface for estimating and evaluating diagnostic classification models (DCMs). DCMs are a class of psychometric models that provide classification of students into profiles of proficiency on a predefined set of knowledge, skills, or understandings. This offers many advantages over traditional assessment models. Because results are reported as proficiency on individual skills, teachers, students, and parents get actionable feedback about which areas could use additional attention. However, these models have not been widely used in applied or operational settings, in part due to a lack of user friendly software for estimating and evaluating these models. measr aims to bridge this gap between psychometric theory and applied practice.\nYou can install it from CRAN with:\ninstall.packages(\"measr\")\nThis blog post will highlight the main features of the package.\nlibrary(measr)"
  },
  {
    "objectID": "blog/2023-04-measr-0.2.1/index.html#estimating-dcms",
    "href": "blog/2023-04-measr-0.2.1/index.html#estimating-dcms",
    "title": "measr 0.2.1",
    "section": "Estimating DCMs",
    "text": "Estimating DCMs\nTo illustrate DCMs and their application with measr, we’ll use a simulated data set. In this data set, we have 1,000 respondents who each answered 15 questions about addition, multiplication, and fractions. For each item, a 1 represents a correct response to the item, and a 0 represents an incorrect response.\n\nlibrary(tidyverse)\n\ndat &lt;- read_csv(\"data/example-data.csv\")\ndat\n#&gt; # A tibble: 1,000 × 16\n#&gt;    resp_id item_01 item_02 item_03 item_04 item_05 item_06 item_07 item_08\n#&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1       1       1       1       1       0       1       1       0       1\n#&gt;  2       2       1       1       0       0       0       1       0       1\n#&gt;  3       3       1       1       0       0       0       1       0       1\n#&gt;  4       4       1       0       1       0       1       0       1       1\n#&gt;  5       5       1       1       0       0       0       1       1       0\n#&gt;  6       6       1       1       0       0       0       1       0       1\n#&gt;  7       7       1       1       1       0       0       1       1       1\n#&gt;  8       8       0       0       0       0       0       0       0       1\n#&gt;  9       9       0       1       0       0       0       1       0       0\n#&gt; 10      10       1       1       0       1       1       1       0       1\n#&gt; # ℹ 990 more rows\n#&gt; # ℹ 7 more variables: item_09 &lt;dbl&gt;, item_10 &lt;dbl&gt;, item_11 &lt;dbl&gt;,\n#&gt; #   item_12 &lt;dbl&gt;, item_13 &lt;dbl&gt;, item_14 &lt;dbl&gt;, item_15 &lt;dbl&gt;\n\nWhen using DCMs, a Q-matrix defines which items measure each attribute, or skill. In the Q-matrix, a 1 indicates that the item measures the attributes, and a 0 indicates the item does not measure the attribute.\n\nqmatrix &lt;- read_csv(\"data/example-qmatrix.csv\")\nqmatrix\n#&gt; # A tibble: 15 × 4\n#&gt;    item_id addition multiplication fractions\n#&gt;    &lt;chr&gt;      &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 item_01        1              1         0\n#&gt;  2 item_02        1              0         0\n#&gt;  3 item_03        0              1         0\n#&gt;  4 item_04        0              0         1\n#&gt;  5 item_05        0              1         0\n#&gt;  6 item_06        1              0         0\n#&gt;  7 item_07        0              1         0\n#&gt;  8 item_08        0              1         1\n#&gt;  9 item_09        0              1         1\n#&gt; 10 item_10        0              0         1\n#&gt; 11 item_11        1              1         0\n#&gt; 12 item_12        1              0         1\n#&gt; 13 item_13        0              0         1\n#&gt; 14 item_14        0              1         1\n#&gt; 15 item_15        1              0         0\n\nOur goal is to estimate whether each respondent is proficient in each of the skills measured by the assessment, and DCMs are our tool. There are many different types of DCMs, each with different constraints or assumptions about how the attributes relate to each other. For example, if an item measures multiple attributes, does a respondent need to be proficient on all attributes to answer the item correctly? Or can proficiency in one attribute compensate for the lack of proficiency on another?\nAlso several DCM subtypes are supported by measr, we are primarily focused on the loglinear cognitive diagnostic model (LCDM), which is a general DCM. This means that it subsumes many of the other DCM subtypes and allows for the data to determine how much (if at all) one attribute might compensate for another.\nmeasr works by wrapping the Stan language to estimate DCMs. We can estimate the LCDM using measr_dcm(). This function use the inputs to write a Stan script defining the model and then estimates the model using either the rstan or cmdstanr packages. To estimate the model, we just supply our data set and the Q-matrix. Because our data set and Q-matrix contain respondent and item identifiers, respectively, we need to specify the names of the identifier columns. We can also specify prior distributions for each of the model parameters. Finally, we specify a file for saving the model once it is estimated. For more information on model estimation, including details on specifying prior distributions, see the model estimation vignette.\n\nlcdm &lt;- measr_dcm(data = dat, qmatrix = qmatrix,\n                  resp_id = \"resp_id\", item_id = \"item_id\", \n                  prior = c(prior(normal(0, 2), class = \"intercept\"),\n                            prior(lognormal(0, 1), class = \"maineffect\"),\n                            prior(normal(0, 2), class = \"interaction\")),\n                  file = \"fits/example-lcdm\")\n\nOnce the model has estimated, we can use measr_extract() to examine different aspects of the model, such as proportion of respondents with each pattern of proficiency across the attributes.\n\nmeasr_extract(lcdm, \"strc_param\")\n#&gt; # A tibble: 8 × 2\n#&gt;   class          estimate\n#&gt;   &lt;chr&gt;        &lt;rvar[1d]&gt;\n#&gt; 1 [0,0,0]  0.165 ± 0.0127\n#&gt; 2 [1,0,0]  0.211 ± 0.0153\n#&gt; 3 [0,1,0]  0.048 ± 0.0089\n#&gt; 4 [0,0,1]  0.072 ± 0.0090\n#&gt; 5 [1,1,0]  0.166 ± 0.0164\n#&gt; 6 [1,0,1]  0.065 ± 0.0115\n#&gt; 7 [0,1,1]  0.084 ± 0.0116\n#&gt; 8 [1,1,1]  0.187 ± 0.0164"
  },
  {
    "objectID": "blog/2023-04-measr-0.2.1/index.html#evaluating-dcms",
    "href": "blog/2023-04-measr-0.2.1/index.html#evaluating-dcms",
    "title": "measr 0.2.1",
    "section": "Evaluating DCMs",
    "text": "Evaluating DCMs\nThere are several functions included for evaluating the model. For example we can examine the probability that each respondent is proficient in each attribute. Here, respondent 1 is likely proficient in all skills, whereas respondent 2 is likely only proficient in addition.\n\nlcdm &lt;- add_respondent_estimates(lcdm)\nmeasr_extract(lcdm, \"attribute_prob\")\n#&gt; # A tibble: 1,000 × 4\n#&gt;    resp_id addition multiplication fractions\n#&gt;    &lt;fct&gt;      &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 1       0.997          0.977      0.920  \n#&gt;  2 2       1.00           0.00297    0.0102 \n#&gt;  3 3       1.00           0.133      0.0370 \n#&gt;  4 4       0.0228         1.00       0.0556 \n#&gt;  5 5       0.999          0.0253     0.0223 \n#&gt;  6 6       1.00           0.00297    0.00224\n#&gt;  7 7       0.998          0.469      0.938  \n#&gt;  8 8       0.000520       0.0233     0.0194 \n#&gt;  9 9       0.984          0.000116   0.00641\n#&gt; 10 10      0.997          0.920      0.957  \n#&gt; # ℹ 990 more rows\n\nOften, we would dichotomize these probabilities into a 0/1 decision (e.g., proficient/not proficient). We can also look at the reliability of those classifications. The add_reliability() function will calculate the classification consistency and accuracy metrics described by Johnson & Sinharay (2018). Here we see that all of the attributes have fairly high accuracy and consistency for classifications.\n\nlcdm &lt;- add_reliability(lcdm)\nmeasr_extract(lcdm, \"classification_reliability\")\n#&gt; # A tibble: 3 × 3\n#&gt;   attribute      accuracy consistency\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 addition          0.960       0.927\n#&gt; 2 multiplication    0.956       0.917\n#&gt; 3 fractions         0.936       0.882\n\nAs you can see, for each type of model evaluation, we follow the same process of add_{metric} and then we can use measr_extract() to view the results. For a complete list of model evaluation options, see ?model_evaluation."
  },
  {
    "objectID": "blog/2023-04-measr-0.2.1/index.html#future-development",
    "href": "blog/2023-04-measr-0.2.1/index.html#future-development",
    "title": "measr 0.2.1",
    "section": "Future Development",
    "text": "Future Development\nWe’re already in the process of adding features and making improvements for the next version of measr. Some highlights include:\n\nAdding support for more DCM subtypes\nMore refined prior specifications\nAdditional vignettes including a complete description of model evaluation and example case studies\n\nIf you have a specific feature request, suggestion for improvement, or run into any problems, please open an issue on the project repository!"
  },
  {
    "objectID": "blog/2023-04-measr-0.2.1/index.html#acknowledgments",
    "href": "blog/2023-04-measr-0.2.1/index.html#acknowledgments",
    "title": "measr 0.2.1",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThe research reported here was supported by the Institute of Education Sciences, U.S. Department of Education, through Grant R305D210045 to the University of Kansas. The opinions expressed are those of the authors and do not represent the views of the the Institute or the U.S. Department of Education.\nFeatured photo by Fleur on Unsplash."
  },
  {
    "objectID": "blog/2024-02-measr-1.0.0/index.html",
    "href": "blog/2024-02-measr-1.0.0/index.html",
    "title": "measr 1.0.0",
    "section": "",
    "text": "We are thrilled to announce the release of measr 1.0.0. The goal of measr is to provide a user-friendly interface for estimating and evaluating diagnostic classification models (DCMs; also called cognitive diagnostic models [CDMs]). This is a major release to mark the conclusion of the initial development work that was funded by the Institute of Education Sciences. Importantly, this does not mean that measr is going dormant! We are still actively developing measr, so we’ll continue to add new features and respond to bug reports.\nYou can install the updated version of measr from CRAN with:\nThis blog post will highlight the major improvements, including improved documentation and vignettes, as well as a some minor updates.\nlibrary(measr)\n#&gt; \n#&gt; Attaching package: 'measr'\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     optim"
  },
  {
    "objectID": "blog/2024-02-measr-1.0.0/index.html#improved-documentation",
    "href": "blog/2024-02-measr-1.0.0/index.html#improved-documentation",
    "title": "measr 1.0.0",
    "section": "Improved Documentation",
    "text": "Improved Documentation\nThe focus of recent work on measr has been to improve documentation. To that end, the existing vignettes have been updated, and several new vignettes have been added.\nFirst, the getting started vignette has been updated to include additional installation information. This should help reduce the friction for setting up the components needed for measr, namely, installing Stan. Because measr interfaces with Stan to estimate DCMs, a working Stan installation is needed for the package to function properly. The updated vignette includes helpful guidance for installing both the rstan and cmdstanr packages for using Stan. Only one of the two is required, but both are supported so you can choose whichever Stan front end you prefer.\nA new vignette on model evaluation has been added to demonstrate different methods for assessing the performance of a DCM after it has been estimated. This includes the M2 statistic (Liu et al., 2016), as well as posterior predictive model checks for evaluating both overall and item-level model fit (Sinharay & Almond, 2007; thompson2019?). To demonstrate how these methods are implemented for measr, a simulated data set is used where we know how well different types of DCMs should fit. The existing model estimation vignette was also updated to use this same simulated data set so that we can compare parameter estimates derived from measr to the known parameter values that were used to generate the data.\nBecause the model estimation and evaluation vignettes now use simulated data, we also added a case study to walk through a DCM-based analysis from start to finish using a real data set. In this case study, data from the Examination for the Certificate of Proficiency in English (ECPE), which has been widely used in the DCM literature (e.g., Templin & Hoffman, 2013). We start with exploratory analyses, then we estimate a DCM, examine parameter estimates, and interpret model fit and reliability analyses.\nFinally, example uses have been added for all functions included in measr. These examples can be found on the documentation pages for each function. All functions are indexed on the reference page, which has been reorganized to group functions with related functionality."
  },
  {
    "objectID": "blog/2024-02-measr-1.0.0/index.html#minor-changes",
    "href": "blog/2024-02-measr-1.0.0/index.html#minor-changes",
    "title": "measr 1.0.0",
    "section": "Minor Changes",
    "text": "Minor Changes\nThere were also a number of minor improvements:\n\nThe Stan code for estimating models has been updated to be compatible with the new array syntax.\nWe published an article on measr in the Journal of Open Source Software (Thompson, 2023), and the preferred citation when using measr has been updated.\n\nFor a complete list of changes, check out the changelog."
  },
  {
    "objectID": "blog/2024-02-measr-1.0.0/index.html#additional-resources",
    "href": "blog/2024-02-measr-1.0.0/index.html#additional-resources",
    "title": "measr 1.0.0",
    "section": "Additional Resources",
    "text": "Additional Resources\nIf you are interested in learing more about measr and diagnostic models, we hosted a workshop in June at StanCon 2023. The workshop provided an overview of diagnostic models and their use cases and then walked through how to estimate and evaluate different models using measr. All of the workshop materials are available online, including slides, exercises, and solutions.\nFinally, if you are interested in attending a workshop in person, we’ll be teaching another workshop in April at the 2024 annual meeting of the National Conference on Measurement in Education (NCME). The materials from this workshop will also be made available online for those who are unable to attend. For more information on the conference and register, see the conference website."
  },
  {
    "objectID": "blog/2024-02-measr-1.0.0/index.html#acknowledgments",
    "href": "blog/2024-02-measr-1.0.0/index.html#acknowledgments",
    "title": "measr 1.0.0",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThe research reported here was supported by the Institute of Education Sciences, U.S. Department of Education, through Grant R305D210045 to the University of Kansas. The opinions expressed are those of the authors and do not represent the views of the the Institute or the U.S. Department of Education.\nFeatured photo by patricia serna on Unsplash."
  },
  {
    "objectID": "blog/2025-11-dcmstan-0.1.0/index.html",
    "href": "blog/2025-11-dcmstan-0.1.0/index.html",
    "title": "dcmstan 0.1.0",
    "section": "",
    "text": "We are very pleased to announce the release of a new package, dcmstan. The goal of dcmstan is to provide users with a friendly interface for creating Stan scripts necessary for estimating diagnostic classification models (DCMs; also called cognitive diagnostic models [CDMs]). dcmstan is primarily intended to serve as a backend for {measr}, which will interface with {rstan} or {cmdstanr} to actually estimate a DCM. However, dcmstan can also be used independently if you want finer control of the estimation process or would like to customize the Stan script used for estimation.\nYou can install dcmstan from CRAN with:\nThis blog post will highlight the major features and describe how dcmstan fits into the larger universe of r-dcm packages.\nlibrary(dcmstan)\n#&gt; Warning: package 'dcmstan' was built under R version 4.5.2"
  },
  {
    "objectID": "blog/2025-11-dcmstan-0.1.0/index.html#dcm-specification",
    "href": "blog/2025-11-dcmstan-0.1.0/index.html#dcm-specification",
    "title": "dcmstan 0.1.0",
    "section": "DCM specification",
    "text": "DCM specification\nFor this example, we’ll create a specification for a DCM that we want to fit to ECPE data, which is available in the {dcmdata} package.\n\nlibrary(dcmdata)\n\necpe_qmatrix\n#&gt; # A tibble: 28 × 4\n#&gt;    item_id morphosyntactic cohesive lexical\n#&gt;    &lt;chr&gt;             &lt;int&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt;  1 E1                    1        1       0\n#&gt;  2 E2                    0        1       0\n#&gt;  3 E3                    1        0       1\n#&gt;  4 E4                    0        0       1\n#&gt;  5 E5                    0        0       1\n#&gt;  6 E6                    0        0       1\n#&gt;  7 E7                    1        0       1\n#&gt;  8 E8                    0        1       0\n#&gt;  9 E9                    0        0       1\n#&gt; 10 E10                   1        0       0\n#&gt; # ℹ 18 more rows\n\necpe_data\n#&gt; # A tibble: 2,922 × 29\n#&gt;    resp_id    E1    E2    E3    E4    E5    E6    E7    E8    E9   E10   E11\n#&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt;  1       1     1     1     1     0     1     1     1     1     1     1     1\n#&gt;  2       2     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  3       3     1     1     1     1     1     1     0     1     1     1     1\n#&gt;  4       4     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  5       5     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  6       6     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  7       7     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  8       8     0     1     1     1     1     1     0     1     1     1     0\n#&gt;  9       9     1     1     1     1     1     1     1     1     1     1     1\n#&gt; 10      10     1     1     1     1     0     0     1     1     1     1     1\n#&gt; # ℹ 2,912 more rows\n#&gt; # ℹ 17 more variables: E12 &lt;int&gt;, E13 &lt;int&gt;, E14 &lt;int&gt;, E15 &lt;int&gt;, E16 &lt;int&gt;,\n#&gt; #   E17 &lt;int&gt;, E18 &lt;int&gt;, E19 &lt;int&gt;, E20 &lt;int&gt;, E21 &lt;int&gt;, E22 &lt;int&gt;,\n#&gt; #   E23 &lt;int&gt;, E24 &lt;int&gt;, E25 &lt;int&gt;, E26 &lt;int&gt;, E27 &lt;int&gt;, E28 &lt;int&gt;\n\nWe can create a DCM specification using dcm_specify(). We must provide our Q-matrix and, if present, the name of the Q-matrix column that contains the item identifiers. We then must choose a measurement and structural model to be used. By default, dcm_specify() will fit a loglinear cognitive diagnostic model (LCDM; Henson et al., 2009; Henson & Templin, 2019) with an unconstrained structural model. These are the measurement models used by Templin & Hoffman (2013) in their examination of the ECPE data, so we will echo those choices here. However, there are many other measurement and structural models we could choose from. For details on the specification options, see vignette(\"dcmstan\", package = \"dcmstan\").\n\necpe_spec &lt;- dcm_specify(\n  qmatrix = ecpe_qmatrix,\n  identifier = \"item_id\",\n  measurement_model = lcdm(),\n  structural_model = unconstrained()\n)\n\necpe_spec\n#&gt; A loglinear cognitive diagnostic model (LCDM) measuring 3 attributes with 28\n#&gt; items.\n#&gt; \n#&gt; ℹ Attributes:\n#&gt; • \"morphosyntactic\" (13 items)\n#&gt; • \"cohesive\" (6 items)\n#&gt; • \"lexical\" (18 items)\n#&gt; \n#&gt; ℹ Attribute structure:\n#&gt;   Unconstrained\n#&gt; \n#&gt; ℹ Prior distributions:\n#&gt;   intercept ~ normal(0, 2)\n#&gt;   maineffect ~ lognormal(0, 1)\n#&gt;   interaction ~ normal(0, 2)\n#&gt;   `Vc` ~ dirichlet(1, 1, 1)\n\nWe can also specify prior distributions for the model. Reasonable priors are defined by default, but custom priors can be specified for specific parameters or an entire type of parameters (e.g., applies to all intercept parameters). For a list of possible parameters for given specification, see get_parameters().\n\nget_parameters(ecpe_spec)\n#&gt; # A tibble: 75 × 4\n#&gt;    item_id type        attributes                coefficient\n#&gt;    &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;                     &lt;chr&gt;      \n#&gt;  1 E1      intercept   &lt;NA&gt;                      l1_0       \n#&gt;  2 E1      maineffect  morphosyntactic           l1_11      \n#&gt;  3 E1      maineffect  cohesive                  l1_12      \n#&gt;  4 E1      interaction morphosyntactic__cohesive l1_212     \n#&gt;  5 E2      intercept   &lt;NA&gt;                      l2_0       \n#&gt;  6 E2      maineffect  cohesive                  l2_12      \n#&gt;  7 E3      intercept   &lt;NA&gt;                      l3_0       \n#&gt;  8 E3      maineffect  morphosyntactic           l3_11      \n#&gt;  9 E3      maineffect  lexical                   l3_13      \n#&gt; 10 E3      interaction morphosyntactic__lexical  l3_213     \n#&gt; # ℹ 65 more rows"
  },
  {
    "objectID": "blog/2025-11-dcmstan-0.1.0/index.html#stan-code-and-data",
    "href": "blog/2025-11-dcmstan-0.1.0/index.html#stan-code-and-data",
    "title": "dcmstan 0.1.0",
    "section": "Stan code and data",
    "text": "Stan code and data\nOnce we have our specification, we can create the Stan code for estimating the model with stan_code(). This code can either be passed directly to the model_code argument of rstan::stan(), or written to a .stan file, which can then be passed to the file argument of rstan::stan() or the stan_file argument of cmdstanr::cmdstan_model().\n\nstan_code(ecpe_spec)\n\n#&gt; data {\n#&gt;   int&lt;lower=1&gt; I;                      // number of items\n#&gt;   int&lt;lower=1&gt; R;                      // number of respondents\n#&gt;   int&lt;lower=1&gt; N;                      // number of observations\n#&gt;   int&lt;lower=1&gt; C;                      // number of classes\n#&gt;   array[N] int&lt;lower=1,upper=I&gt; ii;    // item for observation n\n#&gt;   array[N] int&lt;lower=1,upper=R&gt; rr;    // respondent for observation n\n#&gt;   array[N] int&lt;lower=0,upper=1&gt; y;     // score for observation n\n#&gt;   array[R] int&lt;lower=1,upper=N&gt; start; // starting row for respondent R\n#&gt;   array[R] int&lt;lower=1,upper=I&gt; num;   // number items for respondent R\n#&gt; }\n#&gt; parameters {\n#&gt;   simplex[C] Vc;\n#&gt; \n#&gt;   ////////////////////////////////// item intercepts\n#&gt;   real l1_0;\n#&gt;   real l2_0;\n#&gt;   real l3_0;\n#&gt;   real l4_0;\n#&gt;   real l5_0;\n#&gt;   real l6_0;\n#&gt;   real l7_0;\n#&gt;   real l8_0;\n#&gt;   real l9_0;\n#&gt;   real l10_0;\n#&gt;   real l11_0;\n#&gt;   real l12_0;\n#&gt;   real l13_0;\n#&gt;   real l14_0;\n#&gt;   real l15_0;\n#&gt;   real l16_0;\n#&gt;   real l17_0;\n#&gt;   real l18_0;\n#&gt;   real l19_0;\n#&gt;   real l20_0;\n#&gt;   real l21_0;\n#&gt;   real l22_0;\n#&gt;   real l23_0;\n#&gt;   real l24_0;\n#&gt;   real l25_0;\n#&gt;   real l26_0;\n#&gt;   real l27_0;\n#&gt;   real l28_0;\n#&gt; \n#&gt;   ////////////////////////////////// item main effects\n#&gt;   real&lt;lower=0&gt; l1_11;\n#&gt;   real&lt;lower=0&gt; l1_12;\n#&gt;   real&lt;lower=0&gt; l2_12;\n#&gt;   real&lt;lower=0&gt; l3_11;\n#&gt;   real&lt;lower=0&gt; l3_13;\n#&gt;   real&lt;lower=0&gt; l4_13;\n#&gt;   real&lt;lower=0&gt; l5_13;\n#&gt;   real&lt;lower=0&gt; l6_13;\n#&gt;   real&lt;lower=0&gt; l7_11;\n#&gt;   real&lt;lower=0&gt; l7_13;\n#&gt;   real&lt;lower=0&gt; l8_12;\n#&gt;   real&lt;lower=0&gt; l9_13;\n#&gt;   real&lt;lower=0&gt; l10_11;\n#&gt;   real&lt;lower=0&gt; l11_11;\n#&gt;   real&lt;lower=0&gt; l11_13;\n#&gt;   real&lt;lower=0&gt; l12_11;\n#&gt;   real&lt;lower=0&gt; l12_13;\n#&gt;   real&lt;lower=0&gt; l13_11;\n#&gt;   real&lt;lower=0&gt; l14_11;\n#&gt;   real&lt;lower=0&gt; l15_13;\n#&gt;   real&lt;lower=0&gt; l16_11;\n#&gt;   real&lt;lower=0&gt; l16_13;\n#&gt;   real&lt;lower=0&gt; l17_12;\n#&gt;   real&lt;lower=0&gt; l17_13;\n#&gt;   real&lt;lower=0&gt; l18_13;\n#&gt;   real&lt;lower=0&gt; l19_13;\n#&gt;   real&lt;lower=0&gt; l20_11;\n#&gt;   real&lt;lower=0&gt; l20_13;\n#&gt;   real&lt;lower=0&gt; l21_11;\n#&gt;   real&lt;lower=0&gt; l21_13;\n#&gt;   real&lt;lower=0&gt; l22_13;\n#&gt;   real&lt;lower=0&gt; l23_12;\n#&gt;   real&lt;lower=0&gt; l24_12;\n#&gt;   real&lt;lower=0&gt; l25_11;\n#&gt;   real&lt;lower=0&gt; l26_13;\n#&gt;   real&lt;lower=0&gt; l27_11;\n#&gt;   real&lt;lower=0&gt; l28_13;\n#&gt; \n#&gt;   ////////////////////////////////// item interactions\n#&gt;   real&lt;lower=-1 * min([l1_11,l1_12])&gt; l1_212;\n#&gt;   real&lt;lower=-1 * min([l3_11,l3_13])&gt; l3_213;\n#&gt;   real&lt;lower=-1 * min([l7_11,l7_13])&gt; l7_213;\n#&gt;   real&lt;lower=-1 * min([l11_11,l11_13])&gt; l11_213;\n#&gt;   real&lt;lower=-1 * min([l12_11,l12_13])&gt; l12_213;\n#&gt;   real&lt;lower=-1 * min([l16_11,l16_13])&gt; l16_213;\n#&gt;   real&lt;lower=-1 * min([l17_12,l17_13])&gt; l17_223;\n#&gt;   real&lt;lower=-1 * min([l20_11,l20_13])&gt; l20_213;\n#&gt;   real&lt;lower=-1 * min([l21_11,l21_13])&gt; l21_213;\n#&gt; }\n#&gt; transformed parameters {\n#&gt;   vector[C] log_Vc = log(Vc);\n#&gt;   matrix[I,C] pi;\n#&gt; \n#&gt;   ////////////////////////////////// probability of correct response\n#&gt;   pi[1,1] = inv_logit(l1_0);\n#&gt;   pi[1,2] = inv_logit(l1_0+l1_11);\n#&gt;   pi[1,3] = inv_logit(l1_0+l1_12);\n#&gt;   pi[1,4] = inv_logit(l1_0);\n#&gt;   pi[1,5] = inv_logit(l1_0+l1_11+l1_12+l1_212);\n#&gt;   pi[1,6] = inv_logit(l1_0+l1_11);\n#&gt;   pi[1,7] = inv_logit(l1_0+l1_12);\n#&gt;   pi[1,8] = inv_logit(l1_0+l1_11+l1_12+l1_212);\n#&gt;   pi[2,1] = inv_logit(l2_0);\n#&gt;   pi[2,2] = inv_logit(l2_0);\n#&gt;   pi[2,3] = inv_logit(l2_0+l2_12);\n#&gt;   pi[2,4] = inv_logit(l2_0);\n#&gt;   pi[2,5] = inv_logit(l2_0+l2_12);\n#&gt;   pi[2,6] = inv_logit(l2_0);\n#&gt;   pi[2,7] = inv_logit(l2_0+l2_12);\n#&gt;   pi[2,8] = inv_logit(l2_0+l2_12);\n#&gt;   pi[3,1] = inv_logit(l3_0);\n#&gt;   pi[3,2] = inv_logit(l3_0+l3_11);\n#&gt;   pi[3,3] = inv_logit(l3_0);\n#&gt;   pi[3,4] = inv_logit(l3_0+l3_13);\n#&gt;   pi[3,5] = inv_logit(l3_0+l3_11);\n#&gt;   pi[3,6] = inv_logit(l3_0+l3_11+l3_13+l3_213);\n#&gt;   pi[3,7] = inv_logit(l3_0+l3_13);\n#&gt;   pi[3,8] = inv_logit(l3_0+l3_11+l3_13+l3_213);\n#&gt;   pi[4,1] = inv_logit(l4_0);\n#&gt;   pi[4,2] = inv_logit(l4_0);\n#&gt;   pi[4,3] = inv_logit(l4_0);\n#&gt;   pi[4,4] = inv_logit(l4_0+l4_13);\n#&gt;   pi[4,5] = inv_logit(l4_0);\n#&gt;   pi[4,6] = inv_logit(l4_0+l4_13);\n#&gt;   pi[4,7] = inv_logit(l4_0+l4_13);\n#&gt;   pi[4,8] = inv_logit(l4_0+l4_13);\n#&gt;   pi[5,1] = inv_logit(l5_0);\n#&gt;   pi[5,2] = inv_logit(l5_0);\n#&gt;   pi[5,3] = inv_logit(l5_0);\n#&gt;   pi[5,4] = inv_logit(l5_0+l5_13);\n#&gt;   pi[5,5] = inv_logit(l5_0);\n#&gt;   pi[5,6] = inv_logit(l5_0+l5_13);\n#&gt;   pi[5,7] = inv_logit(l5_0+l5_13);\n#&gt;   pi[5,8] = inv_logit(l5_0+l5_13);\n#&gt;   pi[6,1] = inv_logit(l6_0);\n#&gt;   pi[6,2] = inv_logit(l6_0);\n#&gt;   pi[6,3] = inv_logit(l6_0);\n#&gt;   pi[6,4] = inv_logit(l6_0+l6_13);\n#&gt;   pi[6,5] = inv_logit(l6_0);\n#&gt;   pi[6,6] = inv_logit(l6_0+l6_13);\n#&gt;   pi[6,7] = inv_logit(l6_0+l6_13);\n#&gt;   pi[6,8] = inv_logit(l6_0+l6_13);\n#&gt;   pi[7,1] = inv_logit(l7_0);\n#&gt;   pi[7,2] = inv_logit(l7_0+l7_11);\n#&gt;   pi[7,3] = inv_logit(l7_0);\n#&gt;   pi[7,4] = inv_logit(l7_0+l7_13);\n#&gt;   pi[7,5] = inv_logit(l7_0+l7_11);\n#&gt;   pi[7,6] = inv_logit(l7_0+l7_11+l7_13+l7_213);\n#&gt;   pi[7,7] = inv_logit(l7_0+l7_13);\n#&gt;   pi[7,8] = inv_logit(l7_0+l7_11+l7_13+l7_213);\n#&gt;   pi[8,1] = inv_logit(l8_0);\n#&gt;   pi[8,2] = inv_logit(l8_0);\n#&gt;   pi[8,3] = inv_logit(l8_0+l8_12);\n#&gt;   pi[8,4] = inv_logit(l8_0);\n#&gt;   pi[8,5] = inv_logit(l8_0+l8_12);\n#&gt;   pi[8,6] = inv_logit(l8_0);\n#&gt;   pi[8,7] = inv_logit(l8_0+l8_12);\n#&gt;   pi[8,8] = inv_logit(l8_0+l8_12);\n#&gt;   pi[9,1] = inv_logit(l9_0);\n#&gt;   pi[9,2] = inv_logit(l9_0);\n#&gt;   pi[9,3] = inv_logit(l9_0);\n#&gt;   pi[9,4] = inv_logit(l9_0+l9_13);\n#&gt;   pi[9,5] = inv_logit(l9_0);\n#&gt;   pi[9,6] = inv_logit(l9_0+l9_13);\n#&gt;   pi[9,7] = inv_logit(l9_0+l9_13);\n#&gt;   pi[9,8] = inv_logit(l9_0+l9_13);\n#&gt;   pi[10,1] = inv_logit(l10_0);\n#&gt;   pi[10,2] = inv_logit(l10_0+l10_11);\n#&gt;   pi[10,3] = inv_logit(l10_0);\n#&gt;   pi[10,4] = inv_logit(l10_0);\n#&gt;   pi[10,5] = inv_logit(l10_0+l10_11);\n#&gt;   pi[10,6] = inv_logit(l10_0+l10_11);\n#&gt;   pi[10,7] = inv_logit(l10_0);\n#&gt;   pi[10,8] = inv_logit(l10_0+l10_11);\n#&gt;   pi[11,1] = inv_logit(l11_0);\n#&gt;   pi[11,2] = inv_logit(l11_0+l11_11);\n#&gt;   pi[11,3] = inv_logit(l11_0);\n#&gt;   pi[11,4] = inv_logit(l11_0+l11_13);\n#&gt;   pi[11,5] = inv_logit(l11_0+l11_11);\n#&gt;   pi[11,6] = inv_logit(l11_0+l11_11+l11_13+l11_213);\n#&gt;   pi[11,7] = inv_logit(l11_0+l11_13);\n#&gt;   pi[11,8] = inv_logit(l11_0+l11_11+l11_13+l11_213);\n#&gt;   pi[12,1] = inv_logit(l12_0);\n#&gt;   pi[12,2] = inv_logit(l12_0+l12_11);\n#&gt;   pi[12,3] = inv_logit(l12_0);\n#&gt;   pi[12,4] = inv_logit(l12_0+l12_13);\n#&gt;   pi[12,5] = inv_logit(l12_0+l12_11);\n#&gt;   pi[12,6] = inv_logit(l12_0+l12_11+l12_13+l12_213);\n#&gt;   pi[12,7] = inv_logit(l12_0+l12_13);\n#&gt;   pi[12,8] = inv_logit(l12_0+l12_11+l12_13+l12_213);\n#&gt;   pi[13,1] = inv_logit(l13_0);\n#&gt;   pi[13,2] = inv_logit(l13_0+l13_11);\n#&gt;   pi[13,3] = inv_logit(l13_0);\n#&gt;   pi[13,4] = inv_logit(l13_0);\n#&gt;   pi[13,5] = inv_logit(l13_0+l13_11);\n#&gt;   pi[13,6] = inv_logit(l13_0+l13_11);\n#&gt;   pi[13,7] = inv_logit(l13_0);\n#&gt;   pi[13,8] = inv_logit(l13_0+l13_11);\n#&gt;   pi[14,1] = inv_logit(l14_0);\n#&gt;   pi[14,2] = inv_logit(l14_0+l14_11);\n#&gt;   pi[14,3] = inv_logit(l14_0);\n#&gt;   pi[14,4] = inv_logit(l14_0);\n#&gt;   pi[14,5] = inv_logit(l14_0+l14_11);\n#&gt;   pi[14,6] = inv_logit(l14_0+l14_11);\n#&gt;   pi[14,7] = inv_logit(l14_0);\n#&gt;   pi[14,8] = inv_logit(l14_0+l14_11);\n#&gt;   pi[15,1] = inv_logit(l15_0);\n#&gt;   pi[15,2] = inv_logit(l15_0);\n#&gt;   pi[15,3] = inv_logit(l15_0);\n#&gt;   pi[15,4] = inv_logit(l15_0+l15_13);\n#&gt;   pi[15,5] = inv_logit(l15_0);\n#&gt;   pi[15,6] = inv_logit(l15_0+l15_13);\n#&gt;   pi[15,7] = inv_logit(l15_0+l15_13);\n#&gt;   pi[15,8] = inv_logit(l15_0+l15_13);\n#&gt;   pi[16,1] = inv_logit(l16_0);\n#&gt;   pi[16,2] = inv_logit(l16_0+l16_11);\n#&gt;   pi[16,3] = inv_logit(l16_0);\n#&gt;   pi[16,4] = inv_logit(l16_0+l16_13);\n#&gt;   pi[16,5] = inv_logit(l16_0+l16_11);\n#&gt;   pi[16,6] = inv_logit(l16_0+l16_11+l16_13+l16_213);\n#&gt;   pi[16,7] = inv_logit(l16_0+l16_13);\n#&gt;   pi[16,8] = inv_logit(l16_0+l16_11+l16_13+l16_213);\n#&gt;   pi[17,1] = inv_logit(l17_0);\n#&gt;   pi[17,2] = inv_logit(l17_0);\n#&gt;   pi[17,3] = inv_logit(l17_0+l17_12);\n#&gt;   pi[17,4] = inv_logit(l17_0+l17_13);\n#&gt;   pi[17,5] = inv_logit(l17_0+l17_12);\n#&gt;   pi[17,6] = inv_logit(l17_0+l17_13);\n#&gt;   pi[17,7] = inv_logit(l17_0+l17_12+l17_13+l17_223);\n#&gt;   pi[17,8] = inv_logit(l17_0+l17_12+l17_13+l17_223);\n#&gt;   pi[18,1] = inv_logit(l18_0);\n#&gt;   pi[18,2] = inv_logit(l18_0);\n#&gt;   pi[18,3] = inv_logit(l18_0);\n#&gt;   pi[18,4] = inv_logit(l18_0+l18_13);\n#&gt;   pi[18,5] = inv_logit(l18_0);\n#&gt;   pi[18,6] = inv_logit(l18_0+l18_13);\n#&gt;   pi[18,7] = inv_logit(l18_0+l18_13);\n#&gt;   pi[18,8] = inv_logit(l18_0+l18_13);\n#&gt;   pi[19,1] = inv_logit(l19_0);\n#&gt;   pi[19,2] = inv_logit(l19_0);\n#&gt;   pi[19,3] = inv_logit(l19_0);\n#&gt;   pi[19,4] = inv_logit(l19_0+l19_13);\n#&gt;   pi[19,5] = inv_logit(l19_0);\n#&gt;   pi[19,6] = inv_logit(l19_0+l19_13);\n#&gt;   pi[19,7] = inv_logit(l19_0+l19_13);\n#&gt;   pi[19,8] = inv_logit(l19_0+l19_13);\n#&gt;   pi[20,1] = inv_logit(l20_0);\n#&gt;   pi[20,2] = inv_logit(l20_0+l20_11);\n#&gt;   pi[20,3] = inv_logit(l20_0);\n#&gt;   pi[20,4] = inv_logit(l20_0+l20_13);\n#&gt;   pi[20,5] = inv_logit(l20_0+l20_11);\n#&gt;   pi[20,6] = inv_logit(l20_0+l20_11+l20_13+l20_213);\n#&gt;   pi[20,7] = inv_logit(l20_0+l20_13);\n#&gt;   pi[20,8] = inv_logit(l20_0+l20_11+l20_13+l20_213);\n#&gt;   pi[21,1] = inv_logit(l21_0);\n#&gt;   pi[21,2] = inv_logit(l21_0+l21_11);\n#&gt;   pi[21,3] = inv_logit(l21_0);\n#&gt;   pi[21,4] = inv_logit(l21_0+l21_13);\n#&gt;   pi[21,5] = inv_logit(l21_0+l21_11);\n#&gt;   pi[21,6] = inv_logit(l21_0+l21_11+l21_13+l21_213);\n#&gt;   pi[21,7] = inv_logit(l21_0+l21_13);\n#&gt;   pi[21,8] = inv_logit(l21_0+l21_11+l21_13+l21_213);\n#&gt;   pi[22,1] = inv_logit(l22_0);\n#&gt;   pi[22,2] = inv_logit(l22_0);\n#&gt;   pi[22,3] = inv_logit(l22_0);\n#&gt;   pi[22,4] = inv_logit(l22_0+l22_13);\n#&gt;   pi[22,5] = inv_logit(l22_0);\n#&gt;   pi[22,6] = inv_logit(l22_0+l22_13);\n#&gt;   pi[22,7] = inv_logit(l22_0+l22_13);\n#&gt;   pi[22,8] = inv_logit(l22_0+l22_13);\n#&gt;   pi[23,1] = inv_logit(l23_0);\n#&gt;   pi[23,2] = inv_logit(l23_0);\n#&gt;   pi[23,3] = inv_logit(l23_0+l23_12);\n#&gt;   pi[23,4] = inv_logit(l23_0);\n#&gt;   pi[23,5] = inv_logit(l23_0+l23_12);\n#&gt;   pi[23,6] = inv_logit(l23_0);\n#&gt;   pi[23,7] = inv_logit(l23_0+l23_12);\n#&gt;   pi[23,8] = inv_logit(l23_0+l23_12);\n#&gt;   pi[24,1] = inv_logit(l24_0);\n#&gt;   pi[24,2] = inv_logit(l24_0);\n#&gt;   pi[24,3] = inv_logit(l24_0+l24_12);\n#&gt;   pi[24,4] = inv_logit(l24_0);\n#&gt;   pi[24,5] = inv_logit(l24_0+l24_12);\n#&gt;   pi[24,6] = inv_logit(l24_0);\n#&gt;   pi[24,7] = inv_logit(l24_0+l24_12);\n#&gt;   pi[24,8] = inv_logit(l24_0+l24_12);\n#&gt;   pi[25,1] = inv_logit(l25_0);\n#&gt;   pi[25,2] = inv_logit(l25_0+l25_11);\n#&gt;   pi[25,3] = inv_logit(l25_0);\n#&gt;   pi[25,4] = inv_logit(l25_0);\n#&gt;   pi[25,5] = inv_logit(l25_0+l25_11);\n#&gt;   pi[25,6] = inv_logit(l25_0+l25_11);\n#&gt;   pi[25,7] = inv_logit(l25_0);\n#&gt;   pi[25,8] = inv_logit(l25_0+l25_11);\n#&gt;   pi[26,1] = inv_logit(l26_0);\n#&gt;   pi[26,2] = inv_logit(l26_0);\n#&gt;   pi[26,3] = inv_logit(l26_0);\n#&gt;   pi[26,4] = inv_logit(l26_0+l26_13);\n#&gt;   pi[26,5] = inv_logit(l26_0);\n#&gt;   pi[26,6] = inv_logit(l26_0+l26_13);\n#&gt;   pi[26,7] = inv_logit(l26_0+l26_13);\n#&gt;   pi[26,8] = inv_logit(l26_0+l26_13);\n#&gt;   pi[27,1] = inv_logit(l27_0);\n#&gt;   pi[27,2] = inv_logit(l27_0+l27_11);\n#&gt;   pi[27,3] = inv_logit(l27_0);\n#&gt;   pi[27,4] = inv_logit(l27_0);\n#&gt;   pi[27,5] = inv_logit(l27_0+l27_11);\n#&gt;   pi[27,6] = inv_logit(l27_0+l27_11);\n#&gt;   pi[27,7] = inv_logit(l27_0);\n#&gt;   pi[27,8] = inv_logit(l27_0+l27_11);\n#&gt;   pi[28,1] = inv_logit(l28_0);\n#&gt;   pi[28,2] = inv_logit(l28_0);\n#&gt;   pi[28,3] = inv_logit(l28_0);\n#&gt;   pi[28,4] = inv_logit(l28_0+l28_13);\n#&gt;   pi[28,5] = inv_logit(l28_0);\n#&gt;   pi[28,6] = inv_logit(l28_0+l28_13);\n#&gt;   pi[28,7] = inv_logit(l28_0+l28_13);\n#&gt;   pi[28,8] = inv_logit(l28_0+l28_13);\n#&gt; }\n#&gt; model {\n#&gt;   ////////////////////////////////// priors\n#&gt;   Vc ~ dirichlet(rep_vector(1, C));\n#&gt;   l1_0 ~ normal(0, 2);\n#&gt;   l1_11 ~ lognormal(0, 1);\n#&gt;   l1_12 ~ lognormal(0, 1);\n#&gt;   l1_212 ~ normal(0, 2);\n#&gt;   l2_0 ~ normal(0, 2);\n#&gt;   l2_12 ~ lognormal(0, 1);\n#&gt;   l3_0 ~ normal(0, 2);\n#&gt;   l3_11 ~ lognormal(0, 1);\n#&gt;   l3_13 ~ lognormal(0, 1);\n#&gt;   l3_213 ~ normal(0, 2);\n#&gt;   l4_0 ~ normal(0, 2);\n#&gt;   l4_13 ~ lognormal(0, 1);\n#&gt;   l5_0 ~ normal(0, 2);\n#&gt;   l5_13 ~ lognormal(0, 1);\n#&gt;   l6_0 ~ normal(0, 2);\n#&gt;   l6_13 ~ lognormal(0, 1);\n#&gt;   l7_0 ~ normal(0, 2);\n#&gt;   l7_11 ~ lognormal(0, 1);\n#&gt;   l7_13 ~ lognormal(0, 1);\n#&gt;   l7_213 ~ normal(0, 2);\n#&gt;   l8_0 ~ normal(0, 2);\n#&gt;   l8_12 ~ lognormal(0, 1);\n#&gt;   l9_0 ~ normal(0, 2);\n#&gt;   l9_13 ~ lognormal(0, 1);\n#&gt;   l10_0 ~ normal(0, 2);\n#&gt;   l10_11 ~ lognormal(0, 1);\n#&gt;   l11_0 ~ normal(0, 2);\n#&gt;   l11_11 ~ lognormal(0, 1);\n#&gt;   l11_13 ~ lognormal(0, 1);\n#&gt;   l11_213 ~ normal(0, 2);\n#&gt;   l12_0 ~ normal(0, 2);\n#&gt;   l12_11 ~ lognormal(0, 1);\n#&gt;   l12_13 ~ lognormal(0, 1);\n#&gt;   l12_213 ~ normal(0, 2);\n#&gt;   l13_0 ~ normal(0, 2);\n#&gt;   l13_11 ~ lognormal(0, 1);\n#&gt;   l14_0 ~ normal(0, 2);\n#&gt;   l14_11 ~ lognormal(0, 1);\n#&gt;   l15_0 ~ normal(0, 2);\n#&gt;   l15_13 ~ lognormal(0, 1);\n#&gt;   l16_0 ~ normal(0, 2);\n#&gt;   l16_11 ~ lognormal(0, 1);\n#&gt;   l16_13 ~ lognormal(0, 1);\n#&gt;   l16_213 ~ normal(0, 2);\n#&gt;   l17_0 ~ normal(0, 2);\n#&gt;   l17_12 ~ lognormal(0, 1);\n#&gt;   l17_13 ~ lognormal(0, 1);\n#&gt;   l17_223 ~ normal(0, 2);\n#&gt;   l18_0 ~ normal(0, 2);\n#&gt;   l18_13 ~ lognormal(0, 1);\n#&gt;   l19_0 ~ normal(0, 2);\n#&gt;   l19_13 ~ lognormal(0, 1);\n#&gt;   l20_0 ~ normal(0, 2);\n#&gt;   l20_11 ~ lognormal(0, 1);\n#&gt;   l20_13 ~ lognormal(0, 1);\n#&gt;   l20_213 ~ normal(0, 2);\n#&gt;   l21_0 ~ normal(0, 2);\n#&gt;   l21_11 ~ lognormal(0, 1);\n#&gt;   l21_13 ~ lognormal(0, 1);\n#&gt;   l21_213 ~ normal(0, 2);\n#&gt;   l22_0 ~ normal(0, 2);\n#&gt;   l22_13 ~ lognormal(0, 1);\n#&gt;   l23_0 ~ normal(0, 2);\n#&gt;   l23_12 ~ lognormal(0, 1);\n#&gt;   l24_0 ~ normal(0, 2);\n#&gt;   l24_12 ~ lognormal(0, 1);\n#&gt;   l25_0 ~ normal(0, 2);\n#&gt;   l25_11 ~ lognormal(0, 1);\n#&gt;   l26_0 ~ normal(0, 2);\n#&gt;   l26_13 ~ lognormal(0, 1);\n#&gt;   l27_0 ~ normal(0, 2);\n#&gt;   l27_11 ~ lognormal(0, 1);\n#&gt;   l28_0 ~ normal(0, 2);\n#&gt;   l28_13 ~ lognormal(0, 1);\n#&gt; \n#&gt;   ////////////////////////////////// likelihood\n#&gt;   for (r in 1:R) {\n#&gt;     row_vector[C] ps;\n#&gt;     for (c in 1:C) {\n#&gt;       array[num[r]] real log_items;\n#&gt;       for (m in 1:num[r]) {\n#&gt;         int i = ii[start[r] + m - 1];\n#&gt;         log_items[m] = y[start[r] + m - 1] * log(pi[i,c]) +\n#&gt;                        (1 - y[start[r] + m - 1]) * log(1 - pi[i,c]);\n#&gt;       }\n#&gt;       ps[c] = log_Vc[c] + sum(log_items);\n#&gt;     }\n#&gt;     target += log_sum_exp(ps);\n#&gt;   }\n#&gt; }\n\n\nBoth rstan and cmdstanr also require a list of data objects that correspond to the data block of the Stan code. This can be created with stan_data(). Note that if you edit the generated Stan code to customize the estimation and add to the data block, you will need to also add corresponding objects to the data list before estimation.\n\ndat &lt;- stan_data(ecpe_spec, data = ecpe_data, identifier = \"resp_id\")\n\nstr(dat)\n#&gt; List of 9\n#&gt;  $ I    : int 28\n#&gt;  $ R    : int 2922\n#&gt;  $ N    : int 81816\n#&gt;  $ C    : int 8\n#&gt;  $ ii   : num [1:81816] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  $ rr   : num [1:81816] 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ y    : int [1:81816] 1 1 1 0 1 1 1 1 1 1 ...\n#&gt;  $ start: int [1:2922] 1 29 57 85 113 141 169 197 225 253 ...\n#&gt;  $ num  : int [1:2922] 28 28 28 28 28 28 28 28 28 28 ..."
  },
  {
    "objectID": "blog/2025-11-dcmstan-0.1.0/index.html#dcmstan-measr",
    "href": "blog/2025-11-dcmstan-0.1.0/index.html#dcmstan-measr",
    "title": "dcmstan 0.1.0",
    "section": "dcmstan + measr",
    "text": "dcmstan + measr\ndcmstan is primarily intended to be a backend to {measr}. Many dcmstan functions are reexported by measr so that if you are using measr to estimate your model, you should not need to directly load or interact with dcmstan. For example, dcm_specify() is reexported by measr, so one can simply create a model specification and pass that directly to measr::dcm_estimate().\n\nlibrary(measr)\n\necpe_spec &lt;- dcm_specify(\n  qmatrix = ecpe_qmatrix,\n  identifier = \"item_id\",\n  measurement_model = lcdm(),\n  structural_model = unconstrained()\n)\n\nmodel &lt;- dcm_estimate(\n  dcm_spec = ecpe_spec,\n  data = ecpe_data,\n  identifier = \"resp_id\"\n)\n\nmeasr::dcm_estimate() calls stan_code() and stan_data() internally to create the necessary Stan script and data list and then estimates the model using your chosen backend (i.e., rstan or cmdstanr). Thus, a direct interface with dcmstan should only be necessary if you want to modify the Stan code that is generated by default."
  },
  {
    "objectID": "blog/2025-11-dcmstan-0.1.0/index.html#acknowledgments",
    "href": "blog/2025-11-dcmstan-0.1.0/index.html#acknowledgments",
    "title": "dcmstan 0.1.0",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThe research reported here was supported by the Institute of Education Sciences, U.S. Department of Education, through Grants R305D210045 and R305D240032 to the University of Kansas Center for Research, Inc., ATLAS. The opinions expressed are those of the authors and do not represent the views of the the Institute or the U.S. Department of Education.\nFeatured photo by Andreas Haslinger on Unsplash."
  },
  {
    "objectID": "funding.html",
    "href": "funding.html",
    "title": "Funding",
    "section": "",
    "text": "Development of the r-dcm packages has been supported by:\n\n\nThe research reported here was supported by the Institute of Education Sciences, U.S. Department of Education, through Grants R305D210045 and R305D240032 to the University of Kansas Center for Research, Inc., ATLAS. The opinions expressed are those of the authors and do not represent the views of the Institute or the U.S. Department of Education.\n\n\n\n\nThis work was funded in part by Accessible Teaching, Learning, and Assessment Systems (ATLAS), a research center within the Achievement and Assessment Institute at the University of Kansas. The opinions expressed are those of the authors and do not represent the views of ATLAS or the University of Kansas."
  },
  {
    "objectID": "start/evaluate/index.html",
    "href": "start/evaluate/index.html",
    "title": "Evaluate model performance",
    "section": "",
    "text": "The second article will focus on tools for evaluating an estimated DCM model. This will include tools for calculating:\n\nAbsolute fit (e.g., fit_ppmc(), fit_m2())\nRelative fit for model comparisons (e.g., waic(), loo())\n\nWe will showcase tools for investigating potential causes of misfit such as:\n\nIncorrect Q-matrix specifications (qmatrix_validation())\nLocal item dependence (yens_q3())\nItem and attribute discrimination (cdi())",
    "crumbs": [
      "Get Started",
      "Evaluate model performance"
    ]
  },
  {
    "objectID": "start/evaluate/index.html#introduction",
    "href": "start/evaluate/index.html#introduction",
    "title": "Evaluate model performance",
    "section": "",
    "text": "The second article will focus on tools for evaluating an estimated DCM model. This will include tools for calculating:\n\nAbsolute fit (e.g., fit_ppmc(), fit_m2())\nRelative fit for model comparisons (e.g., waic(), loo())\n\nWe will showcase tools for investigating potential causes of misfit such as:\n\nIncorrect Q-matrix specifications (qmatrix_validation())\nLocal item dependence (yens_q3())\nItem and attribute discrimination (cdi())",
    "crumbs": [
      "Get Started",
      "Evaluate model performance"
    ]
  },
  {
    "objectID": "start/index.html",
    "href": "start/index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Learn what you need to get started with measr and other r-dcm packages in four articles, starting with how to specify a diagnostic classification model (DCM) and ending with a beginning-to-end case study.",
    "crumbs": [
      "Get Started",
      "GET STARTED"
    ]
  },
  {
    "objectID": "start/index.html#if-you-are-new-to-diagnostic-models",
    "href": "start/index.html#if-you-are-new-to-diagnostic-models",
    "title": "Welcome!",
    "section": "If you are new to diagnostic models",
    "text": "If you are new to diagnostic models\nDCMs are a powerful psychometric tool for analyzing assessment data. The r-dcm suite of packages are designed to make the implementation of these models intuitive, and the documentation is focused on how to use the software. Before using these models in practice, we recommend that you start by learning the conceptual and statistical foundations of DCMs, then return here for information and tutorials on implementation. Here are some resources to start learning:\n\nDiagnostic Measurement Checklists, from the NCME ITEMS portal.\nIntroduction to Diagnostic Classification Models, from the r-dcm team.",
    "crumbs": [
      "Get Started",
      "GET STARTED"
    ]
  },
  {
    "objectID": "start/index.html#if-you-are-new-to-r",
    "href": "start/index.html#if-you-are-new-to-r",
    "title": "Welcome!",
    "section": "If you are new to R",
    "text": "If you are new to R\nThe r-dcm packages are built in R and are designed to be compatible with the tidyverse. Therefore, in order to get the most of out r-dcm packages, we recommend that you start by learning some basics about R and the tidyverse first, then return here when you feel ready. Here are some resources to start learning:\n\nFinding Your Way to R, from the RStudio Education team.\nLearn the tidyverse, from the tidyverse team.",
    "crumbs": [
      "Get Started",
      "GET STARTED"
    ]
  },
  {
    "objectID": "team/JeffreyCHoover/index.html",
    "href": "team/JeffreyCHoover/index.html",
    "title": "Jeffrey C. Hoover",
    "section": "",
    "text": "Jeff Hoover is a psychometrician at Accessible Teaching, Learning, and Assessment Systems (ATLAS), a strategic center within the Achievement and Assessment Institute at the University of Kansas. He received a PhD in educational psychology and research from the University of Kansas’ School of Education and Human Sciences in 2022. His research interests include diagnostic classification models and the integration of cutting-edge technologies such as process data and artificial intelligence into operational assessments."
  },
  {
    "objectID": "team/index.html",
    "href": "team/index.html",
    "title": "Meet the team",
    "section": "",
    "text": "W. Jake Thompson\n\n            Principal Investigator\n\n            \n              \n                \n                  \n                    \n                     ORCID\n                  \n                \n                  \n                    \n                     GitHub\n                  \n                \n                  \n                    \n                     Bluesky\n                  \n                \n                  \n                    \n                     Website\n                  \n                \n              \n            \n        \n\n    \n    \n    \n\n        \n        \n            \n        \n        \n\n        \n            Jeffrey C. Hoover\n\n            Psychometrician\n\n            \n              \n                \n                  \n                    \n                     ORCID\n                  \n                \n                  \n                    \n                     GitHub\n                  \n                \n              \n            \n        \n\n    \n    \n    \n\n        \n        \n            \n        \n        \n\n        \n            Auburn Jimenez\n\n            Psychometrician\n\n            \n              \n                \n                  \n                    \n                     ORCID\n                  \n                \n                  \n                    \n                     GitHub\n                  \n                \n              \n            \n        \n\n    \n    \n\nNo matching items"
  }
]