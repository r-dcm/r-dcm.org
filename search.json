[
  {
    "objectID": "team/wjakethompson/index.html",
    "href": "team/wjakethompson/index.html",
    "title": "W. Jake Thompson",
    "section": "",
    "text": "Jake Thompson is the assistant director of psychometrics at Accessible Teaching, Learning, and Assessment Systems (ATLAS), a strategic center within the Achievement and Assessment Institute at the University of Kansas. He received a PhD in educational psychology and research from the University of Kansas’ School of Education and Human Sciences in 2018.\nJake has been the Principal Investigator for two grants funded by the Institute of Education Sciences to develop the r-dcm suite of packages for the estimation and evaluation of diagnostic classification models:\n\nImproving Software and Methods for Estimating Diagnostic Classification Models and Evaluating Model Fit (R305D210045)\nExpanding the Functionality and Accessibility of Software for Diagnostic Measurement (R305D240032)"
  },
  {
    "objectID": "team/auburnjimenez34/index.html",
    "href": "team/auburnjimenez34/index.html",
    "title": "Auburn Jimenez",
    "section": "",
    "text": "Auburn Jimenez is a psychometrician at Accessible Teaching, Learning, and Assessment Systems (ATLAS), a strategic center within the Achievement and Assessment Institute at the University of Kansas. He received a PhD in quantitative psychology from the University of Illinois Urbana-Champaign in 2023. His professional interests include the development and application of diagnostic classification and item response theory models in educational assessment."
  },
  {
    "objectID": "funding.html",
    "href": "funding.html",
    "title": "Funding",
    "section": "",
    "text": "Development of the r-dcm packages has been supported by:\n\n\nThe research reported here was supported by the Institute of Education Sciences, U.S. Department of Education, through Grants R305D210045 and R305D240032 to the University of Kansas Center for Research, Inc., ATLAS. The opinions expressed are those of the authors and do not represent the views of the Institute or the U.S. Department of Education.\n\n\n\n\nThis work was funded in part by Accessible Teaching, Learning, and Assessment Systems (ATLAS), a research center within the Achievement and Assessment Institute at the University of Kansas. The opinions expressed are those of the authors and do not represent the views of ATLAS or the University of Kansas."
  },
  {
    "objectID": "blog/2025-11-dcmstan-0.1.0/index.html",
    "href": "blog/2025-11-dcmstan-0.1.0/index.html",
    "title": "dcmstan 0.1.0",
    "section": "",
    "text": "We are very pleased to announce the release of a new package, dcmstan. The goal of dcmstan is to provide users with a friendly interface for creating Stan scripts necessary for estimating diagnostic classification models (DCMs; also called cognitive diagnostic models [CDMs]). dcmstan is primarily intended to serve as a backend for {measr}, which will interface with {rstan} or {cmdstanr} to actually estimate a DCM. However, dcmstan can also be used independently if you want finer control of the estimation process or would like to customize the Stan script used for estimation.\nYou can install dcmstan from CRAN with:\nThis blog post will highlight the major features and describe how dcmstan fits into the larger universe of r-dcm packages.\nlibrary(dcmstan)"
  },
  {
    "objectID": "blog/2025-11-dcmstan-0.1.0/index.html#dcm-specification",
    "href": "blog/2025-11-dcmstan-0.1.0/index.html#dcm-specification",
    "title": "dcmstan 0.1.0",
    "section": "DCM specification",
    "text": "DCM specification\nFor this example, we’ll create a specification for a DCM that we want to fit to ECPE data, which is available in the {dcmdata} package.\n\nlibrary(dcmdata)\n\necpe_qmatrix\n#&gt; # A tibble: 28 × 4\n#&gt;    item_id morphosyntactic cohesive lexical\n#&gt;    &lt;chr&gt;             &lt;int&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt;  1 E1                    1        1       0\n#&gt;  2 E2                    0        1       0\n#&gt;  3 E3                    1        0       1\n#&gt;  4 E4                    0        0       1\n#&gt;  5 E5                    0        0       1\n#&gt;  6 E6                    0        0       1\n#&gt;  7 E7                    1        0       1\n#&gt;  8 E8                    0        1       0\n#&gt;  9 E9                    0        0       1\n#&gt; 10 E10                   1        0       0\n#&gt; # ℹ 18 more rows\n\necpe_data\n#&gt; # A tibble: 2,922 × 29\n#&gt;    resp_id    E1    E2    E3    E4    E5    E6    E7    E8    E9   E10   E11\n#&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt;  1       1     1     1     1     0     1     1     1     1     1     1     1\n#&gt;  2       2     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  3       3     1     1     1     1     1     1     0     1     1     1     1\n#&gt;  4       4     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  5       5     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  6       6     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  7       7     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  8       8     0     1     1     1     1     1     0     1     1     1     0\n#&gt;  9       9     1     1     1     1     1     1     1     1     1     1     1\n#&gt; 10      10     1     1     1     1     0     0     1     1     1     1     1\n#&gt; # ℹ 2,912 more rows\n#&gt; # ℹ 17 more variables: E12 &lt;int&gt;, E13 &lt;int&gt;, E14 &lt;int&gt;, E15 &lt;int&gt;, E16 &lt;int&gt;,\n#&gt; #   E17 &lt;int&gt;, E18 &lt;int&gt;, E19 &lt;int&gt;, E20 &lt;int&gt;, E21 &lt;int&gt;, E22 &lt;int&gt;,\n#&gt; #   E23 &lt;int&gt;, E24 &lt;int&gt;, E25 &lt;int&gt;, E26 &lt;int&gt;, E27 &lt;int&gt;, E28 &lt;int&gt;\n\nWe can create a DCM specification using dcm_specify(). We must provide our Q-matrix and, if present, the name of the Q-matrix column that contains the item identifiers. We then must choose a measurement and structural model to be used. By default, dcm_specify() will fit a loglinear cognitive diagnostic model (LCDM; Henson et al., 2009; Henson & Templin, 2019) with an unconstrained structural model. These are the measurement models used by Templin & Hoffman (2013) in their examination of the ECPE data, so we will echo those choices here. However, there are many other measurement and structural models we could choose from. For details on the specification options, see vignette(\"dcmstan\", package = \"dcmstan\").\n\necpe_spec &lt;- dcm_specify(\n  qmatrix = ecpe_qmatrix,\n  identifier = \"item_id\",\n  measurement_model = lcdm(),\n  structural_model = unconstrained()\n)\n\necpe_spec\n#&gt; A loglinear cognitive diagnostic model (LCDM) measuring 3 attributes with 28\n#&gt; items.\n#&gt; \n#&gt; ℹ Attributes:\n#&gt; • \"morphosyntactic\" (13 items)\n#&gt; • \"cohesive\" (6 items)\n#&gt; • \"lexical\" (18 items)\n#&gt; \n#&gt; ℹ Attribute structure:\n#&gt;   Unconstrained\n#&gt; \n#&gt; ℹ Prior distributions:\n#&gt;   intercept ~ normal(0, 2)\n#&gt;   maineffect ~ lognormal(0, 1)\n#&gt;   interaction ~ normal(0, 2)\n#&gt;   `Vc` ~ dirichlet(1, 1, 1)\n\nWe can also specify prior distributions for the model. Reasonable priors are defined by default, but custom priors can be specified for specific parameters or an entire type of parameters (e.g., applies to all intercept parameters). For a list of possible parameters for given specification, see get_parameters().\n\nget_parameters(ecpe_spec)\n#&gt; # A tibble: 75 × 4\n#&gt;    item_id type        attributes                coefficient\n#&gt;    &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;                     &lt;chr&gt;      \n#&gt;  1 E1      intercept   &lt;NA&gt;                      l1_0       \n#&gt;  2 E1      maineffect  morphosyntactic           l1_11      \n#&gt;  3 E1      maineffect  cohesive                  l1_12      \n#&gt;  4 E1      interaction morphosyntactic__cohesive l1_212     \n#&gt;  5 E2      intercept   &lt;NA&gt;                      l2_0       \n#&gt;  6 E2      maineffect  cohesive                  l2_12      \n#&gt;  7 E3      intercept   &lt;NA&gt;                      l3_0       \n#&gt;  8 E3      maineffect  morphosyntactic           l3_11      \n#&gt;  9 E3      maineffect  lexical                   l3_13      \n#&gt; 10 E3      interaction morphosyntactic__lexical  l3_213     \n#&gt; # ℹ 65 more rows"
  },
  {
    "objectID": "blog/2025-11-dcmstan-0.1.0/index.html#stan-code-and-data",
    "href": "blog/2025-11-dcmstan-0.1.0/index.html#stan-code-and-data",
    "title": "dcmstan 0.1.0",
    "section": "Stan code and data",
    "text": "Stan code and data\nOnce we have our specification, we can create the Stan code for estimating the model with stan_code(). This code can either be passed directly to the model_code argument of rstan::stan(), or written to a .stan file, which can then be passed to the file argument of rstan::stan() or the stan_file argument of cmdstanr::cmdstan_model().\n\nstan_code(ecpe_spec)\n\n#&gt; data {\n#&gt;   int&lt;lower=1&gt; I;                      // number of items\n#&gt;   int&lt;lower=1&gt; R;                      // number of respondents\n#&gt;   int&lt;lower=1&gt; N;                      // number of observations\n#&gt;   int&lt;lower=1&gt; C;                      // number of classes\n#&gt;   array[N] int&lt;lower=1,upper=I&gt; ii;    // item for observation n\n#&gt;   array[N] int&lt;lower=1,upper=R&gt; rr;    // respondent for observation n\n#&gt;   array[N] int&lt;lower=0,upper=1&gt; y;     // score for observation n\n#&gt;   array[R] int&lt;lower=1,upper=N&gt; start; // starting row for respondent R\n#&gt;   array[R] int&lt;lower=1,upper=I&gt; num;   // number items for respondent R\n#&gt; }\n#&gt; parameters {\n#&gt;   simplex[C] Vc;\n#&gt; \n#&gt;   ////////////////////////////////// item intercepts\n#&gt;   real l1_0;\n#&gt;   real l2_0;\n#&gt;   real l3_0;\n#&gt;   real l4_0;\n#&gt;   real l5_0;\n#&gt;   real l6_0;\n#&gt;   real l7_0;\n#&gt;   real l8_0;\n#&gt;   real l9_0;\n#&gt;   real l10_0;\n#&gt;   real l11_0;\n#&gt;   real l12_0;\n#&gt;   real l13_0;\n#&gt;   real l14_0;\n#&gt;   real l15_0;\n#&gt;   real l16_0;\n#&gt;   real l17_0;\n#&gt;   real l18_0;\n#&gt;   real l19_0;\n#&gt;   real l20_0;\n#&gt;   real l21_0;\n#&gt;   real l22_0;\n#&gt;   real l23_0;\n#&gt;   real l24_0;\n#&gt;   real l25_0;\n#&gt;   real l26_0;\n#&gt;   real l27_0;\n#&gt;   real l28_0;\n#&gt; \n#&gt;   ////////////////////////////////// item main effects\n#&gt;   real&lt;lower=0&gt; l1_11;\n#&gt;   real&lt;lower=0&gt; l1_12;\n#&gt;   real&lt;lower=0&gt; l2_12;\n#&gt;   real&lt;lower=0&gt; l3_11;\n#&gt;   real&lt;lower=0&gt; l3_13;\n#&gt;   real&lt;lower=0&gt; l4_13;\n#&gt;   real&lt;lower=0&gt; l5_13;\n#&gt;   real&lt;lower=0&gt; l6_13;\n#&gt;   real&lt;lower=0&gt; l7_11;\n#&gt;   real&lt;lower=0&gt; l7_13;\n#&gt;   real&lt;lower=0&gt; l8_12;\n#&gt;   real&lt;lower=0&gt; l9_13;\n#&gt;   real&lt;lower=0&gt; l10_11;\n#&gt;   real&lt;lower=0&gt; l11_11;\n#&gt;   real&lt;lower=0&gt; l11_13;\n#&gt;   real&lt;lower=0&gt; l12_11;\n#&gt;   real&lt;lower=0&gt; l12_13;\n#&gt;   real&lt;lower=0&gt; l13_11;\n#&gt;   real&lt;lower=0&gt; l14_11;\n#&gt;   real&lt;lower=0&gt; l15_13;\n#&gt;   real&lt;lower=0&gt; l16_11;\n#&gt;   real&lt;lower=0&gt; l16_13;\n#&gt;   real&lt;lower=0&gt; l17_12;\n#&gt;   real&lt;lower=0&gt; l17_13;\n#&gt;   real&lt;lower=0&gt; l18_13;\n#&gt;   real&lt;lower=0&gt; l19_13;\n#&gt;   real&lt;lower=0&gt; l20_11;\n#&gt;   real&lt;lower=0&gt; l20_13;\n#&gt;   real&lt;lower=0&gt; l21_11;\n#&gt;   real&lt;lower=0&gt; l21_13;\n#&gt;   real&lt;lower=0&gt; l22_13;\n#&gt;   real&lt;lower=0&gt; l23_12;\n#&gt;   real&lt;lower=0&gt; l24_12;\n#&gt;   real&lt;lower=0&gt; l25_11;\n#&gt;   real&lt;lower=0&gt; l26_13;\n#&gt;   real&lt;lower=0&gt; l27_11;\n#&gt;   real&lt;lower=0&gt; l28_13;\n#&gt; \n#&gt;   ////////////////////////////////// item interactions\n#&gt;   real&lt;lower=-1 * min([l1_11,l1_12])&gt; l1_212;\n#&gt;   real&lt;lower=-1 * min([l3_11,l3_13])&gt; l3_213;\n#&gt;   real&lt;lower=-1 * min([l7_11,l7_13])&gt; l7_213;\n#&gt;   real&lt;lower=-1 * min([l11_11,l11_13])&gt; l11_213;\n#&gt;   real&lt;lower=-1 * min([l12_11,l12_13])&gt; l12_213;\n#&gt;   real&lt;lower=-1 * min([l16_11,l16_13])&gt; l16_213;\n#&gt;   real&lt;lower=-1 * min([l17_12,l17_13])&gt; l17_223;\n#&gt;   real&lt;lower=-1 * min([l20_11,l20_13])&gt; l20_213;\n#&gt;   real&lt;lower=-1 * min([l21_11,l21_13])&gt; l21_213;\n#&gt; }\n#&gt; transformed parameters {\n#&gt;   vector[C] log_Vc = log(Vc);\n#&gt;   matrix[I,C] pi;\n#&gt; \n#&gt;   ////////////////////////////////// probability of correct response\n#&gt;   pi[1,1] = inv_logit(l1_0);\n#&gt;   pi[1,2] = inv_logit(l1_0+l1_11);\n#&gt;   pi[1,3] = inv_logit(l1_0+l1_12);\n#&gt;   pi[1,4] = inv_logit(l1_0);\n#&gt;   pi[1,5] = inv_logit(l1_0+l1_11+l1_12+l1_212);\n#&gt;   pi[1,6] = inv_logit(l1_0+l1_11);\n#&gt;   pi[1,7] = inv_logit(l1_0+l1_12);\n#&gt;   pi[1,8] = inv_logit(l1_0+l1_11+l1_12+l1_212);\n#&gt;   pi[2,1] = inv_logit(l2_0);\n#&gt;   pi[2,2] = inv_logit(l2_0);\n#&gt;   pi[2,3] = inv_logit(l2_0+l2_12);\n#&gt;   pi[2,4] = inv_logit(l2_0);\n#&gt;   pi[2,5] = inv_logit(l2_0+l2_12);\n#&gt;   pi[2,6] = inv_logit(l2_0);\n#&gt;   pi[2,7] = inv_logit(l2_0+l2_12);\n#&gt;   pi[2,8] = inv_logit(l2_0+l2_12);\n#&gt;   pi[3,1] = inv_logit(l3_0);\n#&gt;   pi[3,2] = inv_logit(l3_0+l3_11);\n#&gt;   pi[3,3] = inv_logit(l3_0);\n#&gt;   pi[3,4] = inv_logit(l3_0+l3_13);\n#&gt;   pi[3,5] = inv_logit(l3_0+l3_11);\n#&gt;   pi[3,6] = inv_logit(l3_0+l3_11+l3_13+l3_213);\n#&gt;   pi[3,7] = inv_logit(l3_0+l3_13);\n#&gt;   pi[3,8] = inv_logit(l3_0+l3_11+l3_13+l3_213);\n#&gt;   pi[4,1] = inv_logit(l4_0);\n#&gt;   pi[4,2] = inv_logit(l4_0);\n#&gt;   pi[4,3] = inv_logit(l4_0);\n#&gt;   pi[4,4] = inv_logit(l4_0+l4_13);\n#&gt;   pi[4,5] = inv_logit(l4_0);\n#&gt;   pi[4,6] = inv_logit(l4_0+l4_13);\n#&gt;   pi[4,7] = inv_logit(l4_0+l4_13);\n#&gt;   pi[4,8] = inv_logit(l4_0+l4_13);\n#&gt;   pi[5,1] = inv_logit(l5_0);\n#&gt;   pi[5,2] = inv_logit(l5_0);\n#&gt;   pi[5,3] = inv_logit(l5_0);\n#&gt;   pi[5,4] = inv_logit(l5_0+l5_13);\n#&gt;   pi[5,5] = inv_logit(l5_0);\n#&gt;   pi[5,6] = inv_logit(l5_0+l5_13);\n#&gt;   pi[5,7] = inv_logit(l5_0+l5_13);\n#&gt;   pi[5,8] = inv_logit(l5_0+l5_13);\n#&gt;   pi[6,1] = inv_logit(l6_0);\n#&gt;   pi[6,2] = inv_logit(l6_0);\n#&gt;   pi[6,3] = inv_logit(l6_0);\n#&gt;   pi[6,4] = inv_logit(l6_0+l6_13);\n#&gt;   pi[6,5] = inv_logit(l6_0);\n#&gt;   pi[6,6] = inv_logit(l6_0+l6_13);\n#&gt;   pi[6,7] = inv_logit(l6_0+l6_13);\n#&gt;   pi[6,8] = inv_logit(l6_0+l6_13);\n#&gt;   pi[7,1] = inv_logit(l7_0);\n#&gt;   pi[7,2] = inv_logit(l7_0+l7_11);\n#&gt;   pi[7,3] = inv_logit(l7_0);\n#&gt;   pi[7,4] = inv_logit(l7_0+l7_13);\n#&gt;   pi[7,5] = inv_logit(l7_0+l7_11);\n#&gt;   pi[7,6] = inv_logit(l7_0+l7_11+l7_13+l7_213);\n#&gt;   pi[7,7] = inv_logit(l7_0+l7_13);\n#&gt;   pi[7,8] = inv_logit(l7_0+l7_11+l7_13+l7_213);\n#&gt;   pi[8,1] = inv_logit(l8_0);\n#&gt;   pi[8,2] = inv_logit(l8_0);\n#&gt;   pi[8,3] = inv_logit(l8_0+l8_12);\n#&gt;   pi[8,4] = inv_logit(l8_0);\n#&gt;   pi[8,5] = inv_logit(l8_0+l8_12);\n#&gt;   pi[8,6] = inv_logit(l8_0);\n#&gt;   pi[8,7] = inv_logit(l8_0+l8_12);\n#&gt;   pi[8,8] = inv_logit(l8_0+l8_12);\n#&gt;   pi[9,1] = inv_logit(l9_0);\n#&gt;   pi[9,2] = inv_logit(l9_0);\n#&gt;   pi[9,3] = inv_logit(l9_0);\n#&gt;   pi[9,4] = inv_logit(l9_0+l9_13);\n#&gt;   pi[9,5] = inv_logit(l9_0);\n#&gt;   pi[9,6] = inv_logit(l9_0+l9_13);\n#&gt;   pi[9,7] = inv_logit(l9_0+l9_13);\n#&gt;   pi[9,8] = inv_logit(l9_0+l9_13);\n#&gt;   pi[10,1] = inv_logit(l10_0);\n#&gt;   pi[10,2] = inv_logit(l10_0+l10_11);\n#&gt;   pi[10,3] = inv_logit(l10_0);\n#&gt;   pi[10,4] = inv_logit(l10_0);\n#&gt;   pi[10,5] = inv_logit(l10_0+l10_11);\n#&gt;   pi[10,6] = inv_logit(l10_0+l10_11);\n#&gt;   pi[10,7] = inv_logit(l10_0);\n#&gt;   pi[10,8] = inv_logit(l10_0+l10_11);\n#&gt;   pi[11,1] = inv_logit(l11_0);\n#&gt;   pi[11,2] = inv_logit(l11_0+l11_11);\n#&gt;   pi[11,3] = inv_logit(l11_0);\n#&gt;   pi[11,4] = inv_logit(l11_0+l11_13);\n#&gt;   pi[11,5] = inv_logit(l11_0+l11_11);\n#&gt;   pi[11,6] = inv_logit(l11_0+l11_11+l11_13+l11_213);\n#&gt;   pi[11,7] = inv_logit(l11_0+l11_13);\n#&gt;   pi[11,8] = inv_logit(l11_0+l11_11+l11_13+l11_213);\n#&gt;   pi[12,1] = inv_logit(l12_0);\n#&gt;   pi[12,2] = inv_logit(l12_0+l12_11);\n#&gt;   pi[12,3] = inv_logit(l12_0);\n#&gt;   pi[12,4] = inv_logit(l12_0+l12_13);\n#&gt;   pi[12,5] = inv_logit(l12_0+l12_11);\n#&gt;   pi[12,6] = inv_logit(l12_0+l12_11+l12_13+l12_213);\n#&gt;   pi[12,7] = inv_logit(l12_0+l12_13);\n#&gt;   pi[12,8] = inv_logit(l12_0+l12_11+l12_13+l12_213);\n#&gt;   pi[13,1] = inv_logit(l13_0);\n#&gt;   pi[13,2] = inv_logit(l13_0+l13_11);\n#&gt;   pi[13,3] = inv_logit(l13_0);\n#&gt;   pi[13,4] = inv_logit(l13_0);\n#&gt;   pi[13,5] = inv_logit(l13_0+l13_11);\n#&gt;   pi[13,6] = inv_logit(l13_0+l13_11);\n#&gt;   pi[13,7] = inv_logit(l13_0);\n#&gt;   pi[13,8] = inv_logit(l13_0+l13_11);\n#&gt;   pi[14,1] = inv_logit(l14_0);\n#&gt;   pi[14,2] = inv_logit(l14_0+l14_11);\n#&gt;   pi[14,3] = inv_logit(l14_0);\n#&gt;   pi[14,4] = inv_logit(l14_0);\n#&gt;   pi[14,5] = inv_logit(l14_0+l14_11);\n#&gt;   pi[14,6] = inv_logit(l14_0+l14_11);\n#&gt;   pi[14,7] = inv_logit(l14_0);\n#&gt;   pi[14,8] = inv_logit(l14_0+l14_11);\n#&gt;   pi[15,1] = inv_logit(l15_0);\n#&gt;   pi[15,2] = inv_logit(l15_0);\n#&gt;   pi[15,3] = inv_logit(l15_0);\n#&gt;   pi[15,4] = inv_logit(l15_0+l15_13);\n#&gt;   pi[15,5] = inv_logit(l15_0);\n#&gt;   pi[15,6] = inv_logit(l15_0+l15_13);\n#&gt;   pi[15,7] = inv_logit(l15_0+l15_13);\n#&gt;   pi[15,8] = inv_logit(l15_0+l15_13);\n#&gt;   pi[16,1] = inv_logit(l16_0);\n#&gt;   pi[16,2] = inv_logit(l16_0+l16_11);\n#&gt;   pi[16,3] = inv_logit(l16_0);\n#&gt;   pi[16,4] = inv_logit(l16_0+l16_13);\n#&gt;   pi[16,5] = inv_logit(l16_0+l16_11);\n#&gt;   pi[16,6] = inv_logit(l16_0+l16_11+l16_13+l16_213);\n#&gt;   pi[16,7] = inv_logit(l16_0+l16_13);\n#&gt;   pi[16,8] = inv_logit(l16_0+l16_11+l16_13+l16_213);\n#&gt;   pi[17,1] = inv_logit(l17_0);\n#&gt;   pi[17,2] = inv_logit(l17_0);\n#&gt;   pi[17,3] = inv_logit(l17_0+l17_12);\n#&gt;   pi[17,4] = inv_logit(l17_0+l17_13);\n#&gt;   pi[17,5] = inv_logit(l17_0+l17_12);\n#&gt;   pi[17,6] = inv_logit(l17_0+l17_13);\n#&gt;   pi[17,7] = inv_logit(l17_0+l17_12+l17_13+l17_223);\n#&gt;   pi[17,8] = inv_logit(l17_0+l17_12+l17_13+l17_223);\n#&gt;   pi[18,1] = inv_logit(l18_0);\n#&gt;   pi[18,2] = inv_logit(l18_0);\n#&gt;   pi[18,3] = inv_logit(l18_0);\n#&gt;   pi[18,4] = inv_logit(l18_0+l18_13);\n#&gt;   pi[18,5] = inv_logit(l18_0);\n#&gt;   pi[18,6] = inv_logit(l18_0+l18_13);\n#&gt;   pi[18,7] = inv_logit(l18_0+l18_13);\n#&gt;   pi[18,8] = inv_logit(l18_0+l18_13);\n#&gt;   pi[19,1] = inv_logit(l19_0);\n#&gt;   pi[19,2] = inv_logit(l19_0);\n#&gt;   pi[19,3] = inv_logit(l19_0);\n#&gt;   pi[19,4] = inv_logit(l19_0+l19_13);\n#&gt;   pi[19,5] = inv_logit(l19_0);\n#&gt;   pi[19,6] = inv_logit(l19_0+l19_13);\n#&gt;   pi[19,7] = inv_logit(l19_0+l19_13);\n#&gt;   pi[19,8] = inv_logit(l19_0+l19_13);\n#&gt;   pi[20,1] = inv_logit(l20_0);\n#&gt;   pi[20,2] = inv_logit(l20_0+l20_11);\n#&gt;   pi[20,3] = inv_logit(l20_0);\n#&gt;   pi[20,4] = inv_logit(l20_0+l20_13);\n#&gt;   pi[20,5] = inv_logit(l20_0+l20_11);\n#&gt;   pi[20,6] = inv_logit(l20_0+l20_11+l20_13+l20_213);\n#&gt;   pi[20,7] = inv_logit(l20_0+l20_13);\n#&gt;   pi[20,8] = inv_logit(l20_0+l20_11+l20_13+l20_213);\n#&gt;   pi[21,1] = inv_logit(l21_0);\n#&gt;   pi[21,2] = inv_logit(l21_0+l21_11);\n#&gt;   pi[21,3] = inv_logit(l21_0);\n#&gt;   pi[21,4] = inv_logit(l21_0+l21_13);\n#&gt;   pi[21,5] = inv_logit(l21_0+l21_11);\n#&gt;   pi[21,6] = inv_logit(l21_0+l21_11+l21_13+l21_213);\n#&gt;   pi[21,7] = inv_logit(l21_0+l21_13);\n#&gt;   pi[21,8] = inv_logit(l21_0+l21_11+l21_13+l21_213);\n#&gt;   pi[22,1] = inv_logit(l22_0);\n#&gt;   pi[22,2] = inv_logit(l22_0);\n#&gt;   pi[22,3] = inv_logit(l22_0);\n#&gt;   pi[22,4] = inv_logit(l22_0+l22_13);\n#&gt;   pi[22,5] = inv_logit(l22_0);\n#&gt;   pi[22,6] = inv_logit(l22_0+l22_13);\n#&gt;   pi[22,7] = inv_logit(l22_0+l22_13);\n#&gt;   pi[22,8] = inv_logit(l22_0+l22_13);\n#&gt;   pi[23,1] = inv_logit(l23_0);\n#&gt;   pi[23,2] = inv_logit(l23_0);\n#&gt;   pi[23,3] = inv_logit(l23_0+l23_12);\n#&gt;   pi[23,4] = inv_logit(l23_0);\n#&gt;   pi[23,5] = inv_logit(l23_0+l23_12);\n#&gt;   pi[23,6] = inv_logit(l23_0);\n#&gt;   pi[23,7] = inv_logit(l23_0+l23_12);\n#&gt;   pi[23,8] = inv_logit(l23_0+l23_12);\n#&gt;   pi[24,1] = inv_logit(l24_0);\n#&gt;   pi[24,2] = inv_logit(l24_0);\n#&gt;   pi[24,3] = inv_logit(l24_0+l24_12);\n#&gt;   pi[24,4] = inv_logit(l24_0);\n#&gt;   pi[24,5] = inv_logit(l24_0+l24_12);\n#&gt;   pi[24,6] = inv_logit(l24_0);\n#&gt;   pi[24,7] = inv_logit(l24_0+l24_12);\n#&gt;   pi[24,8] = inv_logit(l24_0+l24_12);\n#&gt;   pi[25,1] = inv_logit(l25_0);\n#&gt;   pi[25,2] = inv_logit(l25_0+l25_11);\n#&gt;   pi[25,3] = inv_logit(l25_0);\n#&gt;   pi[25,4] = inv_logit(l25_0);\n#&gt;   pi[25,5] = inv_logit(l25_0+l25_11);\n#&gt;   pi[25,6] = inv_logit(l25_0+l25_11);\n#&gt;   pi[25,7] = inv_logit(l25_0);\n#&gt;   pi[25,8] = inv_logit(l25_0+l25_11);\n#&gt;   pi[26,1] = inv_logit(l26_0);\n#&gt;   pi[26,2] = inv_logit(l26_0);\n#&gt;   pi[26,3] = inv_logit(l26_0);\n#&gt;   pi[26,4] = inv_logit(l26_0+l26_13);\n#&gt;   pi[26,5] = inv_logit(l26_0);\n#&gt;   pi[26,6] = inv_logit(l26_0+l26_13);\n#&gt;   pi[26,7] = inv_logit(l26_0+l26_13);\n#&gt;   pi[26,8] = inv_logit(l26_0+l26_13);\n#&gt;   pi[27,1] = inv_logit(l27_0);\n#&gt;   pi[27,2] = inv_logit(l27_0+l27_11);\n#&gt;   pi[27,3] = inv_logit(l27_0);\n#&gt;   pi[27,4] = inv_logit(l27_0);\n#&gt;   pi[27,5] = inv_logit(l27_0+l27_11);\n#&gt;   pi[27,6] = inv_logit(l27_0+l27_11);\n#&gt;   pi[27,7] = inv_logit(l27_0);\n#&gt;   pi[27,8] = inv_logit(l27_0+l27_11);\n#&gt;   pi[28,1] = inv_logit(l28_0);\n#&gt;   pi[28,2] = inv_logit(l28_0);\n#&gt;   pi[28,3] = inv_logit(l28_0);\n#&gt;   pi[28,4] = inv_logit(l28_0+l28_13);\n#&gt;   pi[28,5] = inv_logit(l28_0);\n#&gt;   pi[28,6] = inv_logit(l28_0+l28_13);\n#&gt;   pi[28,7] = inv_logit(l28_0+l28_13);\n#&gt;   pi[28,8] = inv_logit(l28_0+l28_13);\n#&gt; }\n#&gt; model {\n#&gt;   ////////////////////////////////// priors\n#&gt;   Vc ~ dirichlet(rep_vector(1, C));\n#&gt;   l1_0 ~ normal(0, 2);\n#&gt;   l1_11 ~ lognormal(0, 1);\n#&gt;   l1_12 ~ lognormal(0, 1);\n#&gt;   l1_212 ~ normal(0, 2);\n#&gt;   l2_0 ~ normal(0, 2);\n#&gt;   l2_12 ~ lognormal(0, 1);\n#&gt;   l3_0 ~ normal(0, 2);\n#&gt;   l3_11 ~ lognormal(0, 1);\n#&gt;   l3_13 ~ lognormal(0, 1);\n#&gt;   l3_213 ~ normal(0, 2);\n#&gt;   l4_0 ~ normal(0, 2);\n#&gt;   l4_13 ~ lognormal(0, 1);\n#&gt;   l5_0 ~ normal(0, 2);\n#&gt;   l5_13 ~ lognormal(0, 1);\n#&gt;   l6_0 ~ normal(0, 2);\n#&gt;   l6_13 ~ lognormal(0, 1);\n#&gt;   l7_0 ~ normal(0, 2);\n#&gt;   l7_11 ~ lognormal(0, 1);\n#&gt;   l7_13 ~ lognormal(0, 1);\n#&gt;   l7_213 ~ normal(0, 2);\n#&gt;   l8_0 ~ normal(0, 2);\n#&gt;   l8_12 ~ lognormal(0, 1);\n#&gt;   l9_0 ~ normal(0, 2);\n#&gt;   l9_13 ~ lognormal(0, 1);\n#&gt;   l10_0 ~ normal(0, 2);\n#&gt;   l10_11 ~ lognormal(0, 1);\n#&gt;   l11_0 ~ normal(0, 2);\n#&gt;   l11_11 ~ lognormal(0, 1);\n#&gt;   l11_13 ~ lognormal(0, 1);\n#&gt;   l11_213 ~ normal(0, 2);\n#&gt;   l12_0 ~ normal(0, 2);\n#&gt;   l12_11 ~ lognormal(0, 1);\n#&gt;   l12_13 ~ lognormal(0, 1);\n#&gt;   l12_213 ~ normal(0, 2);\n#&gt;   l13_0 ~ normal(0, 2);\n#&gt;   l13_11 ~ lognormal(0, 1);\n#&gt;   l14_0 ~ normal(0, 2);\n#&gt;   l14_11 ~ lognormal(0, 1);\n#&gt;   l15_0 ~ normal(0, 2);\n#&gt;   l15_13 ~ lognormal(0, 1);\n#&gt;   l16_0 ~ normal(0, 2);\n#&gt;   l16_11 ~ lognormal(0, 1);\n#&gt;   l16_13 ~ lognormal(0, 1);\n#&gt;   l16_213 ~ normal(0, 2);\n#&gt;   l17_0 ~ normal(0, 2);\n#&gt;   l17_12 ~ lognormal(0, 1);\n#&gt;   l17_13 ~ lognormal(0, 1);\n#&gt;   l17_223 ~ normal(0, 2);\n#&gt;   l18_0 ~ normal(0, 2);\n#&gt;   l18_13 ~ lognormal(0, 1);\n#&gt;   l19_0 ~ normal(0, 2);\n#&gt;   l19_13 ~ lognormal(0, 1);\n#&gt;   l20_0 ~ normal(0, 2);\n#&gt;   l20_11 ~ lognormal(0, 1);\n#&gt;   l20_13 ~ lognormal(0, 1);\n#&gt;   l20_213 ~ normal(0, 2);\n#&gt;   l21_0 ~ normal(0, 2);\n#&gt;   l21_11 ~ lognormal(0, 1);\n#&gt;   l21_13 ~ lognormal(0, 1);\n#&gt;   l21_213 ~ normal(0, 2);\n#&gt;   l22_0 ~ normal(0, 2);\n#&gt;   l22_13 ~ lognormal(0, 1);\n#&gt;   l23_0 ~ normal(0, 2);\n#&gt;   l23_12 ~ lognormal(0, 1);\n#&gt;   l24_0 ~ normal(0, 2);\n#&gt;   l24_12 ~ lognormal(0, 1);\n#&gt;   l25_0 ~ normal(0, 2);\n#&gt;   l25_11 ~ lognormal(0, 1);\n#&gt;   l26_0 ~ normal(0, 2);\n#&gt;   l26_13 ~ lognormal(0, 1);\n#&gt;   l27_0 ~ normal(0, 2);\n#&gt;   l27_11 ~ lognormal(0, 1);\n#&gt;   l28_0 ~ normal(0, 2);\n#&gt;   l28_13 ~ lognormal(0, 1);\n#&gt; \n#&gt;   ////////////////////////////////// likelihood\n#&gt;   for (r in 1:R) {\n#&gt;     row_vector[C] ps;\n#&gt;     for (c in 1:C) {\n#&gt;       array[num[r]] real log_items;\n#&gt;       for (m in 1:num[r]) {\n#&gt;         int i = ii[start[r] + m - 1];\n#&gt;         log_items[m] = y[start[r] + m - 1] * log(pi[i,c]) +\n#&gt;                        (1 - y[start[r] + m - 1]) * log(1 - pi[i,c]);\n#&gt;       }\n#&gt;       ps[c] = log_Vc[c] + sum(log_items);\n#&gt;     }\n#&gt;     target += log_sum_exp(ps);\n#&gt;   }\n#&gt; }\n\n\nBoth rstan and cmdstanr also require a list of data objects that correspond to the data block of the Stan code. This can be created with stan_data(). Note that if you edit the generated Stan code to customize the estimation and add to the data block, you will need to also add corresponding objects to the data list before estimation.\n\ndat &lt;- stan_data(ecpe_spec, data = ecpe_data, identifier = \"resp_id\")\n\nstr(dat)\n#&gt; List of 9\n#&gt;  $ I    : int 28\n#&gt;  $ R    : int 2922\n#&gt;  $ N    : int 81816\n#&gt;  $ C    : int 8\n#&gt;  $ ii   : num [1:81816] 1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  $ rr   : num [1:81816] 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ y    : int [1:81816] 1 1 1 0 1 1 1 1 1 1 ...\n#&gt;  $ start: int [1:2922] 1 29 57 85 113 141 169 197 225 253 ...\n#&gt;  $ num  : int [1:2922] 28 28 28 28 28 28 28 28 28 28 ..."
  },
  {
    "objectID": "blog/2025-11-dcmstan-0.1.0/index.html#dcmstan-measr",
    "href": "blog/2025-11-dcmstan-0.1.0/index.html#dcmstan-measr",
    "title": "dcmstan 0.1.0",
    "section": "dcmstan + measr",
    "text": "dcmstan + measr\ndcmstan is primarily intended to be a backend to {measr}. Many dcmstan functions are reexported by measr so that if you are using measr to estimate your model, you should not need to directly load or interact with dcmstan. For example, dcm_specify() is reexported by measr, so one can simply create a model specification and pass that directly to measr::dcm_estimate().\n\nlibrary(measr)\n\necpe_spec &lt;- dcm_specify(\n  qmatrix = ecpe_qmatrix,\n  identifier = \"item_id\",\n  measurement_model = lcdm(),\n  structural_model = unconstrained()\n)\n\nmodel &lt;- dcm_estimate(\n  dcm_spec = ecpe_spec,\n  data = ecpe_data,\n  identifier = \"resp_id\"\n)\n\ndcm_estimate() calls stan_code() and stan_data() internally to create the necessary Stan script and data list and then estimates the model using your chosen backend (i.e., rstan or cmdstanr). Thus, a direct interface with dcmstan should only be necessary if you want to modify the Stan code that is generated by default."
  },
  {
    "objectID": "blog/2025-11-dcmstan-0.1.0/index.html#acknowledgments",
    "href": "blog/2025-11-dcmstan-0.1.0/index.html#acknowledgments",
    "title": "dcmstan 0.1.0",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThe research reported here was supported by the Institute of Education Sciences, U.S. Department of Education, through Grants R305D210045 and R305D240032 to the University of Kansas Center for Research, Inc., ATLAS. The opinions expressed are those of the authors and do not represent the views of the the Institute or the U.S. Department of Education.\nFeatured photo by Andreas Haslinger on Unsplash."
  },
  {
    "objectID": "blog/2024-02-measr-1.0.0/index.html",
    "href": "blog/2024-02-measr-1.0.0/index.html",
    "title": "measr 1.0.0",
    "section": "",
    "text": "We are thrilled to announce the release of measr 1.0.0. The goal of measr is to provide a user-friendly interface for estimating and evaluating diagnostic classification models (DCMs; also called cognitive diagnostic models [CDMs]). This is a major release to mark the conclusion of the initial development work that was funded by the Institute of Education Sciences. Importantly, this does not mean that measr is going dormant! We are still actively developing measr, so we’ll continue to add new features and respond to bug reports.\nYou can install the updated version of measr from CRAN with:\nThis blog post will highlight the major improvements, including improved documentation and vignettes, as well as a some minor updates.\nlibrary(measr)"
  },
  {
    "objectID": "blog/2024-02-measr-1.0.0/index.html#improved-documentation",
    "href": "blog/2024-02-measr-1.0.0/index.html#improved-documentation",
    "title": "measr 1.0.0",
    "section": "Improved Documentation",
    "text": "Improved Documentation\nThe focus of recent work on measr has been to improve documentation. To that end, the existing vignettes have been updated, and several new vignettes have been added.\nFirst, the getting started vignette has been updated to include additional installation information. This should help reduce the friction for setting up the components needed for measr, namely, installing Stan. Because measr interfaces with Stan to estimate DCMs, a working Stan installation is needed for the package to function properly. The updated vignette includes helpful guidance for installing both the rstan and cmdstanr packages for using Stan. Only one of the two is required, but both are supported so you can choose whichever Stan front end you prefer.\nA new vignette on model evaluation has been added to demonstrate different methods for assessing the performance of a DCM after it has been estimated. This includes the M2 statistic (Liu et al., 2016), as well as posterior predictive model checks for evaluating both overall and item-level model fit (Sinharay & Almond, 2007; Thompson, 2019). To demonstrate how these methods are implemented for measr, a simulated data set is used where we know how well different types of DCMs should fit. The existing model estimation vignette was also updated to use this same simulated data set so that we can compare parameter estimates derived from measr to the known parameter values that were used to generate the data.\nBecause the model estimation and evaluation vignettes now use simulated data, we also added a case study to walk through a DCM-based analysis from start to finish using a real data set. In this case study, data from the Examination for the Certificate of Proficiency in English (ECPE), which has been widely used in the DCM literature (e.g., Templin & Hoffman, 2013). We start with exploratory analyses, then we estimate a DCM, examine parameter estimates, and interpret model fit and reliability analyses.\nFinally, example uses have been added for all functions included in measr. These examples can be found on the documentation pages for each function. All functions are indexed on the reference page, which has been reorganized to group functions with related functionality."
  },
  {
    "objectID": "blog/2024-02-measr-1.0.0/index.html#minor-changes",
    "href": "blog/2024-02-measr-1.0.0/index.html#minor-changes",
    "title": "measr 1.0.0",
    "section": "Minor Changes",
    "text": "Minor Changes\nThere were also a number of minor improvements:\n\nThe Stan code for estimating models has been updated to be compatible with the new array syntax.\nWe published an article on measr in the Journal of Open Source Software (Thompson, 2023), and the preferred citation when using measr has been updated.\n\nFor a complete list of changes, check out the changelog."
  },
  {
    "objectID": "blog/2024-02-measr-1.0.0/index.html#additional-resources",
    "href": "blog/2024-02-measr-1.0.0/index.html#additional-resources",
    "title": "measr 1.0.0",
    "section": "Additional Resources",
    "text": "Additional Resources\nIf you are interested in learing more about measr and diagnostic models, we hosted a workshop in June at StanCon 2023. The workshop provided an overview of diagnostic models and their use cases and then walked through how to estimate and evaluate different models using measr. All of the workshop materials are available online, including slides, exercises, and solutions.\nFinally, if you are interested in attending a workshop in person, we’ll be teaching another workshop in April at the 2024 annual meeting of the National Conference on Measurement in Education (NCME). The materials from this workshop will also be made available online for those who are unable to attend. For more information on the conference and register, see the conference website."
  },
  {
    "objectID": "blog/2024-02-measr-1.0.0/index.html#acknowledgments",
    "href": "blog/2024-02-measr-1.0.0/index.html#acknowledgments",
    "title": "measr 1.0.0",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThe research reported here was supported by the Institute of Education Sciences, U.S. Department of Education, through Grant R305D210045 to the University of Kansas. The opinions expressed are those of the authors and do not represent the views of the the Institute or the U.S. Department of Education.\nFeatured photo by patricia serna on Unsplash."
  },
  {
    "objectID": "blog/2023-04-measr-0.2.1/index.html",
    "href": "blog/2023-04-measr-0.2.1/index.html",
    "title": "measr 0.2.1",
    "section": "",
    "text": "We’re excited to announce the release of measr 0.2.1. The goal of measr is to provide applied researchers and psychometricians with a user friendly interface for estimating and evaluating diagnostic classification models (DCMs). DCMs are a class of psychometric models that provide classification of students into profiles of proficiency on a predefined set of knowledge, skills, or understandings. This offers many advantages over traditional assessment models. Because results are reported as proficiency on individual skills, teachers, students, and parents get actionable feedback about which areas could use additional attention. However, these models have not been widely used in applied or operational settings, in part due to a lack of user friendly software for estimating and evaluating these models. measr aims to bridge this gap between psychometric theory and applied practice.\nYou can install it from CRAN with:\ninstall.packages(\"measr\")\nThis blog post will highlight the main features of the package.\nlibrary(measr)"
  },
  {
    "objectID": "blog/2023-04-measr-0.2.1/index.html#estimating-dcms",
    "href": "blog/2023-04-measr-0.2.1/index.html#estimating-dcms",
    "title": "measr 0.2.1",
    "section": "Estimating DCMs",
    "text": "Estimating DCMs\nTo illustrate DCMs and their application with measr, we’ll use a simulated data set. In this data set, we have 1,000 respondents who each answered 15 questions about addition, multiplication, and fractions. For each item, a 1 represents a correct response to the item, and a 0 represents an incorrect response.\n\nlibrary(tidyverse)\n\ndat &lt;- read_csv(\"data/example-data.csv\")\ndat\n#&gt; # A tibble: 1,000 × 16\n#&gt;    resp_id item_01 item_02 item_03 item_04 item_05 item_06 item_07 item_08\n#&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1       1       1       1       1       0       1       1       0       1\n#&gt;  2       2       1       1       0       0       0       1       0       1\n#&gt;  3       3       1       1       0       0       0       1       0       1\n#&gt;  4       4       1       0       1       0       1       0       1       1\n#&gt;  5       5       1       1       0       0       0       1       1       0\n#&gt;  6       6       1       1       0       0       0       1       0       1\n#&gt;  7       7       1       1       1       0       0       1       1       1\n#&gt;  8       8       0       0       0       0       0       0       0       1\n#&gt;  9       9       0       1       0       0       0       1       0       0\n#&gt; 10      10       1       1       0       1       1       1       0       1\n#&gt; # ℹ 990 more rows\n#&gt; # ℹ 7 more variables: item_09 &lt;dbl&gt;, item_10 &lt;dbl&gt;, item_11 &lt;dbl&gt;,\n#&gt; #   item_12 &lt;dbl&gt;, item_13 &lt;dbl&gt;, item_14 &lt;dbl&gt;, item_15 &lt;dbl&gt;\n\nWhen using DCMs, a Q-matrix defines which items measure each attribute, or skill. In the Q-matrix, a 1 indicates that the item measures the attributes, and a 0 indicates the item does not measure the attribute.\n\nqmatrix &lt;- read_csv(\"data/example-qmatrix.csv\")\nqmatrix\n#&gt; # A tibble: 15 × 4\n#&gt;    item_id addition multiplication fractions\n#&gt;    &lt;chr&gt;      &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 item_01        1              1         0\n#&gt;  2 item_02        1              0         0\n#&gt;  3 item_03        0              1         0\n#&gt;  4 item_04        0              0         1\n#&gt;  5 item_05        0              1         0\n#&gt;  6 item_06        1              0         0\n#&gt;  7 item_07        0              1         0\n#&gt;  8 item_08        0              1         1\n#&gt;  9 item_09        0              1         1\n#&gt; 10 item_10        0              0         1\n#&gt; 11 item_11        1              1         0\n#&gt; 12 item_12        1              0         1\n#&gt; 13 item_13        0              0         1\n#&gt; 14 item_14        0              1         1\n#&gt; 15 item_15        1              0         0\n\nOur goal is to estimate whether each respondent is proficient in each of the skills measured by the assessment, and DCMs are our tool. There are many different types of DCMs, each with different constraints or assumptions about how the attributes relate to each other. For example, if an item measures multiple attributes, does a respondent need to be proficient on all attributes to answer the item correctly? Or can proficiency in one attribute compensate for the lack of proficiency on another?\nAlso several DCM subtypes are supported by measr, we are primarily focused on the loglinear cognitive diagnostic model (LCDM), which is a general DCM. This means that it subsumes many of the other DCM subtypes and allows for the data to determine how much (if at all) one attribute might compensate for another.\nmeasr works by wrapping the Stan language to estimate DCMs. We can estimate the LCDM using measr_dcm(). This function use the inputs to write a Stan script defining the model and then estimates the model using either the rstan or cmdstanr packages. To estimate the model, we just supply our data set and the Q-matrix. Because our data set and Q-matrix contain respondent and item identifiers, respectively, we need to specify the names of the identifier columns. We can also specify prior distributions for each of the model parameters. Finally, we specify a file for saving the model once it is estimated. For more information on model estimation, including details on specifying prior distributions, see the model estimation vignette.\n\nlcdm &lt;- measr_dcm(data = dat, qmatrix = qmatrix,\n                  resp_id = \"resp_id\", item_id = \"item_id\", \n                  prior = c(prior(normal(0, 2), class = \"intercept\"),\n                            prior(lognormal(0, 1), class = \"maineffect\"),\n                            prior(normal(0, 2), class = \"interaction\")),\n                  file = \"fits/example-lcdm\")\n\nOnce the model has estimated, we can use measr_extract() to examine different aspects of the model, such as proportion of respondents with each pattern of proficiency across the attributes.\n\nmeasr_extract(lcdm, \"strc_param\")\n#&gt; # A tibble: 8 × 2\n#&gt;   class          estimate\n#&gt;   &lt;chr&gt;        &lt;rvar[1d]&gt;\n#&gt; 1 [0,0,0]  0.165 ± 0.0127\n#&gt; 2 [1,0,0]  0.211 ± 0.0153\n#&gt; 3 [0,1,0]  0.048 ± 0.0089\n#&gt; 4 [0,0,1]  0.072 ± 0.0090\n#&gt; 5 [1,1,0]  0.166 ± 0.0164\n#&gt; 6 [1,0,1]  0.065 ± 0.0115\n#&gt; 7 [0,1,1]  0.084 ± 0.0116\n#&gt; 8 [1,1,1]  0.187 ± 0.0164"
  },
  {
    "objectID": "blog/2023-04-measr-0.2.1/index.html#evaluating-dcms",
    "href": "blog/2023-04-measr-0.2.1/index.html#evaluating-dcms",
    "title": "measr 0.2.1",
    "section": "Evaluating DCMs",
    "text": "Evaluating DCMs\nThere are several functions included for evaluating the model. For example we can examine the probability that each respondent is proficient in each attribute. Here, respondent 1 is likely proficient in all skills, whereas respondent 2 is likely only proficient in addition.\n\nlcdm &lt;- add_respondent_estimates(lcdm)\nmeasr_extract(lcdm, \"attribute_prob\")\n#&gt; # A tibble: 1,000 × 4\n#&gt;    resp_id addition multiplication fractions\n#&gt;    &lt;fct&gt;      &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 1       0.997          0.977      0.920  \n#&gt;  2 2       1.00           0.00297    0.0102 \n#&gt;  3 3       1.00           0.133      0.0370 \n#&gt;  4 4       0.0228         1.00       0.0556 \n#&gt;  5 5       0.999          0.0253     0.0223 \n#&gt;  6 6       1.00           0.00297    0.00224\n#&gt;  7 7       0.998          0.469      0.938  \n#&gt;  8 8       0.000520       0.0233     0.0194 \n#&gt;  9 9       0.984          0.000116   0.00641\n#&gt; 10 10      0.997          0.920      0.957  \n#&gt; # ℹ 990 more rows\n\nOften, we would dichotomize these probabilities into a 0/1 decision (e.g., proficient/not proficient). We can also look at the reliability of those classifications. The add_reliability() function will calculate the classification consistency and accuracy metrics described by Johnson & Sinharay (2018). Here we see that all of the attributes have fairly high accuracy and consistency for classifications.\n\nlcdm &lt;- add_reliability(lcdm)\nmeasr_extract(lcdm, \"classification_reliability\")\n#&gt; # A tibble: 3 × 3\n#&gt;   attribute      accuracy consistency\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 addition          0.960       0.927\n#&gt; 2 multiplication    0.956       0.917\n#&gt; 3 fractions         0.936       0.882\n\nAs you can see, for each type of model evaluation, we follow the same process of add_{metric} and then we can use measr_extract() to view the results. For a complete list of model evaluation options, see ?model_evaluation."
  },
  {
    "objectID": "blog/2023-04-measr-0.2.1/index.html#future-development",
    "href": "blog/2023-04-measr-0.2.1/index.html#future-development",
    "title": "measr 0.2.1",
    "section": "Future Development",
    "text": "Future Development\nWe’re already in the process of adding features and making improvements for the next version of measr. Some highlights include:\n\nAdding support for more DCM subtypes\nMore refined prior specifications\nAdditional vignettes including a complete description of model evaluation and example case studies\n\nIf you have a specific feature request, suggestion for improvement, or run into any problems, please open an issue on the project repository!"
  },
  {
    "objectID": "blog/2023-04-measr-0.2.1/index.html#acknowledgments",
    "href": "blog/2023-04-measr-0.2.1/index.html#acknowledgments",
    "title": "measr 0.2.1",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThe research reported here was supported by the Institute of Education Sciences, U.S. Department of Education, through Grant R305D210045 to the University of Kansas. The opinions expressed are those of the authors and do not represent the views of the the Institute or the U.S. Department of Education.\nFeatured photo by Fleur on Unsplash."
  },
  {
    "objectID": "blog/2023-06-measr-0.3.1/index.html",
    "href": "blog/2023-06-measr-0.3.1/index.html",
    "title": "measr 0.3.1",
    "section": "",
    "text": "We’re stoked to announce the release of measr 0.3.1. The goal of measr is to provide applied researchers and psychometricians with a user friendly interface for estimating and evaluating diagnostic classification models (DCMs). This update is a minor release to introduce a couple of enhancements to model and prior specifications.\nYou can install the update version of measr from CRAN with:\ninstall.packages(\"measr\")\nThis blog post will highlight the two enhancements included in the update. For a complete list of changes, check out the changelog.\nlibrary(measr)"
  },
  {
    "objectID": "blog/2023-06-measr-0.3.1/index.html#model-specifications",
    "href": "blog/2023-06-measr-0.3.1/index.html#model-specifications",
    "title": "measr 0.3.1",
    "section": "Model Specifications",
    "text": "Model Specifications\nIn this version, support for a new DCM subtype was added. The compensatory reparameterized unified model (C-RUM) model can now be specified in measr_dcm() with type = \"crum\". The C-RUM is similar to the log-linear cognitive diagnostic model (LCDM), except the C-RUM estimates only item-level intercepts and main effects (i.e., no interactions when multiple attributes are measured by an item). Because of this, along with the addition of the C-RUM, the LCDM now has additional flexibility through the new max_interaction argument. When using type = \"lcdm\", max_interaction specifies the highest level interaction to be estimated. For example, setting max_interaction = 2 would estimate the LCDM with only intercepts, main effects, and two-way interactions. If an item measures 3 or more attributes, the 3-way interactions and higher will be excluded. Specifying type = \"lcdm\" with max_interaction = 1 is equivalent to the C-RUM, as 1 indicates that main effects are the highest-level interaction to be estimated.\nThis version also introduces new options for the structural component of the DCMs. Currently two attributes structures are possible, and are defined through the attribute_structure argument. The first is attribute_structure = \"unconstrained\". This is the default, which makes no assumptions about the relationships between attributes. Specifying and unconstrained structural model is equivalent to the saturated structural model described by Hu & Templin (2020) and in Chapter 8 of Rupp et al. (2010). The other option currently supported is attribute_structure = \"independent\". When an independent attribute structure is specified, the proficiency or the presence of one attribute is independent of other attributes. Future development will include additional attribute structure specifications such as a reduced loglinear model (e.g., Thompson, 2018) or a Bayesian Network (e.g., Hu & Templin, 2020; Martinez & Templin, 2023)."
  },
  {
    "objectID": "blog/2023-06-measr-0.3.1/index.html#prior-specifications",
    "href": "blog/2023-06-measr-0.3.1/index.html#prior-specifications",
    "title": "measr 0.3.1",
    "section": "Prior Specifications",
    "text": "Prior Specifications\nThere are two main improvements to prior specifications included in this release. First, custom prior distributions can be specified for the structural model parameters. We can view the default parameters for each attribute structure specification with:\n\ndefault_dcm_priors(type = \"lcdm\", attribute_structure = \"unconstrained\")\n#&gt; # A tibble: 4 × 3\n#&gt;   class       coef  prior_def                  \n#&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;                      \n#&gt; 1 intercept   &lt;NA&gt;  normal(0, 2)               \n#&gt; 2 maineffect  &lt;NA&gt;  lognormal(0, 1)            \n#&gt; 3 interaction &lt;NA&gt;  normal(0, 2)               \n#&gt; 4 structural  Vc    dirichlet(rep_vector(1, C))\n\ndefault_dcm_priors(type = \"lcdm\", attribute_structure = \"independent\")\n#&gt; # A tibble: 4 × 3\n#&gt;   class       coef  prior_def      \n#&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;          \n#&gt; 1 intercept   &lt;NA&gt;  normal(0, 2)   \n#&gt; 2 maineffect  &lt;NA&gt;  lognormal(0, 1)\n#&gt; 3 interaction &lt;NA&gt;  normal(0, 2)   \n#&gt; 4 structural  &lt;NA&gt;  beta(1, 1)\n\nWe can also view the specific parameters available for a specific model using the get_parameters() function. For example, using the ECPE Q-matrix, we can see the available parameters for each type of model.\n\nlibrary(tidyverse)\n\nget_parameters(ecpe_qmatrix, attribute_structure = \"unconstrained\") |&gt; \n  filter(class == \"structural\")\n#&gt; # A tibble: 1 × 4\n#&gt;   item_id class      attributes coef  \n#&gt;     &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;glue&gt;\n#&gt; 1      NA structural &lt;NA&gt;       Vc\n\nget_parameters(ecpe_qmatrix, attribute_structure = \"independent\") |&gt; \n  filter(class == \"structural\")\n#&gt; # A tibble: 3 × 4\n#&gt;   item_id class      attributes coef  \n#&gt;     &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;glue&gt;\n#&gt; 1      NA structural &lt;NA&gt;       eta[1]\n#&gt; 2      NA structural &lt;NA&gt;       eta[2]\n#&gt; 3      NA structural &lt;NA&gt;       eta[3]\n\nThe second improvement is additional checking of user-specified priors. Specifically, measr_dcm() will now throw and error if we try to specify a prior for a class or coefficient that is inconsistent with our chosen DCM. For example, in the previous example of structural model priors that there are different parameters depending on whether an unconstrained or independent structure is specified. If we try to define a prior for eta[1], which is only relevant for an independent structure, but an unconstrained structure is specified, we get an error.\n\nmeasr_dcm(data = ecpe_data, qmatrix = ecpe_qmatrix,\n          resp_id = \"resp_id\", item_id = \"item_id\",\n          attribute_structure = \"unconstrained\",\n          prior = prior(beta(5,17), class = \"structural\", coef = \"eta[1]\"))\n#&gt; Error in `abort_bad_argument()`:\n#&gt; ! Prior for parameter `eta[1]` with class `structural` is not relevant for the chosen model or specified Q-matrix. See `?get_parameters()` for a list of relevant parameters.\n\nAs always, please open an issue with any bugs or feature requests for future development!"
  },
  {
    "objectID": "blog/2025-08-dcmdata-0.1.0/index.html",
    "href": "blog/2025-08-dcmdata-0.1.0/index.html",
    "title": "dcmdata 0.1.0",
    "section": "",
    "text": "We are so happy to announce the release of a new package, dcmdata. The goal of dcmdata is to provide easy access to data sets that can be used for demonstrating and testing diagnostic classification models (DCM; also called cognitive diagnostic models [CDMs]).\nYou can install dcmdata from CRAN with:\nThis blog post will highlight the major features and plans for future development.\nlibrary(dcmdata)"
  },
  {
    "objectID": "blog/2025-08-dcmdata-0.1.0/index.html#data-sets",
    "href": "blog/2025-08-dcmdata-0.1.0/index.html#data-sets",
    "title": "dcmdata 0.1.0",
    "section": "Data sets",
    "text": "Data sets\ndcmdata contains both real and simulated data sets. All data sets include both response data and a Q-matrix. The real data sets include the MacReady and Dayton (1977) multiplication data (MDM) and the Examination for the certificate of proficiency in English (ECPE), as described by Templin & Hoffman (2013).\nThe MDM data are a small data set of four items that measure a single attribute, multiplication. As such, this data is useful for use cases where you are interested in a fairly short estimation time. For example, this data could be used to quickly interate while testing model code, or in training workshops where time is limited.\n\nmdm_data\n#&gt; # A tibble: 142 × 5\n#&gt;    respondent  mdm1  mdm2  mdm3  mdm4\n#&gt;    &lt;fct&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt;  1 m8qre          1     1     1     1\n#&gt;  2 8wMPc          1     1     1     1\n#&gt;  3 xdbT8          1     1     1     1\n#&gt;  4 Ee9ob          1     1     1     1\n#&gt;  5 0tyTA          1     1     1     1\n#&gt;  6 L4bzq          1     1     1     1\n#&gt;  7 QTW1v          1     1     1     1\n#&gt;  8 w4NOH          1     1     1     1\n#&gt;  9 t9sIe          1     1     1     1\n#&gt; 10 FDa7I          1     1     1     1\n#&gt; # ℹ 132 more rows\n\nmdm_qmatrix\n#&gt; # A tibble: 4 × 2\n#&gt;   item  multiplication\n#&gt;   &lt;chr&gt;          &lt;int&gt;\n#&gt; 1 mdm1               1\n#&gt; 2 mdm2               1\n#&gt; 3 mdm3               1\n#&gt; 4 mdm4               1\n\nIn contrast, the ECPE data are perhaps a more representative of data you might gather in practice. These data consist of 28 items measuring 3 attributes and taken by 2,922 respondents. Because multiple attributes are measured, this data can be used to demonstrate how different attributes interact on a single item when using different compensatory and noncompensatory DCMs. Additionally, Templin & Bradshaw (2014) demonstrated that the three attributes follow a linear hierarchy, such that respondents are typically demonstrate proficiency on lexical, cohesive, and morphosyntactic rules, in that order. That is, the earlier skills represent precursor knowledge necessary for proficiency on the later skills. The hierarchy makes the ECPE data excellent for demonstrating the effect of different structural model specifications in a DCM.\n\necpe_data\n#&gt; # A tibble: 2,922 × 29\n#&gt;    resp_id    E1    E2    E3    E4    E5    E6    E7    E8    E9   E10   E11\n#&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt;  1       1     1     1     1     0     1     1     1     1     1     1     1\n#&gt;  2       2     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  3       3     1     1     1     1     1     1     0     1     1     1     1\n#&gt;  4       4     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  5       5     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  6       6     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  7       7     1     1     1     1     1     1     1     1     1     1     1\n#&gt;  8       8     0     1     1     1     1     1     0     1     1     1     0\n#&gt;  9       9     1     1     1     1     1     1     1     1     1     1     1\n#&gt; 10      10     1     1     1     1     0     0     1     1     1     1     1\n#&gt; # ℹ 2,912 more rows\n#&gt; # ℹ 17 more variables: E12 &lt;int&gt;, E13 &lt;int&gt;, E14 &lt;int&gt;, E15 &lt;int&gt;, E16 &lt;int&gt;,\n#&gt; #   E17 &lt;int&gt;, E18 &lt;int&gt;, E19 &lt;int&gt;, E20 &lt;int&gt;, E21 &lt;int&gt;, E22 &lt;int&gt;,\n#&gt; #   E23 &lt;int&gt;, E24 &lt;int&gt;, E25 &lt;int&gt;, E26 &lt;int&gt;, E27 &lt;int&gt;, E28 &lt;int&gt;\n\necpe_qmatrix\n#&gt; # A tibble: 28 × 4\n#&gt;    item_id morphosyntactic cohesive lexical\n#&gt;    &lt;chr&gt;             &lt;int&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt;  1 E1                    1        1       0\n#&gt;  2 E2                    0        1       0\n#&gt;  3 E3                    1        0       1\n#&gt;  4 E4                    0        0       1\n#&gt;  5 E5                    0        0       1\n#&gt;  6 E6                    0        0       1\n#&gt;  7 E7                    1        0       1\n#&gt;  8 E8                    0        1       0\n#&gt;  9 E9                    0        0       1\n#&gt; 10 E10                   1        0       0\n#&gt; # ℹ 18 more rows\n\nFinally, one simulated data set is included. This data set is based on the diagnosing teachers’ multiplicative reasoning (DTMR) data presented by Bradshaw et al. (2014), and consists of 990 responses to 27 items that collectively measure 4 attributes. Consistent with the results presented by Bradshaw et al. (2014), the data was generated using the loglinear cognitive diagnostic model (LCDM; Henson et al., 2009; Henson & Templin, 2019), and item and attribute names are included as reported in their Table 1.\n\ndtmr_data\n#&gt; # A tibble: 990 × 28\n#&gt;    id      `1`   `2`   `3`   `4`   `5`   `6`   `7`  `8a`  `8b`  `8c`  `8d`   `9`\n#&gt;    &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt;  1 0008…     1     1     0     1     0     0     1     1     0     1     1     0\n#&gt;  2 0009…     0     1     0     0     0     0     0     1     1     1     0     1\n#&gt;  3 0024…     0     1     0     0     0     0     1     1     1     1     0     0\n#&gt;  4 0031…     0     1     0     0     1     0     1     1     1     0     0     0\n#&gt;  5 0061…     0     1     1     0     0     0     0     0     0     1     0     0\n#&gt;  6 0087…     0     1     1     1     0     0     0     1     1     1     1     0\n#&gt;  7 0092…     0     1     1     1     1     0     0     1     1     1     0     0\n#&gt;  8 0097…     0     0     0     1     0     0     0     1     0     1     0     0\n#&gt;  9 0111…     0     1     1     0     0     0     0     1     0     1     1     0\n#&gt; 10 0121…     0     1     0     0     0     0     0     1     1     1     1     0\n#&gt; # ℹ 980 more rows\n#&gt; # ℹ 15 more variables: `10a` &lt;int&gt;, `10b` &lt;int&gt;, `10c` &lt;int&gt;, `11` &lt;int&gt;,\n#&gt; #   `12` &lt;int&gt;, `13` &lt;int&gt;, `14` &lt;int&gt;, `15a` &lt;int&gt;, `15b` &lt;int&gt;, `15c` &lt;int&gt;,\n#&gt; #   `16` &lt;int&gt;, `17` &lt;int&gt;, `18` &lt;int&gt;, `21` &lt;int&gt;, `22` &lt;int&gt;\n\ndtmr_qmatrix\n#&gt; # A tibble: 27 × 5\n#&gt;    item  referent_units partitioning_iterating appropriateness\n#&gt;    &lt;chr&gt;          &lt;dbl&gt;                  &lt;dbl&gt;           &lt;dbl&gt;\n#&gt;  1 1                  1                      0               0\n#&gt;  2 2                  0                      0               1\n#&gt;  3 3                  0                      1               0\n#&gt;  4 4                  1                      0               0\n#&gt;  5 5                  1                      0               0\n#&gt;  6 6                  0                      1               0\n#&gt;  7 7                  1                      0               0\n#&gt;  8 8a                 0                      0               1\n#&gt;  9 8b                 0                      0               1\n#&gt; 10 8c                 0                      0               1\n#&gt; # ℹ 17 more rows\n#&gt; # ℹ 1 more variable: multiplicative_comparison &lt;dbl&gt;\n\nBecause the data is simulated we have access to the “true” values for respondents and items. The class probabilities of respondents belonging to a given proficiency pattern are reported in Izsák et al. (2019). Using these probabilities, we generated a class for each of the 990 respondents, which is available in dtmr_true_profiles.\n\ndtmr_true_structural\n#&gt; # A tibble: 16 × 5\n#&gt;    referent_units partitioning_iterating appropriateness multiplicative_compar…¹\n#&gt;             &lt;dbl&gt;                  &lt;dbl&gt;           &lt;dbl&gt;                   &lt;dbl&gt;\n#&gt;  1              0                      0               0                       0\n#&gt;  2              1                      0               0                       0\n#&gt;  3              0                      1               0                       0\n#&gt;  4              0                      0               1                       0\n#&gt;  5              0                      0               0                       1\n#&gt;  6              1                      1               0                       0\n#&gt;  7              1                      0               1                       0\n#&gt;  8              1                      0               0                       1\n#&gt;  9              0                      1               1                       0\n#&gt; 10              0                      1               0                       1\n#&gt; 11              0                      0               1                       1\n#&gt; 12              1                      1               1                       0\n#&gt; 13              1                      1               0                       1\n#&gt; 14              1                      0               1                       1\n#&gt; 15              0                      1               1                       1\n#&gt; 16              1                      1               1                       1\n#&gt; # ℹ abbreviated name: ¹​multiplicative_comparison\n#&gt; # ℹ 1 more variable: class_probability &lt;dbl&gt;\n\ndtmr_true_profiles\n#&gt; # A tibble: 990 × 5\n#&gt;    id     referent_units partitioning_iterating appropriateness\n#&gt;    &lt;fct&gt;           &lt;dbl&gt;                  &lt;dbl&gt;           &lt;dbl&gt;\n#&gt;  1 039517              0                      0               0\n#&gt;  2 500170              0                      0               0\n#&gt;  3 104795              1                      1               1\n#&gt;  4 113558              1                      1               1\n#&gt;  5 564266              0                      1               1\n#&gt;  6 039726              1                      1               1\n#&gt;  7 075968              0                      1               1\n#&gt;  8 375846              0                      0               0\n#&gt;  9 032129              0                      0               0\n#&gt; 10 138501              0                      1               0\n#&gt; # ℹ 980 more rows\n#&gt; # ℹ 1 more variable: multiplicative_comparison &lt;dbl&gt;\n\nThe item parameters for the LCDM are reported in Table 1 of Bradshaw et al. (2014). Using the item parameters and the true profiles, we can calculate the probability that each simulated respondent provides a correct response to each item. These probabilities are then used to generate the simulated item responses in dtmr_data.\n\ndtmr_true_items\n#&gt; # A tibble: 27 × 7\n#&gt;    item  intercept referent_units partitioning_iterating appropriateness\n#&gt;    &lt;chr&gt;     &lt;dbl&gt;          &lt;dbl&gt;                  &lt;dbl&gt;           &lt;dbl&gt;\n#&gt;  1 1         -1.12           2.24                  NA              NA   \n#&gt;  2 2          0.59          NA                     NA               1.27\n#&gt;  3 3         -2.07          NA                      1.7            NA   \n#&gt;  4 4         -1.19           0.65                  NA              NA   \n#&gt;  5 5         -1.67           1.52                  NA              NA   \n#&gt;  6 6         -3.81          NA                      2.08           NA   \n#&gt;  7 7         -0.73           1.2                   NA              NA   \n#&gt;  8 8a        -0.62          NA                     NA               4.25\n#&gt;  9 8b        -0.09          NA                     NA               2.16\n#&gt; 10 8c         0.28          NA                     NA               0.87\n#&gt; # ℹ 17 more rows\n#&gt; # ℹ 2 more variables: multiplicative_comparison &lt;dbl&gt;,\n#&gt; #   referent_units__partitioning_iterating &lt;dbl&gt;\n\nFor a complete description of how the data was simulated, see ?dtmr."
  },
  {
    "objectID": "blog/2025-08-dcmdata-0.1.0/index.html#future-work",
    "href": "blog/2025-08-dcmdata-0.1.0/index.html#future-work",
    "title": "dcmdata 0.1.0",
    "section": "Future work",
    "text": "Future work\nFuture work will focus on providing tools for simulating data from a variety of DCMs. The goal is to provide a tool that will make it easier to quickly simulate a large number a data sets, as is often required for simulation studies.\nWe also plan to continue adding more real data sets (e.g., from the Item Response Warehouse). If you know of any data sets that would be a good fit, or have a data set you’d like to contribute yourself, please open an issue!"
  },
  {
    "objectID": "blog/2025-08-dcmdata-0.1.0/index.html#acknowledgments",
    "href": "blog/2025-08-dcmdata-0.1.0/index.html#acknowledgments",
    "title": "dcmdata 0.1.0",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThe research reported here was supported by the Institute of Education Sciences, U.S. Department of Education, through Grants R305D210045 and R305D240032 to the University of Kansas Center for Research, Inc., ATLAS. The opinions expressed are those of the authors and do not represent the views of the the Institute or the U.S. Department of Education.\nFeatured photo by Scott Graham on Unsplash."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Posts",
    "section": "",
    "text": "dcmstan 0.1.0\n\n\n\n\n\n\npackage\n\ndcmstan\n\n\n\nInitial release dcmstan for specifying diagnostic models and generating Stan code.\n\n\n\n\n\nNovember 26, 2025\n\n\nW. Jake Thompson\n\n\n\n\n\n\n\n\n\n\n\n\ndcmdata 0.1.0\n\n\n\n\n\n\npackage\n\ndcmdata\n\n\n\nInitial release dcmdata for accessing data from diagnostic assessments.\n\n\n\n\n\nAugust 21, 2025\n\n\nW. Jake Thompson\n\n\n\n\n\n\n\n\n\n\n\n\nmeasr 1.0.0\n\n\n\n\n\n\npackage\n\nmeasr\n\n\n\nA major release to reflect development milestones.\n\n\n\n\n\nFebruary 5, 2024\n\n\nW. Jake Thompson\n\n\n\n\n\n\n\n\n\n\n\n\nmeasr 0.3.1\n\n\n\n\n\n\npackage\n\nmeasr\n\n\n\nA few updates for specifying models and priors.\n\n\n\n\n\nJune 6, 2023\n\n\nW. Jake Thompson\n\n\n\n\n\n\n\n\n\n\n\n\nmeasr 0.2.1\n\n\n\n\n\n\npackage\n\nmeasr\n\n\n\nInitial release of measr package for estimating and evaluating diagnostic classification models.\n\n\n\n\n\nApril 10, 2023\n\n\nW. Jake Thompson\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Under construction",
    "section": "",
    "text": "This site is currently under development. Thanks for your patience as we continue our work!\n\n\n\nPhoto by Stefan Lehner on Unsplash"
  },
  {
    "objectID": "team/JeffreyCHoover/index.html",
    "href": "team/JeffreyCHoover/index.html",
    "title": "Jeffrey C. Hoover",
    "section": "",
    "text": "Jeff Hoover is a psychometrician at Accessible Teaching, Learning, and Assessment Systems (ATLAS), a strategic center within the Achievement and Assessment Institute at the University of Kansas. He received a PhD in educational psychology and research from the University of Kansas’ School of Education and Human Sciences in 2022. His research interests include diagnostic classification models and the integration of cutting-edge technologies such as process data and artificial intelligence into operational assessments."
  },
  {
    "objectID": "team/index.html",
    "href": "team/index.html",
    "title": "Meet the team",
    "section": "",
    "text": "W. Jake Thompson\n\n            Principal Investigator\n\n            \n              \n                \n                  \n                    \n                     ORCID\n                  \n                \n                  \n                    \n                     GitHub\n                  \n                \n                  \n                    \n                     Bluesky\n                  \n                \n                  \n                    \n                     Website\n                  \n                \n              \n            \n        \n\n    \n    \n    \n\n        \n        \n            \n        \n        \n\n        \n            Jeffrey C. Hoover\n\n            Psychometrician\n\n            \n              \n                \n                  \n                    \n                     ORCID\n                  \n                \n                  \n                    \n                     GitHub\n                  \n                \n              \n            \n        \n\n    \n    \n    \n\n        \n        \n            \n        \n        \n\n        \n            Auburn Jimenez\n\n            Psychometrician\n\n            \n              \n                \n                  \n                    \n                     ORCID\n                  \n                \n                  \n                    \n                     GitHub\n                  \n                \n              \n            \n        \n\n    \n    \n\nNo matching items"
  }
]